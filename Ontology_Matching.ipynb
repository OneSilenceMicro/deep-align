{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deep-Learning-PC\\Desktop\\Various_for_biomedical_semantics_journal\\Rebuttal\\deep-align\\main\\\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd() + '\\\\main\\\\'\n",
    "sys.path.append(cwd)\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 #define _CUDA_NDARRAY_C\n",
      "2 \n",
      "3 #include <Python.h>\n",
      "4 #include <structmember.h>\n",
      "5 #include \"theano_mod_helper.h\"\n",
      "6 \n",
      "7 #include <numpy/arrayobject.h>\n",
      "8 #include <iostream>\n",
      "9 \n",
      "10 #include \"cuda_ndarray.cuh\"\n",
      "11 \n",
      "12 #ifndef CNMEM_DLLEXPORT\n",
      "13 #define CNMEM_DLLEXPORT\n",
      "14 #endif\n",
      "15 \n",
      "16 #include \"cnmem.h\"\n",
      "17 #include \"cnmem.cpp\"\n",
      "18 \n",
      "19 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
      "20 #define COMPUTE_GPU_MEM_USED 0\n",
      "21 \n",
      "22 //If true, we fill with NAN allocated device memory.\n",
      "23 #define ALLOC_MEMSET 0\n",
      "24 \n",
      "25 //If true, we print out when we free a device pointer, uninitialize a\n",
      "26 //CudaNdarray, or allocate a device pointer\n",
      "27 #define PRINT_FREE_MALLOC 0\n",
      "28 \n",
      "29 //If true, we do error checking at the start of functions, to make sure there\n",
      "30 //is not a pre-existing error when the function is called.\n",
      "31 //You probably need to set the environment variable\n",
      "32 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
      "33 //preprocessor macro in cuda_ndarray.cuh\n",
      "34 //if you want this to work.\n",
      "35 #define PRECHECK_ERROR 0\n",
      "36 \n",
      "37 cublasHandle_t handle = NULL;\n",
      "38 int* err_var = NULL;\n",
      "39 \n",
      "40 /////////////////////////\n",
      "41 // Alloc and Free\n",
      "42 /////////////////////////\n",
      "43 \n",
      "44 static int g_gpu_context_active = 0;\n",
      "45 \n",
      "46 \n",
      "47 PyObject *\n",
      "48 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
      "49 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
      "50 \n",
      "51 \n",
      "52 /**\n",
      "53  *\n",
      "54  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
      "55  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
      "56  *\n",
      "57  */\n",
      "58 int _outstanding_mallocs[] = {0,0};\n",
      "59 \n",
      "60 #if COMPUTE_GPU_MEM_USED\n",
      "61 size_t _allocated_size = 0;\n",
      "62 size_t _max_allocated_size = 0;\n",
      "63 \n",
      "64 const int TABLE_SIZE = 10000;\n",
      "65 struct table_struct{\n",
      "66     void* ptr;\n",
      "67     size_t size;\n",
      "68 };\n",
      "69 table_struct _alloc_size_table[TABLE_SIZE];\n",
      "70 #endif\n",
      "71 \n",
      "72 void * device_malloc(size_t size)\n",
      "73 {\n",
      "74     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
      "75 }\n",
      "76 \n",
      "77 ///@TODO: thejaswi: link this option to a theano config variable?\n",
      "78 static bool g_use_cnmem = false;\n",
      "79 static const int g_max_devices = 8;\n",
      "80 int initCnmem(int card_number_provided, int card_nb, size_t mem) {\n",
      "81     static bool cnmemInitialized = false;\n",
      "82     if(cnmemInitialized) {\n",
      "83         return 0;\n",
      "84     }\n",
      "85     // On stderr to be at the same place as \"Using gpu device...\"\n",
      "86     int numDevices = 0;\n",
      "87     cnmemDevice_t devices[g_max_devices];\n",
      "88     if(cudaGetDeviceCount(&numDevices) != cudaSuccess) {\n",
      "89         PyErr_Format(PyExc_RuntimeError,\n",
      "90                      \"initCnmem: 'cudaGetDeviceCount' failed! Reason=%s\\n\",\n",
      "91                      cudaGetErrorString(cudaGetLastError()));\n",
      "92         return -1;\n",
      "93     }\n",
      "94     if(card_number_provided){\n",
      "95         numDevices = 1;\n",
      "96         int i = 0;\n",
      "97         devices[i].device = card_nb;\n",
      "98         devices[i].size = mem;\n",
      "99         ///@TODO: thejaswi: add support for multiple streams\n",
      "100         devices[i].numStreams = 0;\n",
      "101         devices[i].streams = NULL;\n",
      "102         devices[i].streamSizes = NULL;\n",
      "103     }else{\n",
      "104         for(int i=0;i<numDevices;++i) {\n",
      "105             devices[i].device = i;\n",
      "106             devices[i].size = mem;\n",
      "107             ///@TODO: thejaswi: add support for multiple streams\n",
      "108             devices[i].numStreams = 0;\n",
      "109             devices[i].streams = NULL;\n",
      "110         }\n",
      "111     }\n",
      "112 \n",
      "113     ///@TODO: thejaswi: passing custom cnmem flags?\n",
      "114     cnmemStatus_t status = cnmemInit(numDevices, devices, CNMEM_FLAGS_DEFAULT);\n",
      "115     if(status != CNMEM_STATUS_SUCCESS) {\n",
      "116         PyErr_Format(PyExc_RuntimeError,\n",
      "117                      \"initCnmem: cnmemInit call failed! Reason=%s. numdev=%d\\n\",\n",
      "118                      cnmemGetErrorString(status), numDevices);\n",
      "119         return -1;\n",
      "120     }\n",
      "121     cnmemInitialized = true;\n",
      "122     return 0;\n",
      "123 }\n",
      "124 \n",
      "125 void * device_malloc(size_t size, int verbose)\n",
      "126 {\n",
      "127     #if PRECHECK_ERROR\n",
      "128         cudaThreadSynchronize();\n",
      "129         cudaError_t prevError = cudaGetLastError();\n",
      "130         if (cudaSuccess != prevError)\n",
      "131         {\n",
      "132             fprintf(stderr,\n",
      "133                     \"Error existed before calling device_malloc. %s\\n\",\n",
      "134                     cudaGetErrorString(prevError)\n",
      "135                     );\n",
      "136         }\n",
      "137     #endif\n",
      "138     void * rval=NULL;\n",
      "139     ///@TODO: thejaswi: support for multiple-streams?\n",
      "140     if(g_use_cnmem) {\n",
      "141         cnmemStatus_t status = CNMEM_STATUS_SUCCESS;\n",
      "142         status = cnmemMalloc(&rval, size, NULL);\n",
      "143         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "144             PyErr_Format(PyExc_MemoryError,\n",
      "145                          \"Error allocating %zd bytes of device memory (%s).\",\n",
      "146                          size, cnmemGetErrorString(status));\n",
      "147             return NULL;\n",
      "148         }\n",
      "149     }\n",
      "150     else {\n",
      "151         cudaError_t err = cudaMalloc(&rval, size);\n",
      "152         if (cudaSuccess != err)\n",
      "153         {\n",
      "154             // Clear the error flag, cudaMalloc doesn't do it.\n",
      "155             // Currently this returns the same thing as err, but if in future\n",
      "156             // it returns something else I still don't see why we should ignore\n",
      "157             // it.  All we want to do here is reset the flag.\n",
      "158             cudaGetLastError();\n",
      "159             if (verbose)\n",
      "160             {\n",
      "161                 size_t free = 0, total = 0;\n",
      "162                 cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "163                 if (err2 != cudaSuccess){\n",
      "164                     cudaGetLastError();\n",
      "165                     fprintf(stderr,\n",
      "166                             \"Error when trying to find the memory information\"\n",
      "167                             \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "168                 }\n",
      "169                 #if COMPUTE_GPU_MEM_USED\n",
      "170                     fprintf(stderr,\n",
      "171                             \"Error allocating %zd bytes of device memory (%s).\"\n",
      "172                             \" new total bytes allocated: %d.\"\n",
      "173                             \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "174                             size, cudaGetErrorString(err), _allocated_size,\n",
      "175                             free, total);\n",
      "176                 #else\n",
      "177                     fprintf(stderr,\n",
      "178                             \"Error allocating %zd bytes of device memory (%s).\"\n",
      "179                             \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "180                             size, cudaGetErrorString(err), free, total);\n",
      "181                 #endif\n",
      "182             }\n",
      "183             PyErr_Format(PyExc_MemoryError,\n",
      "184                          \"Error allocating %zd bytes of device memory (%s).\",\n",
      "185                          size, cudaGetErrorString(err));\n",
      "186             return NULL;\n",
      "187         }\n",
      "188     }\n",
      "189     if (rval != NULL){\n",
      "190         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
      "191         // Could this be what happen if size is 0?\n",
      "192         _outstanding_mallocs[0] += 1;\n",
      "193 \n",
      "194 #if COMPUTE_GPU_MEM_USED\n",
      "195         _allocated_size += size;\n",
      "196         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
      "197         int i = 0;\n",
      "198         for(;i<TABLE_SIZE;i++){\n",
      "199             if(NULL==_alloc_size_table[i].ptr){\n",
      "200                 _alloc_size_table[i].ptr=rval;\n",
      "201                 _alloc_size_table[i].size=size;\n",
      "202                 break;\n",
      "203             }\n",
      "204         }\n",
      "205         if (i == TABLE_SIZE){\n",
      "206             fprintf(stderr,\n",
      "207                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
      "208                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
      "209         }\n",
      "210 #endif\n",
      "211     }\n",
      "212     //fprintf(stderr,\n",
      "213     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
      "214     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
      "215 \n",
      "216     if(ALLOC_MEMSET){\n",
      "217         //We init them to nan to make sure we catch more debug case.\n",
      "218         cudaMemset(rval, 0xFF, size);\n",
      "219         //printf(\"MEMSET\\n\");\n",
      "220     }\n",
      "221     #if PRINT_FREE_MALLOC\n",
      "222         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
      "223     #endif\n",
      "224     return rval;\n",
      "225 }\n",
      "226 \n",
      "227 int device_free(void *ptr)\n",
      "228 {\n",
      "229     #if PRECHECK_ERROR\n",
      "230         cudaThreadSynchronize();\n",
      "231         cudaError_t prevError = cudaGetLastError();\n",
      "232         if (cudaSuccess != prevError)\n",
      "233         {\n",
      "234             fprintf(stderr,\n",
      "235                     \"Error existed before calling device_free. %s\\n\",\n",
      "236                     cudaGetErrorString(prevError)\n",
      "237                     );\n",
      "238         }\n",
      "239     #endif\n",
      "240     #if PRINT_FREE_MALLOC\n",
      "241         size_t free = 0, total = 0;\n",
      "242         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "243         if (err2 != cudaSuccess){\n",
      "244             cudaGetLastError();\n",
      "245             fprintf(stderr,\n",
      "246                     \"Error when tring to find the memory information\"\n",
      "247                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "248         }\n",
      "249         #if COMPUTE_GPU_MEM_USED\n",
      "250         {\n",
      "251             int i = 0;\n",
      "252             for(;i<TABLE_SIZE;i++)\n",
      "253                 if(_alloc_size_table[i].ptr==ptr){\n",
      "254                     break;\n",
      "255                 }\n",
      "256             assert(i<TABLE_SIZE);\n",
      "257             fprintf(stderr, \"device_free %p of size %d.\"\n",
      "258                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "259                     ptr, _alloc_size_table[i].size, free, total);\n",
      "260         }\n",
      "261         #else\n",
      "262             fprintf(stderr, \"device_free %p.\"\n",
      "263                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "264                     ptr, free, total);\n",
      "265         #endif\n",
      "266     #endif\n",
      "267 \n",
      "268     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
      "269     if(!g_gpu_context_active) {\n",
      "270         return 0;\n",
      "271     }\n",
      "272 \n",
      "273     ///@TODO: thejaswi: multi-stream support\n",
      "274     if(g_use_cnmem) {\n",
      "275         cnmemStatus_t status = cnmemFree(ptr, NULL);\n",
      "276         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "277             fprintf(stderr, \"device_free: cnmemFree call failed! Reason=%s\\n\",\n",
      "278                     cnmemGetErrorString(status));\n",
      "279         }\n",
      "280     }\n",
      "281     else {\n",
      "282         // We need sync as the Theano's GC could remove intermediate variable that\n",
      "283         // are still needed as the gpu kernel are running or in the queue.\n",
      "284         CNDA_BEGIN_ALLOW_THREADS\n",
      "285         cudaThreadSynchronize();\n",
      "286         CNDA_END_ALLOW_THREADS\n",
      "287 \n",
      "288         cudaError_t err =  cudaFree(ptr);\n",
      "289         if (cudaSuccess != err)\n",
      "290         {\n",
      "291             // Clear the error flag, cudaFree doesn't do it.\n",
      "292             // Currently this returns the same thing as err, but if in future\n",
      "293             // it returns something else I still don't see why we should ignore\n",
      "294             // it.  All we want to do here is reset the flag.\n",
      "295             cudaGetLastError();\n",
      "296             size_t free = 0, total = 0;\n",
      "297             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "298             if (err2 != cudaSuccess){\n",
      "299                 cudaGetLastError();\n",
      "300                 fprintf(stderr,\n",
      "301                         \"Error when tring to find the memory information\"\n",
      "302                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "303             }\n",
      "304             #if COMPUTE_GPU_MEM_USED\n",
      "305             {\n",
      "306                 int i = 0;\n",
      "307                 for(;i<TABLE_SIZE;i++)\n",
      "308                     if(_alloc_size_table[i].ptr==ptr){\n",
      "309                         break;\n",
      "310                     }\n",
      "311                 assert(i<TABLE_SIZE);\n",
      "312                 fprintf(stderr,\n",
      "313                         \"Error freeing device pointer %p (%s) of size %d. %zd byte already allocated.\"\n",
      "314                         \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "315                         ptr, cudaGetErrorString(err),\n",
      "316                         _alloc_size_table[i].size, _allocated_size, free, total);\n",
      "317             }\n",
      "318             #else\n",
      "319                 fprintf(stderr,\n",
      "320                         \"Error freeing device pointer %p (%s).\"\n",
      "321                         \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "322                         ptr,\n",
      "323                         cudaGetErrorString(err), free, total);\n",
      "324             #endif\n",
      "325             if (NULL != PyErr_Occurred()){\n",
      "326                 fprintf(stderr,\n",
      "327                         \"device_free: cudaFree() returned an error, but there is already an\"\n",
      "328                         \" Python error set. This happen during the clean up when there is a\"\n",
      "329                         \" first error and the CUDA driver is in a so bad state that it don't\"\n",
      "330                         \" work anymore. We keep the previous error set to help debugging it.\");\n",
      "331                 return -1;\n",
      "332             }\n",
      "333             PyErr_Format(PyExc_MemoryError,\n",
      "334                     \"error freeing device pointer %p (%s)\",\n",
      "335                     ptr,\n",
      "336                     cudaGetErrorString(err));\n",
      "337             return -1;\n",
      "338         }\n",
      "339     }\n",
      "340     _outstanding_mallocs[0] -= (ptr != NULL);\n",
      "341     #if COMPUTE_GPU_MEM_USED\n",
      "342         int i=0;\n",
      "343         size_t total_freed = 0;\n",
      "344         for(;i<TABLE_SIZE;i++)\n",
      "345             if(_alloc_size_table[i].ptr==ptr){\n",
      "346                 _allocated_size -= _alloc_size_table[i].size;\n",
      "347                 total_freed += _alloc_size_table[i].size;\n",
      "348                 _alloc_size_table[i].ptr=0;\n",
      "349                 _alloc_size_table[i].size=0;\n",
      "350 \n",
      "351                 break;\n",
      "352             }\n",
      "353         //if(i==TABLE_SIZE)\n",
      "354         //    printf(\"Unallocated unknow size!\\n\");\n",
      "355         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
      "356     #endif\n",
      "357     return 0;\n",
      "358 }\n",
      "359 \n",
      "360 static PyObject *\n",
      "361 outstanding_mallocs(PyObject* self, PyObject * args)\n",
      "362 {\n",
      "363     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
      "364 }\n",
      "365 \n",
      "366 \n",
      "367 static void *work_mem = NULL;\n",
      "368 static size_t work_size = 0;\n",
      "369 \n",
      "370 /*\n",
      "371  * Returns a chunk of memory for temporary work inside of an op. You can only\n",
      "372  * request a single chunk of memory at a time since it is reused.\n",
      "373  */\n",
      "374 void *get_work_mem(size_t sz) {\n",
      "375     if (sz <= work_size)\n",
      "376         return work_mem;\n",
      "377     device_free(work_mem);\n",
      "378     work_mem = device_malloc(sz);\n",
      "379     work_size = sz;\n",
      "380     if (work_mem == NULL)\n",
      "381         work_size = 0;\n",
      "382     return work_mem;\n",
      "383 }\n",
      "384 \n",
      "385 /////////////////////////\n",
      "386 // Static helper methods\n",
      "387 /////////////////////////\n",
      "388 \n",
      "389 static void\n",
      "390 CudaNdarray_null_init(CudaNdarray*self)\n",
      "391 {\n",
      "392     self->base = NULL;\n",
      "393     self->nd = -1;\n",
      "394     self->host_structure = NULL;\n",
      "395     self->data_allocated = 0;\n",
      "396     self->dev_structure_fresh = 1;\n",
      "397     self->dev_structure = NULL;\n",
      "398     self->devdata = NULL;\n",
      "399 }\n",
      "400 \n",
      "401 static int\n",
      "402 CudaNdarray_uninit(CudaNdarray*self)\n",
      "403 {\n",
      "404     #if PRINT_FREE_MALLOC\n",
      "405         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
      "406     #endif\n",
      "407     int rval = 0;\n",
      "408     if (self->data_allocated) {\n",
      "409         assert(self->devdata);\n",
      "410         if (device_free(self->devdata))\n",
      "411         {\n",
      "412             fprintf(stderr,\n",
      "413                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
      "414                     self, self->devdata);\n",
      "415             rval = -1;\n",
      "416         }\n",
      "417         self->devdata = NULL;\n",
      "418         self->data_allocated = 0;\n",
      "419     }\n",
      "420     if (self->dev_structure)\n",
      "421     {\n",
      "422         if (device_free(self->dev_structure))\n",
      "423         {\n",
      "424             fprintf(stderr,\n",
      "425                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
      "426                     self->dev_structure, self);\n",
      "427             rval = -1;\n",
      "428         }\n",
      "429         self->dev_structure = NULL;\n",
      "430     }\n",
      "431     if (self->host_structure)\n",
      "432     {\n",
      "433         free(self->host_structure);\n",
      "434         self->host_structure = NULL;\n",
      "435     }\n",
      "436     self->nd = -1;\n",
      "437     Py_XDECREF(self->base);\n",
      "438     self->base = NULL;\n",
      "439     return rval;\n",
      "440 }\n",
      "441 \n",
      "442 \n",
      "443 //make the rightmost coords change fastest\n",
      "444 //TODO: why does a downward for-loop not work????\n",
      "445 //TODO: use the log2_dims and driver code to remove / and %\n",
      "446 //TODO: skip the last division (when d == 0)\n",
      "447 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
      "448 __global__ void name (unsigned int numEls,  \\\n",
      "449         unsigned int nd, \\\n",
      "450         const int * dim,  \\\n",
      "451         const float * a_data, const int * a_str, \\\n",
      "452         float * z_data, const int * z_str) \\\n",
      "453 { \\\n",
      "454     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
      "455     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
      "456  \\\n",
      "457     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
      "458     { \\\n",
      "459         unsigned int ii = i; \\\n",
      "460         const float * a_i = a_data; \\\n",
      "461         float * z_i = z_data; \\\n",
      "462         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
      "463         { \\\n",
      "464             unsigned int d = nd - _d-1;  \\\n",
      "465             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
      "466             ii = ii / dim[d]; \\\n",
      "467             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
      "468             z_i += i_d * z_str[d]; \\\n",
      "469         } \\\n",
      "470         z_i[0] = F(a_i[0]); \\\n",
      "471     } \\\n",
      "472 }\n",
      "473 \n",
      "474 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
      "475 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
      "476 \n",
      "477 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
      "478 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
      "479 \n",
      "480 /////////////////////////////\n",
      "481 // Satisfying reqs to be Type\n",
      "482 /////////////////////////////\n",
      "483 \n",
      "484 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
      "485 static void\n",
      "486 CudaNdarray_dealloc(CudaNdarray* self)\n",
      "487 {\n",
      "488     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
      "489     if(Py_REFCNT(self) > 1)\n",
      "490       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
      "491     CudaNdarray_uninit(self);\n",
      "492     Py_TYPE(self)->tp_free((PyObject*)self);\n",
      "493     --_outstanding_mallocs[1];\n",
      "494     if (0)\n",
      "495     {\n",
      "496         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
      "497                 _outstanding_mallocs[0],\n",
      "498                 _outstanding_mallocs[1]);\n",
      "499     }\n",
      "500 }\n",
      "501 \n",
      "502 static PyObject *\n",
      "503 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
      "504 {\n",
      "505     CudaNdarray *self;\n",
      "506 \n",
      "507     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
      "508     if (self != NULL)\n",
      "509     {\n",
      "510         CudaNdarray_null_init(self);\n",
      "511         ++_outstanding_mallocs[1];\n",
      "512     }\n",
      "513     return (PyObject *)self;\n",
      "514 }\n",
      "515 static int\n",
      "516 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
      "517 {\n",
      "518     PyObject *arr=NULL;\n",
      "519 \n",
      "520     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
      "521         return -1;\n",
      "522     if (! PyArray_Check(arr))\n",
      "523     {\n",
      "524         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
      "525         return -1;\n",
      "526     }\n",
      "527     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
      "528     return rval;\n",
      "529 }\n",
      "530 static PyMemberDef CudaNdarray_members[] =\n",
      "531 {\n",
      "532     /*\n",
      "533     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
      "534      \"first name\"},\n",
      "535     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
      "536      \"last name\"},\n",
      "537     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
      "538      \"noddy number\"},\n",
      "539      */\n",
      "540     {NULL}  /* Sentinel */\n",
      "541 };\n",
      "542 \n",
      "543 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
      "544 {\n",
      "545     PyObject * dtype = NULL;\n",
      "546     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
      "547         return NULL;\n",
      "548     if (dtype) {\n",
      "549         PyArray_Descr* dtype2;\n",
      "550         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
      "551         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
      "552         {\n",
      "553             PyObject * str = PyObject_Repr(dtype);\n",
      "554             PyErr_Format(PyExc_TypeError,\n",
      "555                          \"CudaNdarray dtype parameter not understood: %s\",\n",
      "556                          PyString_AsString(str)\n",
      "557                          );\n",
      "558             Py_CLEAR(str);\n",
      "559             return NULL;\n",
      "560         }\n",
      "561         int typeNum = dtype2->type_num;\n",
      "562         Py_DECREF(dtype2);\n",
      "563         if (typeNum != NPY_FLOAT32)\n",
      "564         {\n",
      "565             PyObject * str = PyObject_Repr(dtype);\n",
      "566             PyErr_Format(PyExc_TypeError,\n",
      "567                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
      "568                          typeNum\n",
      "569                          );\n",
      "570             Py_CLEAR(str);\n",
      "571             return NULL;\n",
      "572         }\n",
      "573     }\n",
      "574 \n",
      "575     int verbose = 0;\n",
      "576     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
      "577         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "578         assert (npydims);\n",
      "579         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "580         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
      "581         free(npydims);\n",
      "582         if (!rval){\n",
      "583             return NULL;\n",
      "584         }\n",
      "585         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
      "586         return rval;\n",
      "587     }\n",
      "588     if ((self->nd < 0) || (self->devdata == 0))\n",
      "589     {\n",
      "590         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
      "591         return NULL;\n",
      "592     }\n",
      "593     CudaNdarray * contiguous_self = NULL;\n",
      "594     if (CudaNdarray_is_c_contiguous(self))\n",
      "595     {\n",
      "596         contiguous_self = self;\n",
      "597         Py_INCREF(contiguous_self);\n",
      "598         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
      "599     }\n",
      "600     else\n",
      "601     {\n",
      "602         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
      "603         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
      "604     }\n",
      "605     if (!contiguous_self)\n",
      "606     {\n",
      "607         return NULL;\n",
      "608     }\n",
      "609 \n",
      "610     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "611     assert (npydims);\n",
      "612     for (int i = 0; i < self->nd; ++i)\n",
      "613         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "614     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
      "615                                                                npydims,\n",
      "616                                                                REAL_TYPENUM);\n",
      "617     free(npydims);\n",
      "618     if (!rval)\n",
      "619     {\n",
      "620         Py_DECREF(contiguous_self);\n",
      "621         return NULL;\n",
      "622     }\n",
      "623 \n",
      "624     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
      "625 \n",
      "626     npy_intp rval_size = PyArray_SIZE(rval);\n",
      "627     void *rval_data = PyArray_DATA(rval);\n",
      "628     cudaError_t err;\n",
      "629     CNDA_BEGIN_ALLOW_THREADS;\n",
      "630 \n",
      "631     err = cudaMemcpy(rval_data, contiguous_self->devdata,\n",
      "632                      rval_size * sizeof(real),\n",
      "633                      cudaMemcpyDeviceToHost\n",
      "634                      );\n",
      "635     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "636     CNDA_END_ALLOW_THREADS;\n",
      "637 \n",
      "638     if (cudaSuccess != err)\n",
      "639     {\n",
      "640         PyErr_Format(PyExc_RuntimeError, \"error (%s)copying data to host\",\n",
      "641                      cudaGetErrorString(err));\n",
      "642         Py_DECREF(rval);\n",
      "643         rval = NULL;\n",
      "644     }\n",
      "645 \n",
      "646     Py_DECREF(contiguous_self);\n",
      "647     return (PyObject *)rval;\n",
      "648 }\n",
      "649 \n",
      "650 // TODO-- we have two functions here, ZEROS and Zeros.\n",
      "651 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
      "652 // but this naming is very weird, makes it look like a macro\n",
      "653 // we should figure out the correct convention and change to that\n",
      "654 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
      "655 {\n",
      "656 \n",
      "657     size_t total_elements = 1;\n",
      "658 \n",
      "659     for(size_t i=0;i<n;i++){\n",
      "660         // Detect overflow on unsigned integer\n",
      "661         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {\n",
      "662             PyErr_Format(PyExc_RuntimeError,\n",
      "663                          \"Can't store in size_t for the bytes requested %llu * %llu\",\n",
      "664                          (unsigned long long)total_elements,\n",
      "665                          (unsigned long long)dims[i]);\n",
      "666             return NULL;\n",
      "667         }\n",
      "668         total_elements*=dims[i];\n",
      "669     }\n",
      "670 \n",
      "671     // total_elements now contains the size of the array, in reals\n",
      "672     if (total_elements > (SIZE_MAX / sizeof(real))){\n",
      "673         PyErr_Format(PyExc_RuntimeError,\n",
      "674                      \"Can't store in size_t for the bytes requested %llu * 4\",\n",
      "675                      (unsigned long long)total_elements);\n",
      "676         return NULL;\n",
      "677     }\n",
      "678     size_t total_size = total_elements * sizeof(real);\n",
      "679 \n",
      "680     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
      "681     if (!rval)\n",
      "682     {\n",
      "683         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
      "684         return NULL;\n",
      "685     }\n",
      "686 \n",
      "687     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
      "688     {\n",
      "689         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
      "690         Py_DECREF(rval);\n",
      "691         return NULL;\n",
      "692     }\n",
      "693 \n",
      "694     // Fill with zeros\n",
      "695     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
      "696     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
      "697     {\n",
      "698         PyErr_Format(PyExc_MemoryError,\n",
      "699                      \"CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.\",\n",
      "700                      (unsigned long long)total_size);\n",
      "701         Py_DECREF(rval);\n",
      "702         return NULL;\n",
      "703     }\n",
      "704 \n",
      "705     if (cnda_copy_structure_to_device(rval))\n",
      "706     {\n",
      "707         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
      "708         Py_DECREF(rval);\n",
      "709         return NULL;\n",
      "710     }\n",
      "711     return (PyObject*) rval;\n",
      "712 }\n",
      "713 \n",
      "714 // declared as a static method (hence 1st parameter is not used)\n",
      "715 // Based on _Copy and _dimshuffle\n",
      "716 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
      "717 {\n",
      "718     if(!shape)\n",
      "719     {\n",
      "720         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
      "721         return NULL;\n",
      "722     }\n",
      "723     if(!PySequence_Check(shape))\n",
      "724     {\n",
      "725         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
      "726         return NULL;\n",
      "727     }\n",
      "728 \n",
      "729     int shplen = PySequence_Length(shape);\n",
      "730 \n",
      "731     if (shplen == 0)\n",
      "732     {\n",
      "733         return CudaNdarray_ZEROS(0, NULL);\n",
      "734     }\n",
      "735 \n",
      "736     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
      "737 \n",
      "738     if (!newdims)\n",
      "739     {\n",
      "740         PyErr_SetString(PyExc_MemoryError,\n",
      "741             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
      "742         return NULL;\n",
      "743     }\n",
      "744 \n",
      "745     // start from the end to compute strides\n",
      "746     for (int i = shplen-1; i >= 0; --i)\n",
      "747     {\n",
      "748         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
      "749         if(shp_el_obj == NULL)\n",
      "750         {\n",
      "751             // shouldn't happen since we checked length before...\n",
      "752             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
      "753             free(newdims);\n",
      "754             return NULL;\n",
      "755         }\n",
      "756 \n",
      "757         int shp_el = PyInt_AsLong(shp_el_obj);\n",
      "758         Py_DECREF(shp_el_obj);\n",
      "759 \n",
      "760         if (shp_el < 0)\n",
      "761         {\n",
      "762             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
      "763             free(newdims);\n",
      "764             return NULL;\n",
      "765         }\n",
      "766 \n",
      "767         newdims[i] = shp_el;\n",
      "768     }\n",
      "769 \n",
      "770     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
      "771 \n",
      "772     free(newdims);\n",
      "773 \n",
      "774     return (PyObject*)rval;\n",
      "775 }\n",
      "776 \n",
      "777 \n",
      "778 \n",
      "779 \n",
      "780 \n",
      "781 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
      "782 {\n",
      "783     PyObject * rval = CudaNdarray_New();\n",
      "784     if ((!rval) || (-1 == self->nd))\n",
      "785     {\n",
      "786         return rval;\n",
      "787     }\n",
      "788     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "789     {\n",
      "790         Py_DECREF(rval);\n",
      "791         return NULL;\n",
      "792     }\n",
      "793     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
      "794     {\n",
      "795         Py_DECREF(rval);\n",
      "796         return NULL;\n",
      "797     }\n",
      "798     return rval;\n",
      "799 }\n",
      "800 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
      "801 {\n",
      "802     assert(PyDict_Check(memo));\n",
      "803     PyObject * selfkey = PyInt_FromLong((long)self);\n",
      "804     assert(selfkey);\n",
      "805     if (PyDict_Contains(memo, selfkey))\n",
      "806     {\n",
      "807         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
      "808         Py_DECREF(selfkey);\n",
      "809         Py_XINCREF(rval);\n",
      "810         return rval;\n",
      "811     }\n",
      "812     else\n",
      "813     {\n",
      "814         PyObject * rval = CudaNdarray_Copy(self);\n",
      "815         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
      "816         if (NULL == rval)\n",
      "817         {\n",
      "818             Py_DECREF(selfkey);\n",
      "819             return NULL;\n",
      "820         }\n",
      "821         if (PyDict_SetItem(memo, selfkey, rval))\n",
      "822         {\n",
      "823             Py_DECREF(rval);\n",
      "824             Py_DECREF(selfkey);\n",
      "825             return NULL;\n",
      "826         }\n",
      "827         Py_DECREF(selfkey);\n",
      "828         return rval;\n",
      "829     }\n",
      "830 }\n",
      "831 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
      "832 {\n",
      "833     if (!PySequence_Check(py_reduce_mask))\n",
      "834     {\n",
      "835         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
      "836         return NULL;\n",
      "837     }\n",
      "838     int len = PySequence_Length(py_reduce_mask);\n",
      "839     if (len != self->nd)\n",
      "840     {\n",
      "841         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
      "842         return NULL;\n",
      "843     }\n",
      "844     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
      "845     if (!self_sum)\n",
      "846     {\n",
      "847         return NULL;\n",
      "848     }\n",
      "849     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
      "850     //      and use it if it is big enough.\n",
      "851     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
      "852     int * sum_dims = dimshuffle_pattern + len;\n",
      "853     int n_remaining_dims = 0;\n",
      "854     if (!dimshuffle_pattern)\n",
      "855     {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "856         Py_DECREF(self_sum);\n",
      "857         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
      "858         return NULL;\n",
      "859     }\n",
      "860     for (int i = 0; i < len; ++i)\n",
      "861     {\n",
      "862         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
      "863         int o_i_int = PyInt_AsLong(o_i);\n",
      "864         Py_XDECREF(o_i);\n",
      "865         if (PyErr_Occurred())\n",
      "866         {\n",
      "867             Py_DECREF(self_sum);\n",
      "868             free(dimshuffle_pattern);\n",
      "869             return NULL;\n",
      "870         }\n",
      "871         if (o_i_int) // this is a dimension over which we are reducing\n",
      "872         {\n",
      "873             sum_dims[i] = 1;\n",
      "874         }\n",
      "875         else\n",
      "876         {\n",
      "877             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "878             dimshuffle_pattern[n_remaining_dims++] = i;\n",
      "879         }\n",
      "880     }\n",
      "881     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
      "882             || CudaNdarray_reduce_sum(self_sum, self)\n",
      "883             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
      "884     {\n",
      "885         Py_DECREF(self_sum);\n",
      "886         free(dimshuffle_pattern);\n",
      "887         return NULL;\n",
      "888     }\n",
      "889     free(dimshuffle_pattern);\n",
      "890     return (PyObject*)self_sum;\n",
      "891 }\n",
      "892 \n",
      "893 // Reshape self to the new shape gived by the tuple shape.\n",
      "894 //\n",
      "895 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
      "896 // TODO: make it return a view when the strides allow it even if it is not\n",
      "897 //       c contiguous\n",
      "898 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
      "899 {\n",
      "900     if(!CudaNdarray_is_c_contiguous(self))\n",
      "901     {\n",
      "902         // allocate new space\n",
      "903         //TODO: test to see if we can re-use old one and take a new param to\n",
      "904         //  use this\n",
      "905         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
      "906         if (!rval)\n",
      "907         {\n",
      "908             return NULL;\n",
      "909         }\n",
      "910 \n",
      "911         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
      "912         Py_XDECREF(rval);\n",
      "913         return (PyObject*)ret;\n",
      "914     }\n",
      "915 \n",
      "916     // check shape tuple\n",
      "917     unsigned int rval_nd;\n",
      "918     unsigned int * rval_dims;\n",
      "919     size_t rval_size = 1;\n",
      "920 \n",
      "921     if (PyTuple_Check(shape)){\n",
      "922         // copy shape to integer array\n",
      "923         rval_nd = PyTuple_Size(shape);\n",
      "924     }else if (PyInt_Check(shape)){\n",
      "925         rval_nd = 1;\n",
      "926     }else{\n",
      "927         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
      "928         return NULL;\n",
      "929     }\n",
      "930     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
      "931 \n",
      "932     if(PyTuple_Check(shape)){\n",
      "933         for (int i = 0; i < rval_nd; ++i)\n",
      "934         {\n",
      "935             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
      "936             if (PyErr_Occurred()) //error in AsLong\n",
      "937             {\n",
      "938                 free(rval_dims);\n",
      "939                 return NULL;\n",
      "940             }\n",
      "941             if(rval_dims[i]<0){\n",
      "942                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >=0)\",rval_dims[i]);\n",
      "943                 free(rval_dims);\n",
      "944                 return NULL;\n",
      "945             }\n",
      "946             rval_size = rval_size * rval_dims[i];\n",
      "947         }\n",
      "948     }else{\n",
      "949         rval_size = PyInt_AsLong(shape);\n",
      "950         rval_dims[0] = rval_size;\n",
      "951     }\n",
      "952     // calculate new size, assert same as old size\n",
      "953     if (rval_size != CudaNdarray_SIZE(self))\n",
      "954     {\n",
      "955         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %lld to %lld\", CudaNdarray_SIZE(self), rval_size);\n",
      "956         free(rval_dims);\n",
      "957         return NULL;\n",
      "958     }\n",
      "959     if (rval_size==0)\n",
      "960     {\n",
      "961         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
      "962         free(rval_dims);\n",
      "963         return rval;\n",
      "964     }\n",
      "965 \n",
      "966     //return a view, not a copy\n",
      "967     //we can do this as we checked self is c_contiguous\n",
      "968     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
      "969 \n",
      "970     if (!rval || 0 != rval->data_allocated\n",
      "971         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "972     {\n",
      "973         Py_XDECREF(rval);\n",
      "974         free(rval_dims);\n",
      "975         return NULL;\n",
      "976     }\n",
      "977     //set dim and stride\n",
      "978     int size = 1;\n",
      "979     for (int i = rval_nd-1; i >= 0; --i)\n",
      "980     {\n",
      "981         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
      "982         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
      "983         size = size * rval_dims[i];\n",
      "984     }\n",
      "985     free(rval_dims);\n",
      "986     return (PyObject*)rval;\n",
      "987 }\n",
      "988 \n",
      "989 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
      "990 {\n",
      "991     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
      "992     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "993     {\n",
      "994         Py_XDECREF(rval);\n",
      "995         rval = NULL;\n",
      "996     }\n",
      "997     else\n",
      "998     {\n",
      "999         for (int i = 0; i < self->nd; ++i)\n",
      "1000         {\n",
      "1001             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
      "1002             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "1003         }\n",
      "1004     }\n",
      "1005     return (PyObject*)rval;\n",
      "1006 }\n",
      "1007 \n",
      "1008 /*\n",
      "1009  * d0,... are the output dims\n",
      "1010  * indices are a list of index to operate on\n",
      "1011  *         They are int32 viewed as float32.\n",
      "1012  * a is the output\n",
      "1013  * b is the input\n",
      "1014  * dB0, the source leading dimensions size\n",
      "1015  */\n",
      "1016 template <int operator_num>\n",
      "1017 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
      "1018                          const npy_int64* indices,\n",
      "1019                          float* a,\n",
      "1020                          const int sA0, const int sA1, const int sA2,\n",
      "1021                          const float* b, const int dB0,\n",
      "1022                          const int sB0, const int sB1, const int sB2,\n",
      "1023                          int* err){\n",
      "1024     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1025         npy_int64 idx = indices[i0];\n",
      "1026         if (idx<0)\n",
      "1027             idx += dB0; // To allow negative indexing.\n",
      "1028         if ((idx < 0) || (idx >= dB0)){\n",
      "1029             // Any value other the 0 probably work. But to be more safe, I want\n",
      "1030             // to change all bits to prevent problem with concurrent write that\n",
      "1031             // could cross cache line. But this should not happen with the\n",
      "1032             // current code and driver.\n",
      "1033             *err = 0xFFFF;\n",
      "1034             continue;\n",
      "1035         }\n",
      "1036         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
      "1037             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
      "1038                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
      "1039                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
      "1040                 a[a_idx] = b[b_idx];\n",
      "1041             }\n",
      "1042         }\n",
      "1043     }\n",
      "1044 }\n",
      "1045 \n",
      "1046 // We try to be similar to the PyArray_TakeFrom function\n",
      "1047 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
      "1048 //TODO: support other clip mode then raise(clip, wrap)\n",
      "1049 //self is the input that we copy data from.\n",
      "1050 //The indices that we receive MUST be an CudaNdarray(float32)\n",
      "1051 //    that is in fact a view to int64 indices\n",
      "1052 PyObject*\n",
      "1053 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
      "1054     int verbose = 0;\n",
      "1055     PyObject * indices_obj = NULL;\n",
      "1056     //int axis; Default None, that mean the flattened array.\n",
      "1057     PyObject * axis_obj = Py_None;\n",
      "1058     PyObject * out_obj = Py_None;\n",
      "1059     PyObject * clipmode_obj = NULL;\n",
      "1060     int max_threads = 1; // max threads per blocks\n",
      "1061 \n",
      "1062     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
      "1063                            &out_obj, &clipmode_obj, &max_threads))\n",
      "1064         return NULL;\n",
      "1065 \n",
      "1066     //Check argument indices\n",
      "1067     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
      "1068     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
      "1069     //TODO: Support ndarray of other dtype then int32\n",
      "1070     //TODO: support list of indices that are not c_contiguous\n",
      "1071     CudaNdarray * indices = NULL;\n",
      "1072     if (CudaNdarray_Check(indices_obj)) {\n",
      "1073         if (verbose) printf(\"cudandarray indices\\n\");\n",
      "1074         indices = (CudaNdarray*) indices_obj;\n",
      "1075         Py_INCREF(indices);\n",
      "1076     } else if (PyArray_Check(indices_obj)) {\n",
      "1077         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1078         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT64) {\n",
      "1079             PyErr_SetString(PyExc_TypeError,\n",
      "1080                             \"CudaNdarray_TakeFrom: need a ndarray for indices\"\n",
      "1081                             \" with dtype int64\");\n",
      "1082             return NULL;\n",
      "1083         }\n",
      "1084         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
      "1085             PyErr_SetString(PyExc_TypeError,\n",
      "1086                             \"CudaNdarray_TakeFrom: need a CudaNdarray of\"\n",
      "1087                             \" indices with only 1 dimensions\");\n",
      "1088             return NULL;\n",
      "1089         }\n",
      "1090         // We need indices_obj to be contiguous, in order to take a view\n",
      "1091         // with a different dtype.\n",
      "1092         if (!PyArray_IS_C_CONTIGUOUS((PyArrayObject*) indices_obj)) {\n",
      "1093             PyObject* indices_obj_contig = PyArray_NewCopy((PyArrayObject*) indices_obj, NPY_CORDER);\n",
      "1094             if (!indices_obj_contig)\n",
      "1095                 return NULL;\n",
      "1096             indices_obj = indices_obj_contig;\n",
      "1097         } else {\n",
      "1098             // Keep the refcount consistent\n",
      "1099             Py_INCREF(indices_obj);\n",
      "1100         }\n",
      "1101         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
      "1102         PyObject * indices_float32 = NULL;\n",
      "1103         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
      "1104                                                   float32_descr, NULL);\n",
      "1105         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1106         if (!indices_float32) {\n",
      "1107             Py_DECREF(indices_obj);\n",
      "1108             return NULL;\n",
      "1109         }\n",
      "1110 \n",
      "1111         indices = (CudaNdarray*) CudaNdarray_New();\n",
      "1112         if (verbose) printf(\"\\nndarray after new\\n\");\n",
      "1113         if (! indices){\n",
      "1114             Py_DECREF(indices_obj);\n",
      "1115             Py_DECREF(indices_float32);\n",
      "1116             return NULL;\n",
      "1117         }\n",
      "1118         if (CudaNdarray_CopyFromArray(indices,\n",
      "1119                                       (PyArrayObject *)indices_float32)){\n",
      "1120             Py_DECREF(indices_obj);\n",
      "1121             Py_DECREF(indices_float32);\n",
      "1122             return NULL;\n",
      "1123         }\n",
      "1124         Py_DECREF(indices_obj);\n",
      "1125         Py_DECREF(indices_float32);\n",
      "1126     } else {\n",
      "1127         PyObject* py_s = PyObject_Str(indices_obj);\n",
      "1128         const char* s = PyString_AsString(py_s);\n",
      "1129         Py_DECREF(py_s);\n",
      "1130         PyErr_Format(PyExc_TypeError,\n",
      "1131                      \"CudaNdarray_TakeFrom: need an ndarray of int64 or a\"\n",
      "1132                      \" CudaNdarray(float32) that is a view from int64 data\"\n",
      "1133                      \" for indices. Got %s\", s);\n",
      "1134         return NULL;\n",
      "1135     }\n",
      "1136 \n",
      "1137     if (verbose) {\n",
      "1138         printf(\"indices used on the gpu\\n\");\n",
      "1139         fprint_CudaNdarray(stdout, indices);\n",
      "1140         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
      "1141         PyObject_Print(used_indices, stdout, 0);\n",
      "1142         Py_DECREF(used_indices);\n",
      "1143     }\n",
      "1144     if (verbose) printf(\"after print of object\\n\");\n",
      "1145     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
      "1146         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1147                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
      "1148         Py_DECREF(indices);\n",
      "1149         return NULL;\n",
      "1150     }\n",
      "1151     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
      "1152 \n",
      "1153     //Check argument axis\n",
      "1154     //TODO: implement the default and other axis\n",
      "1155     long axis = PyInt_AsLong(axis_obj);\n",
      "1156 \n",
      "1157     if (axis != 0) {\n",
      "1158         PyErr_Format(PyExc_NotImplementedError,\n",
      "1159                      \"CudaNdarray_TakeFrom: only axis=0 is currently supported.\"\n",
      "1160                      \" Got %ld.\", axis);\n",
      "1161         Py_DECREF(indices);\n",
      "1162         return NULL;\n",
      "1163     }\n",
      "1164 \n",
      "1165     //Check argument out_obj\n",
      "1166     CudaNdarray * out = NULL;\n",
      "1167     if (out_obj && CudaNdarray_Check(out_obj))\n",
      "1168         out = (CudaNdarray*) out_obj;\n",
      "1169     if (out && (out->nd != self->nd ||\n",
      "1170                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
      "1171         out = NULL;\n",
      "1172     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
      "1173     dims[0] = nb_indices;\n",
      "1174 \n",
      "1175     for (int i=1 ; i<self->nd ; i++) {\n",
      "1176         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "1177         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
      "1178             out = NULL;\n",
      "1179         }\n",
      "1180     }\n",
      "1181     if (!out) {\n",
      "1182         out = (CudaNdarray*)CudaNdarray_New();\n",
      "1183         if (!out){\n",
      "1184             Py_DECREF(indices);\n",
      "1185             free(dims);\n",
      "1186             return NULL;\n",
      "1187         }\n",
      "1188         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
      "1189             Py_DECREF(out);\n",
      "1190             Py_DECREF(indices);\n",
      "1191             free(dims);\n",
      "1192             return NULL;\n",
      "1193         }\n",
      "1194     }else {\n",
      "1195         Py_INCREF(out);\n",
      "1196     }\n",
      "1197 \n",
      "1198     //Check argument clipmode\n",
      "1199     if (clipmode_obj) {\n",
      "1200         char * clipmode = PyString_AsString(clipmode_obj);\n",
      "1201         if (! clipmode){\n",
      "1202             Py_DECREF(indices);\n",
      "1203             Py_DECREF(out);\n",
      "1204             free(dims);\n",
      "1205             return NULL;\n",
      "1206         }\n",
      "1207         if (strcmp(clipmode, \"raise\") != 0) {\n",
      "1208             PyErr_Format(PyExc_NotImplementedError,\n",
      "1209                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
      "1210                          clipmode);\n",
      "1211             Py_DECREF(indices);\n",
      "1212             Py_DECREF(out);\n",
      "1213             free(dims);\n",
      "1214             return NULL;\n",
      "1215         }\n",
      "1216     }\n",
      "1217     void (*k3)(const int, const int, const int,\n",
      "1218                const npy_int64*,\n",
      "1219                float*, const int, const int, const int,\n",
      "1220                const float*, const int,\n",
      "1221                const int, const int, const int,\n",
      "1222                int*);\n",
      "1223     k3 = k_take_3<CPY>;\n",
      "1224 \n",
      "1225     // Create the memory place that will store the error information.\n",
      "1226     if(init_err_var() != 0) return NULL;\n",
      "1227 \n",
      "1228     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
      "1229     if(CudaNdarray_HOST_DIMS(out)[0] == 0){\n",
      "1230         // We take 0 elements, so no need for the rest of the code.\n",
      "1231         // This speed up that case AND fix crash otherwise.\n",
      "1232         free(dims);\n",
      "1233         Py_DECREF(indices);\n",
      "1234         return (PyObject *)out;\n",
      "1235     }\n",
      "1236 \n",
      "1237     switch (self->nd) {\n",
      "1238         case 1:\n",
      "1239             {\n",
      "1240                 dim3 n_threads(1, 1, 1);\n",
      "1241                 if (verbose)\n",
      "1242                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1243                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1244                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1245                            cudaGetLastError(), self->nd,\n",
      "1246                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1247                 k3<<<n_blocks, n_threads>>>(\n",
      "1248                         dims[0],\n",
      "1249                         1,\n",
      "1250                         1,\n",
      "1251                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1252                         CudaNdarray_DEV_DATA(out),\n",
      "1253                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1254                         1,\n",
      "1255                         1,\n",
      "1256                         CudaNdarray_DEV_DATA(self),\n",
      "1257                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1258                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1259                         1,\n",
      "1260                         1,\n",
      "1261                         err_var);\n",
      "1262             }\n",
      "1263             break;\n",
      "1264         case 2:\n",
      "1265             {\n",
      "1266                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
      "1267 \n",
      "1268                 if (verbose)\n",
      "1269                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1270                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1271                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1272                            cudaGetLastError(), self->nd,\n",
      "1273                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1274 \n",
      "1275                 k3<<<n_blocks, n_threads>>>(\n",
      "1276                         dims[0], //dimensions\n",
      "1277                         dims[1],\n",
      "1278                         1,\n",
      "1279                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1280                         CudaNdarray_DEV_DATA(out),\n",
      "1281                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1282                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1283                         1,\n",
      "1284                         CudaNdarray_DEV_DATA(self),\n",
      "1285                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1286                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1287                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1288                         1,\n",
      "1289                         err_var);\n",
      "1290             }\n",
      "1291             break;\n",
      "1292         case 3:\n",
      "1293             {\n",
      "1294                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
      "1295                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
      "1296                 dim3 n_threads(tx, ty, 1);\n",
      "1297                 if (verbose)\n",
      "1298                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1299                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1300                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1301                            cudaGetLastError(), self->nd,\n",
      "1302                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1303                 k3<<<n_blocks, n_threads>>>(\n",
      "1304                         dims[0], //dimensions\n",
      "1305                         dims[1],\n",
      "1306                         dims[2],\n",
      "1307                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1308                         CudaNdarray_DEV_DATA(out),\n",
      "1309                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1310                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1311                         CudaNdarray_HOST_STRIDES(out)[2],\n",
      "1312                         CudaNdarray_DEV_DATA(self),\n",
      "1313                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1314                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1315                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1316                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1317                         err_var);\n",
      "1318             }\n",
      "1319             break;\n",
      "1320     default:\n",
      "1321         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1322                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
      "1323                         \" dimensions are currently supported\");\n",
      "1324 \n",
      "1325     }\n",
      "1326     free(dims);\n",
      "1327     CNDA_THREAD_SYNC;\n",
      "1328     cudaError_t err = cudaGetLastError();\n",
      "1329     if (cudaSuccess != err) {\n",
      "1330         PyErr_Format(PyExc_RuntimeError,\n",
      "1331                      \"Cuda error: %s: %s.\\n\",\n",
      "1332                      \"CudaNdarray_TakeFrom\",\n",
      "1333                      cudaGetErrorString(err));\n",
      "1334         Py_DECREF(indices);\n",
      "1335         Py_DECREF(out);\n",
      "1336         return NULL;\n",
      "1337     }\n",
      "1338 \n",
      "1339     int index_err = check_err_var();\n",
      "1340     Py_DECREF(indices);\n",
      "1341     if (index_err != 0) {\n",
      "1342         Py_DECREF(out);\n",
      "1343         return NULL;\n",
      "1344     }\n",
      "1345 \n",
      "1346     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
      "1347     return (PyObject *)out;\n",
      "1348 }\n",
      "1349 \n",
      "1350 \n",
      "1351 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
      "1352 {\n",
      "1353     int pos, stride;\n",
      "1354     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
      "1355         return NULL;\n",
      "1356     if ((pos < 0) || (pos >= self->nd))\n",
      "1357     {\n",
      "1358         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1359         return NULL;\n",
      "1360     }\n",
      "1361     CudaNdarray_set_stride(self, pos, stride);\n",
      "1362     if (cnda_copy_structure_to_device(self))\n",
      "1363     {\n",
      "1364         return NULL;\n",
      "1365     }\n",
      "1366     Py_INCREF(Py_None);\n",
      "1367     return Py_None;\n",
      "1368 }\n",
      "1369 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
      "1370 {\n",
      "1371     int pos, dim;\n",
      "1372     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
      "1373         return NULL;\n",
      "1374     if ((pos < 0) || (pos >= self->nd))\n",
      "1375     {\n",
      "1376         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1377         return NULL;\n",
      "1378     }\n",
      "1379     CudaNdarray_set_dim(self, pos, dim);\n",
      "1380     if (cnda_copy_structure_to_device(self))\n",
      "1381     {\n",
      "1382         return NULL;\n",
      "1383     }\n",
      "1384     Py_INCREF(Py_None);\n",
      "1385     return Py_None;\n",
      "1386 }\n",
      "1387 \n",
      "1388 static PyObject *\n",
      "1389 CudaNdarray_exp(CudaNdarray* self)\n",
      "1390 {\n",
      "1391     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1392     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1393     {\n",
      "1394         Py_XDECREF(rval);\n",
      "1395         return NULL;\n",
      "1396     }\n",
      "1397     unsigned int size = 1;\n",
      "1398     for (int i = 0; i < self->nd; i++)\n",
      "1399     {\n",
      "1400         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1401     }\n",
      "1402     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1403     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1404     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
      "1405             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
      "1406             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
      "1407 \n",
      "1408     //TODO: don't do this right away, do it when we need the result\n",
      "1409     CNDA_THREAD_SYNC;\n",
      "1410     cudaError_t err = cudaGetLastError();\n",
      "1411     if( cudaSuccess != err)\n",
      "1412     {\n",
      "1413         Py_DECREF(rval);\n",
      "1414         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
      "1415         return NULL;\n",
      "1416     }\n",
      "1417 \n",
      "1418     return (PyObject*)rval;\n",
      "1419 }\n",
      "1420 \n",
      "1421 static PyMethodDef CudaNdarray_methods[] =\n",
      "1422 {\n",
      "1423     {\"__array__\",\n",
      "1424         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
      "1425         \"Copy from the device to a numpy ndarray\"},\n",
      "1426     {\"__copy__\",\n",
      "1427         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1428         \"Create a shallow copy of this object. used by module copy\"},\n",
      "1429     {\"__deepcopy__\",\n",
      "1430         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
      "1431         \"Create a copy of this object\"},\n",
      "1432     {\"zeros\",\n",
      "1433         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
      "1434         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
      "1435     {\"copy\",\n",
      "1436         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
      "1437         \"Create a copy of this object\"},\n",
      "1438     {\"is_c_contiguous\",\n",
      "1439         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
      "1440         \"Return True is the object is c contiguous. False otherwise.\"},\n",
      "1441     {\"reduce_sum\",\n",
      "1442         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
      "1443         \"Reduce over the given dimensions by summation\"},\n",
      "1444     {\"exp\",\n",
      "1445         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
      "1446         \"Return the exponential of all elements\"},\n",
      "1447     {\"reshape\",\n",
      "1448         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
      "1449         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
      "1450             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
      "1451     {\"view\",\n",
      "1452         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1453         \"Return an alias of this ndarray\"},\n",
      "1454     {\"_set_stride\",\n",
      "1455         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
      "1456         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
      "1457     {\"take\",\n",
      "1458         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
      "1459         \"Equivalent of numpy.take\"},\n",
      "1460     {\"_set_shape_i\",\n",
      "1461         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
      "1462         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
      "1463     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "1464 };\n",
      "1465 \n",
      "1466 \n",
      "1467 ////////////////////\n",
      "1468 // Number protocol\n",
      "1469 ////////////////////\n",
      "1470 \n",
      "1471 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
      "1472     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "1473     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "1474 \n",
      "1475     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
      "1476         dest[i] = a[i] + b[i];\n",
      "1477     }\n",
      "1478 }\n",
      "1479 \n",
      "1480 // Will be called by __add__ in Python\n",
      "1481 static PyObject *\n",
      "1482 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
      "1483 {\n",
      "1484     if (! CudaNdarray_Check(py_self)) {\n",
      "1485         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
      "1486         return NULL;\n",
      "1487     }\n",
      "1488     if (! CudaNdarray_Check(py_other)) {\n",
      "1489         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
      "1490         return NULL;\n",
      "1491     }\n",
      "1492     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1493     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1494     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
      "1495         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
      "1496         return NULL;\n",
      "1497     }\n",
      "1498 \n",
      "1499     //standard elemwise size checks\n",
      "1500     if (self->nd != other->nd)\n",
      "1501     {\n",
      "1502         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
      "1503         return NULL;\n",
      "1504     }\n",
      "1505     //standard elemwise dim checks\n",
      "1506     unsigned int size = 1;\n",
      "1507     for (int i = 0; i< self->nd; ++i)\n",
      "1508     {\n",
      "1509         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "1510         {\n",
      "1511             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
      "1512             return NULL;\n",
      "1513         }\n",
      "1514         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1515     }\n",
      "1516     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1517     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1518     {\n",
      "1519         Py_XDECREF(rval);\n",
      "1520         return NULL;\n",
      "1521     }\n",
      "1522 \n",
      "1523     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
      "1524       return (PyObject *) rval;\n",
      "1525     }\n",
      "1526 \n",
      "1527     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1528     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1529     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
      "1530             self->devdata, other->devdata, rval->devdata, size);\n",
      "1531     CNDA_THREAD_SYNC;\n",
      "1532     cudaError_t err = cudaGetLastError();\n",
      "1533     if( cudaSuccess != err)\n",
      "1534     {\n",
      "1535         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
      "1536         Py_DECREF(rval);\n",
      "1537         return NULL;\n",
      "1538     }\n",
      "1539     return (PyObject *) rval;\n",
      "1540 }\n",
      "1541 \n",
      "1542 template <int operator_num>\n",
      "1543 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
      "1544         float* a, const int sA0, const int sA1, const int sA2,\n",
      "1545         const float* b, const int sB0, const int sB1, const int sB2){\n",
      "1546     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1547         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1548             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1549                 switch (operator_num)\n",
      "1550                 {\n",
      "1551                   case IADD:\n",
      "1552                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1553                     break;\n",
      "1554                   case IDIV:\n",
      "1555                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1556                     break;\n",
      "1557                   case CPY:\n",
      "1558                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1559                     break;\n",
      "1560                 }\n",
      "1561             }\n",
      "1562         }\n",
      "1563     }\n",
      "1564 }\n",
      "1565 \n",
      "1566 template <int operator_num>\n",
      "1567 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
      "1568                          float* a, const int sA0, const int sA1,\n",
      "1569                          const int sA2, const int sA3,\n",
      "1570                          const float* b, const int sB0, const int sB1,\n",
      "1571                          const int sB2, const int sB3){\n",
      "1572     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1573         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1574             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1575                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
      "1576                     switch (operator_num) {\n",
      "1577                         case IADD:\n",
      "1578                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1579                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1580                             break;\n",
      "1581                         case IDIV:\n",
      "1582                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1583                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1584                             break;\n",
      "1585                         case CPY:\n",
      "1586                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1587                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1588                             break;\n",
      "1589                     }\n",
      "1590                 }\n",
      "1591             }\n",
      "1592         }\n",
      "1593     }\n",
      "1594 }\n",
      "1595 \n",
      "1596 template <int operator_num>\n",
      "1597 __global__ void k_ielem_6(const int d0, const int d1,\n",
      "1598                           const int d2, const int d3,\n",
      "1599                           const int d4, const int d5,\n",
      "1600                           float* a, const int sA0, const int sA1,\n",
      "1601                           const int sA2, const int sA3,\n",
      "1602                           const int sA4, const int sA5,\n",
      "1603                           const float* b, const int sB0, const int sB1,\n",
      "1604                           const int sB2, const int sB3,\n",
      "1605                           const int sB4, const int sB5\n",
      "1606                           ){\n",
      "1607     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1608         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1609             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
      "1610                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
      "1611                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
      "1612                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
      "1613                             switch (operator_num) {\n",
      "1614                             case IADD:\n",
      "1615                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1616                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1617                                 break;\n",
      "1618                             case IDIV:\n",
      "1619                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1620                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1621                                 break;\n",
      "1622                             case CPY:\n",
      "1623                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1624                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1625                                 break;\n",
      "1626                             }\n",
      "1627                         }\n",
      "1628                     }\n",
      "1629                 }\n",
      "1630             }\n",
      "1631         }\n",
      "1632     }\n",
      "1633 }\n",
      "1634 \n",
      "1635 /*\n",
      "1636 CudaNdarray_inplace_elemwise\n",
      "1637 Compute elemwise, working inplace on A.\n",
      "1638 Currently implemented A / B, A + B and A = B\n",
      "1639 (the last is not tested and not used!)\n",
      "1640 \n",
      "1641 py_self - the CudaNdarray that we'll modify (A)\n",
      "1642 py_other - the other argument (B)\n",
      "1643 fct_nb - which operation to perform (operator_t)\n",
      "1644 \n",
      "1645 Returns 0 on success.\n",
      "1646 Returns -1 on failure, and sets Python exception.\n",
      "1647 \n",
      "1648 */\n",
      "1649 int\n",
      "1650 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
      "1651 {\n",
      "1652     int verbose = 0;\n",
      "1653     void (*k3)(const int, const int, const int,\n",
      "1654                     float*, const int, const int, const int,\n",
      "1655                     const float*, const int, const int, const int);\n",
      "1656     void (*k4)(const int, const int, const int, const int,\n",
      "1657                     float*, const int, const int,\n",
      "1658                     const int, const int,\n",
      "1659                     const float*, const int, const int,\n",
      "1660                     const int, const int);\n",
      "1661     void (*k6)(const int, const int,\n",
      "1662                const int, const int,\n",
      "1663                const int, const int,\n",
      "1664                float*, const int, const int,\n",
      "1665                const int, const int,\n",
      "1666                const int, const int,\n",
      "1667                const float*, const int, const int,\n",
      "1668                const int, const int,\n",
      "1669                const int, const int);\n",
      "1670     switch (fct_nb)\n",
      "1671     {\n",
      "1672         case IADD:\n",
      "1673             k3 = k_ielem_3<IADD>;\n",
      "1674             k4 = k_ielem_4<IADD>;\n",
      "1675             k6 = k_ielem_6<IADD>;\n",
      "1676             break;\n",
      "1677         case IDIV:\n",
      "1678             k3 = k_ielem_3<IDIV>;\n",
      "1679             k4 = k_ielem_4<IDIV>;\n",
      "1680             k6 = k_ielem_6<IDIV>;\n",
      "1681             break;\n",
      "1682         case CPY:\n",
      "1683             k3 = k_ielem_3<CPY>;\n",
      "1684             k4 = k_ielem_4<CPY>;\n",
      "1685             k6 = k_ielem_6<CPY>;\n",
      "1686             break;\n",
      "1687         default:\n",
      "1688             assert (0);\n",
      "1689             PyErr_Format(\n",
      "1690                 PyExc_TypeError,\n",
      "1691                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
      "1692                 (int)fct_nb);\n",
      "1693             return -1;\n",
      "1694     }\n",
      "1695     if (!CudaNdarray_Check(py_self)) {\n",
      "1696         PyErr_SetString(\n",
      "1697             PyExc_TypeError,\n",
      "1698             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
      "1699         return -1;\n",
      "1700     }\n",
      "1701     CudaNdarray * new_other = NULL;\n",
      "1702     if (!CudaNdarray_Check(py_other)) {\n",
      "1703         new_other = (CudaNdarray*) CudaNdarray_New();\n",
      "1704         if(!new_other)\n",
      "1705         {\n",
      "1706             return -1;\n",
      "1707         }\n",
      "1708         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
      "1709         {\n",
      "1710             Py_XDECREF(new_other);\n",
      "1711             return -1;\n",
      "1712         }\n",
      "1713         py_other = (PyObject *) new_other;\n",
      "1714     }\n",
      "1715 \n",
      "1716     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1717     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1718 \n",
      "1719     if (verbose)\n",
      "1720     {\n",
      "1721         fprintf(stderr,\n",
      "1722             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
      "1723             self->nd, other->nd);\n",
      "1724     }\n",
      "1725 \n",
      "1726     //standard elemwise nb dim checks\n",
      "1727     if (self->nd < other->nd)\n",
      "1728     {\n",
      "1729         PyErr_Format(\n",
      "1730             PyExc_TypeError,\n",
      "1731             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
      "1732             \" same number of dimensions then the source. Got %d and %d.\",\n",
      "1733             self->nd, other->nd);\n",
      "1734         Py_XDECREF(new_other);\n",
      "1735         return -1;\n",
      "1736     }\n",
      "1737 \n",
      "1738     //broadcast to the same number of dimensions.\n",
      "1739     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
      "1740     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
      "1741     int added_dims = self->nd - other->nd;\n",
      "1742     // Add the added broadcasted dimensions\n",
      "1743     for (int i = 0; i< added_dims; ++i)\n",
      "1744     {\n",
      "1745         other_dims[i] = 1;\n",
      "1746         other_strides[i] = 0;\n",
      "1747     }\n",
      "1748     // Copy the existing dimensions\n",
      "1749     for (int i = 0; i< other->nd; ++i)\n",
      "1750     {\n",
      "1751         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
      "1752         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
      "1753     }\n",
      "1754 \n",
      "1755     //standard elemwise dim checks\n",
      "1756     unsigned int size = 1;\n",
      "1757     for (int i = 0; i< self->nd; ++i)\n",
      "1758     {\n",
      "1759         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
      "1760             && (other_dims[i] != 1))\n",
      "1761         {\n",
      "1762             PyErr_SetString(\n",
      "1763                 PyExc_ValueError,\n",
      "1764                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
      "1765             Py_XDECREF(new_other);\n",
      "1766             return -1;\n",
      "1767         }\n",
      "1768         // if we're broadcasting other, then make sure it has stride 0\n",
      "1769         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
      "1770             || (other_strides[i] == 0));\n",
      "1771         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1772     }\n",
      "1773 \n",
      "1774     if (size==0)\n",
      "1775     {\n",
      "1776         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
      "1777         if (!(other_size == 0 || other_size == 1))\n",
      "1778         {\n",
      "1779             PyErr_SetString(\n",
      "1780                 PyExc_ValueError,\n",
      "1781                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
      "1782                 \" un-initialized array when the new value have more than\"\n",
      "1783                 \" 0 or 1 broadcastable dimensions\");\n",
      "1784             Py_XDECREF(new_other);\n",
      "1785             return 0;\n",
      "1786         }\n",
      "1787         Py_XDECREF(new_other);\n",
      "1788         return 0;\n",
      "1789     }\n",
      "1790 \n",
      "1791     switch(self->nd)\n",
      "1792     {\n",
      "1793         case 0:\n",
      "1794             {\n",
      "1795                 dim3 n_blocks(1, 1, 1);\n",
      "1796                 dim3 n_threads(1);\n",
      "1797                 k3<<<n_blocks, n_threads>>>(\n",
      "1798                         1, //d0\n",
      "1799                         1, //d1\n",
      "1800                         1, //d2\n",
      "1801                         CudaNdarray_DEV_DATA(self),\n",
      "1802                         1, //strides\n",
      "1803                         1,\n",
      "1804                         1,\n",
      "1805                         CudaNdarray_DEV_DATA(other),\n",
      "1806                         1, //strides\n",
      "1807                         1,\n",
      "1808                         1);\n",
      "1809                 CNDA_THREAD_SYNC;\n",
      "1810                 cudaError_t err = cudaGetLastError();\n",
      "1811                 if (cudaSuccess != err)\n",
      "1812                 {\n",
      "1813                     PyErr_Format(\n",
      "1814                         PyExc_RuntimeError,\n",
      "1815                         \"CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\\n\",\n",
      "1816                         \"k3\",\n",
      "1817                         cudaGetErrorString(err));\n",
      "1818                     Py_XDECREF(new_other);\n",
      "1819                     return -1;\n",
      "1820                 }\n",
      "1821             }\n",
      "1822             break;\n",
      "1823         case 1:\n",
      "1824             {\n",
      "1825                 dim3 n_blocks(1, 1, 1);\n",
      "1826                 dim3 n_threads(\n",
      "1827                         std::min(\n",
      "1828                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1829                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1830                 k3<<<n_blocks, n_threads>>>(\n",
      "1831                         1, //dimensions\n",
      "1832                         1,\n",
      "1833                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1834                         CudaNdarray_DEV_DATA(self),\n",
      "1835                         1, //strides\n",
      "1836                         1,\n",
      "1837                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1838                         CudaNdarray_DEV_DATA(other),\n",
      "1839                         1, //strides\n",
      "1840                         1,\n",
      "1841                         other_strides[0]);\n",
      "1842                 CNDA_THREAD_SYNC;\n",
      "1843                 cudaError_t err = cudaGetLastError();\n",
      "1844                 if (cudaSuccess != err)\n",
      "1845                 {\n",
      "1846                     PyErr_Format(\n",
      "1847                         PyExc_RuntimeError,\n",
      "1848                         \"CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\\n\",\n",
      "1849                         \"k3\",\n",
      "1850                         cudaGetErrorString(err));\n",
      "1851                     Py_XDECREF(new_other);\n",
      "1852                     return -1;\n",
      "1853                 }\n",
      "1854             }\n",
      "1855             break;\n",
      "1856         case 2:\n",
      "1857             {\n",
      "1858                 //TODO:  if both self and other are f-contiguous\n",
      "1859                 //       Then flip the block and thread dimensions\n",
      "1860                 //       to make contiguous reads & writes\n",
      "1861                 dim3 n_blocks(1,\n",
      "1862                         std::min(\n",
      "1863                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1864                             NUM_VECTOR_OP_BLOCKS));\n",
      "1865                 dim3 n_threads(\n",
      "1866                         std::min(\n",
      "1867                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1868                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1869                 k3<<<n_blocks, n_threads>>>(1,\n",
      "1870                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1871                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1872                         CudaNdarray_DEV_DATA(self),\n",
      "1873                         1,\n",
      "1874                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1875                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1876                         CudaNdarray_DEV_DATA(other),\n",
      "1877                         1,\n",
      "1878                         other_strides[0],\n",
      "1879                         other_strides[1]);\n",
      "1880                 CNDA_THREAD_SYNC;\n",
      "1881                 cudaError_t err = cudaGetLastError();\n",
      "1882                 if (cudaSuccess != err)\n",
      "1883                 {\n",
      "1884                     PyErr_Format(\n",
      "1885                         PyExc_RuntimeError,\n",
      "1886                         \"CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\\n\",\n",
      "1887                         \"k3\",\n",
      "1888                         cudaGetErrorString(err));\n",
      "1889                     Py_XDECREF(new_other);\n",
      "1890                     return -1;\n",
      "1891                 }\n",
      "1892             }\n",
      "1893             break;\n",
      "1894         case 3:\n",
      "1895             {\n",
      "1896                 //TODO:  Dimshuffle so that at least one of the arrays\n",
      "1897                 //       has a contiguous dimension on the thread idx.\n",
      "1898                 dim3 n_blocks(\n",
      "1899                         std::min(\n",
      "1900                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1901                             NUM_VECTOR_OP_BLOCKS),\n",
      "1902                         CudaNdarray_HOST_DIMS(self)[1]);\n",
      "1903                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1904                     n_blocks.y /= 2;\n",
      "1905                 dim3 n_threads(\n",
      "1906                         std::min(\n",
      "1907                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1908                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1909                 k3<<<n_blocks, n_threads>>>(\n",
      "1910                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1911                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1912                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1913                         CudaNdarray_DEV_DATA(self),\n",
      "1914                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1915                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1916                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1917                         CudaNdarray_DEV_DATA(other),\n",
      "1918                         other_strides[0],\n",
      "1919                         other_strides[1],\n",
      "1920                         other_strides[2]);\n",
      "1921                 CNDA_THREAD_SYNC;\n",
      "1922                 cudaError_t err = cudaGetLastError();\n",
      "1923                 if (cudaSuccess != err)\n",
      "1924                 {\n",
      "1925                     PyErr_Format(\n",
      "1926                         PyExc_RuntimeError,\n",
      "1927                         \"CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\\n\",\n",
      "1928                         \"k3\",\n",
      "1929                         cudaGetErrorString(err));\n",
      "1930                     Py_XDECREF(new_other);\n",
      "1931                     return -1;\n",
      "1932                 }\n",
      "1933             }\n",
      "1934             break;\n",
      "1935         case 4:\n",
      "1936             {\n",
      "1937                 dim3 n_blocks(\n",
      "1938                         std::min(\n",
      "1939                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1940                             NUM_VECTOR_OP_BLOCKS),\n",
      "1941                         CudaNdarray_HOST_DIMS(self)[1]\n",
      "1942                         );\n",
      "1943                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1944                     n_blocks.y /= 2;\n",
      "1945                 dim3 n_threads(\n",
      "1946                         std::min(\n",
      "1947                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1948                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1949                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1950                             );\n",
      "1951                 k4<<<n_blocks, n_threads>>>(\n",
      "1952                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1953                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1954                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1955                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "1956                         CudaNdarray_DEV_DATA(self),\n",
      "1957                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1958                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1959                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1960                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1961                         CudaNdarray_DEV_DATA(other),\n",
      "1962                         other_strides[0],\n",
      "1963                         other_strides[1],\n",
      "1964                         other_strides[2],\n",
      "1965                         other_strides[3]);\n",
      "1966                 CNDA_THREAD_SYNC;\n",
      "1967                 cudaError_t err = cudaGetLastError();\n",
      "1968                 if (cudaSuccess != err)\n",
      "1969                 {\n",
      "1970                     PyErr_Format(\n",
      "1971                         PyExc_RuntimeError,\n",
      "1972                         \"CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\\n\",\n",
      "1973                         \"k4\",\n",
      "1974                         cudaGetErrorString(err));\n",
      "1975                     Py_XDECREF(new_other);\n",
      "1976                     return -1;\n",
      "1977                 }\n",
      "1978             }\n",
      "1979             break;\n",
      "1980         case 5:\n",
      "1981             {\n",
      "1982                 dim3 n_blocks(\n",
      "1983                         std::min(\n",
      "1984                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1985                             NUM_VECTOR_OP_BLOCKS),\n",
      "1986                         CudaNdarray_HOST_DIMS(self)[2]);\n",
      "1987                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1988                     n_blocks.y /= 2;\n",
      "1989                 dim3 n_threads(\n",
      "1990                         std::min(\n",
      "1991                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1992                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1993                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1994                     );\n",
      "1995                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
      "1996                 {\n",
      "1997                      k4<<<n_blocks, n_threads>>>(\n",
      "1998                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1999                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "2000                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2001                             CudaNdarray_HOST_DIMS(self)[4],\n",
      "2002                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2003                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2004                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2005                             CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2006                             CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2007                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
      "2008                             other_strides[1],\n",
      "2009                             other_strides[2],\n",
      "2010                             other_strides[3],\n",
      "2011                             other_strides[4]);\n",
      "2012                     CNDA_THREAD_SYNC;\n",
      "2013                     cudaError_t err = cudaGetLastError();\n",
      "2014                     if( cudaSuccess != err)\n",
      "2015                     {\n",
      "2016                         PyErr_Format(\n",
      "2017                             PyExc_RuntimeError,\n",
      "2018                             \"CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
      "2019                             \"k5 with loop over k4\",\n",
      "2020                             cudaGetErrorString(err),\n",
      "2021                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
      "2022                         Py_XDECREF(new_other);\n",
      "2023                         return -1;\n",
      "2024                     }\n",
      "2025                 }\n",
      "2026             }\n",
      "2027             break;\n",
      "2028         case 6:\n",
      "2029             {\n",
      "2030                 dim3 n_blocks(\n",
      "2031                         std::min(\n",
      "2032                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "2033                             NUM_VECTOR_OP_BLOCKS),\n",
      "2034                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2035                         CudaNdarray_HOST_DIMS(self)[2]\n",
      "2036                         );\n",
      "2037                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "2038                     n_blocks.y /= 2;\n",
      "2039                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
      "2040                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
      "2041                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
      "2042                 //    n_blocks.z /= 2;\n",
      "2043                 n_blocks.z = 1;\n",
      "2044                 dim3 n_threads(\n",
      "2045                         std::min(\n",
      "2046                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2047                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "2048                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
      "2049                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
      "2050                             );\n",
      "2051                 k6<<<n_blocks, n_threads>>>(\n",
      "2052                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "2053                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2054                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "2055                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "2056                         CudaNdarray_HOST_DIMS(self)[4],\n",
      "2057                         CudaNdarray_HOST_DIMS(self)[5],\n",
      "2058                         CudaNdarray_DEV_DATA(self),\n",
      "2059                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2060                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2061                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2062                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2063                         CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2064                         CudaNdarray_HOST_STRIDES(self)[5],\n",
      "2065                         CudaNdarray_DEV_DATA(other),\n",
      "2066                         other_strides[0],\n",
      "2067                         other_strides[1],\n",
      "2068                         other_strides[2],\n",
      "2069                         other_strides[3],\n",
      "2070                         other_strides[4],\n",
      "2071                         other_strides[5]);\n",
      "2072                 CNDA_THREAD_SYNC;\n",
      "2073                 cudaError_t err = cudaGetLastError();\n",
      "2074                 if (cudaSuccess != err)\n",
      "2075                 {\n",
      "2076                     PyErr_Format(\n",
      "2077                         PyExc_RuntimeError,\n",
      "2078                         \"CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
      "2079                         \"k6\",\n",
      "2080                         cudaGetErrorString(err),\n",
      "2081                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
      "2082                         (long) n_threads.x);\n",
      "2083                     Py_XDECREF(new_other);\n",
      "2084                     return -1;\n",
      "2085                 }\n",
      "2086             }\n",
      "2087             break;\n",
      "2088         default:\n",
      "2089         {\n",
      "2090             PyErr_Format(\n",
      "2091                 PyExc_NotImplementedError,\n",
      "2092                 \"inplace_elemwise w nd=%i\\n\",\n",
      "2093                 self->nd);\n",
      "2094             Py_XDECREF(new_other);\n",
      "2095             return -1;\n",
      "2096         }\n",
      "2097     }\n",
      "2098     if (verbose)\n",
      "2099         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
      "2100     Py_XDECREF(new_other);\n",
      "2101     return 0;\n",
      "2102 }\n",
      "2103 \n",
      "2104 /*\n",
      "2105  * We need this inplace Add to support IncSubTensor\n",
      "2106  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2107  */\n",
      "2108 // Will be called by __iadd__ in Python\n",
      "2109 PyObject *\n",
      "2110 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
      "2111 {\n",
      "2112     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
      "2113     {\n",
      "2114         return NULL;\n",
      "2115     }\n",
      "2116     Py_INCREF(py_self);\n",
      "2117     return py_self;\n",
      "2118 }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2119 \n",
      "2120 /*\n",
      "2121  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
      "2122  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2123  */\n",
      "2124 // Will be called by __idiv__ in Python\n",
      "2125 static PyObject *\n",
      "2126 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
      "2127 {\n",
      "2128     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
      "2129     {\n",
      "2130         return NULL;\n",
      "2131     }\n",
      "2132     Py_INCREF(py_self);\n",
      "2133     return py_self;\n",
      "2134 }\n",
      "2135 \n",
      "2136 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
      "2137 #if PY_MAJOR_VERSION == 3\n",
      "2138 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2139 {\n",
      "2140     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2141     0,  //binaryfunc nb_subtract;\n",
      "2142     0,  //binaryfunc nb_multiply;\n",
      "2143     0,  //binaryfunc nb_remainder;\n",
      "2144     0,  //binaryfunc nb_divmod;\n",
      "2145     0,  //ternaryfunc nb_power;\n",
      "2146     0,  //unaryfunc nb_negative;\n",
      "2147     0,  //unaryfunc nb_positive;\n",
      "2148     0,  //unaryfunc nb_absolute;\n",
      "2149     0,  //inquiry nb_bool;\n",
      "2150     0,  //unaryfunc nb_invert;\n",
      "2151     0,  //binaryfunc nb_lshift;\n",
      "2152     0,  //binaryfunc nb_rshift;\n",
      "2153     0,  //binaryfunc nb_and;\n",
      "2154     0,  //binaryfunc nb_xor;\n",
      "2155     0,  //binaryfunc nb_or;\n",
      "2156     0,  //unaryfunc nb_int;\n",
      "2157     0,  //void *nb_reserved;\n",
      "2158     0,  //unaryfunc nb_float;\n",
      "2159 \n",
      "2160     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2161     0,  //binaryfunc nb_inplace_subtract;\n",
      "2162     0,  //binaryfunc nb_inplace_multiply;\n",
      "2163     0,  //binaryfunc nb_inplace_remainder;\n",
      "2164     0,  //ternaryfunc nb_inplace_power;\n",
      "2165     0,  //binaryfunc nb_inplace_lshift;\n",
      "2166     0,  //binaryfunc nb_inplace_rshift;\n",
      "2167     0,  //binaryfunc nb_inplace_and;\n",
      "2168     0,  //binaryfunc nb_inplace_xor;\n",
      "2169     0,  //binaryfunc nb_inplace_or;\n",
      "2170 \n",
      "2171     0,  //binaryfunc nb_floor_divide;\n",
      "2172     0,  //binaryfunc nb_true_divide;\n",
      "2173     0,  //binaryfunc nb_inplace_floor_divide;\n",
      "2174     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
      "2175 \n",
      "2176     0,  //unaryfunc nb_index\n",
      "2177 };\n",
      "2178 #else\n",
      "2179 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2180 {\n",
      "2181     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2182     0,  //binaryfunc nb_subtract;      __sub__\n",
      "2183     0,  //binaryfunc nb_multiply;      __mul__\n",
      "2184     0,  //binaryfunc nb_divide;        __div__\n",
      "2185     0,  //binaryfunc nb_remainder;     __mod__\n",
      "2186     0,  //binaryfunc nb_divmod;        __divmod__\n",
      "2187     0,  //ternaryfunc nb_power;        __pow__\n",
      "2188     0,  //unaryfunc nb_negative;       __neg__\n",
      "2189     0,  //unaryfunc nb_positive;       __pos__\n",
      "2190     0,  //unaryfunc nb_absolute;       __abs__\n",
      "2191     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
      "2192     0,  //unaryfunc nb_invert;         __invert__\n",
      "2193     0,  //binaryfunc nb_lshift;        __lshift__\n",
      "2194     0,  //binaryfunc nb_rshift;        __rshift__\n",
      "2195     0,  //binaryfunc nb_and;           __and__\n",
      "2196     0,  //binaryfunc nb_xor;           __xor__\n",
      "2197     0,  //binaryfunc nb_or;            __or__\n",
      "2198     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
      "2199     0,  //unaryfunc nb_int;            __int__\n",
      "2200     0,  //unaryfunc nb_long;           __long__\n",
      "2201     0,  //unaryfunc nb_float;          __float__\n",
      "2202     0,  //unaryfunc nb_oct;            __oct__\n",
      "2203     0,  //unaryfunc nb_hex;            __hex__\n",
      "2204 \n",
      "2205     /* Added in release 2.0 */\n",
      "2206     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2207     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
      "2208     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
      "2209     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
      "2210     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
      "2211     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
      "2212     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
      "2213     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
      "2214     0,  //binaryfunc nb_inplace_and;           __iand__\n",
      "2215     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
      "2216     0,  //binaryfunc nb_inplace_or;            __ior__\n",
      "2217 \n",
      "2218     /* Added in release 2.2 */\n",
      "2219     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
      "2220     0,  //binaryfunc nb_true_divide;           __truediv__\n",
      "2221     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
      "2222     0,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
      "2223 \n",
      "2224 #if PY_MINOR_VERSION > 4\n",
      "2225     /* Added in release 2.5 */\n",
      "2226     0  //unaryfunc nb_index;  __index__\n",
      "2227 #endif\n",
      "2228 };\n",
      "2229 #endif\n",
      "2230 \n",
      "2231 \n",
      "2232 /////////////////////\n",
      "2233 // Mapping protocol\n",
      "2234 /////////////////////\n",
      "2235 \n",
      "2236 // Will by called by __len__ in Python\n",
      "2237 static Py_ssize_t\n",
      "2238 CudaNdarray_len(PyObject * py_self)\n",
      "2239 {\n",
      "2240     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2241     if (self->nd <= 0)\n",
      "2242     {\n",
      "2243         return (Py_ssize_t) 0;\n",
      "2244     }\n",
      "2245     else\n",
      "2246     {\n",
      "2247         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
      "2248     }\n",
      "2249 }\n",
      "2250 \n",
      "2251 // Will by called by __getitem__ in Python\n",
      "2252 PyObject *\n",
      "2253 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
      "2254 {\n",
      "2255     int verbose = 0;\n",
      "2256     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
      "2257     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2258     PyObject * py_rval = NULL;\n",
      "2259     CudaNdarray * rval = NULL;\n",
      "2260     PyObject * intobj = NULL;\n",
      "2261 \n",
      "2262     //PyObject_Print(key, stderr, 0);\n",
      "2263 \n",
      "2264     if (key == Py_Ellipsis)\n",
      "2265     {\n",
      "2266         Py_INCREF(py_self);\n",
      "2267         return py_self;\n",
      "2268     }\n",
      "2269     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
      "2270     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
      "2271     {\n",
      "2272         int d_idx = PyInt_AsLong(intobj);\n",
      "2273         Py_DECREF(intobj); intobj=NULL;\n",
      "2274         //int d_idx = PyInt_AsLong(key);\n",
      "2275         if (self->nd == 0)\n",
      "2276         {\n",
      "2277             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
      "2278             return NULL;\n",
      "2279         }\n",
      "2280         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2281         int offset = 0;\n",
      "2282 \n",
      "2283         if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2284         {\n",
      "2285             //normal indexing\n",
      "2286             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2287         }\n",
      "2288         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2289         {\n",
      "2290             //end-based indexing\n",
      "2291             // d_idx is negative\n",
      "2292             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2293         }\n",
      "2294         else\n",
      "2295         {\n",
      "2296             PyErr_Format(PyExc_IndexError,\n",
      "2297                          \"index out of bounds. Asked %d, but size of %d\",\n",
      "2298                          d_idx, d_dim);\n",
      "2299             return NULL;\n",
      "2300         }\n",
      "2301 \n",
      "2302         //allocate our subtensor view\n",
      "2303         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
      "2304         rval = (CudaNdarray*) py_rval;\n",
      "2305         if (!rval) return NULL;\n",
      "2306         assert (0 == rval->data_allocated);\n",
      "2307 \n",
      "2308         //initialize the view's data pointer to our own.\n",
      "2309         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
      "2310         {\n",
      "2311             Py_DECREF(rval);\n",
      "2312             return NULL;\n",
      "2313         }\n",
      "2314         for (int d = 1; d < self->nd; ++d)\n",
      "2315         {\n",
      "2316             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2317             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2318         }\n",
      "2319     }\n",
      "2320     else\n",
      "2321     {\n",
      "2322         PyErr_Clear();\n",
      "2323     }\n",
      "2324     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
      "2325     {\n",
      "2326         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
      "2327         if (self->nd == 0)\n",
      "2328         {\n",
      "2329             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
      "2330             return NULL;\n",
      "2331         }\n",
      "2332 \n",
      "2333         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2334         Py_ssize_t start, stop, step, slen;\n",
      "2335         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
      "2336         {\n",
      "2337             if (verbose)\n",
      "2338                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
      "2339             return NULL;\n",
      "2340         }\n",
      "2341         if (verbose)\n",
      "2342         {\n",
      "2343             std::cerr << \"start \" << start << \"\\n\";\n",
      "2344             std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2345             std::cerr << \"step \" << step << \"\\n\";\n",
      "2346             std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2347         }\n",
      "2348 \n",
      "2349         //allocate our subtensor view\n",
      "2350         py_rval = CudaNdarray_new_nd(self->nd);\n",
      "2351         rval = (CudaNdarray*) py_rval;\n",
      "2352         if (!rval) return NULL;\n",
      "2353         assert (0 == rval->data_allocated);\n",
      "2354 \n",
      "2355 \n",
      "2356         //initialize the view's data pointer to our own.\n",
      "2357         if (CudaNdarray_set_device_data(rval,\n",
      "2358                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2359                     self))\n",
      "2360         {\n",
      "2361             Py_DECREF(rval);\n",
      "2362             return NULL;\n",
      "2363         }\n",
      "2364         //initialize dimension 0 of rval\n",
      "2365         CudaNdarray_set_stride(rval, 0,\n",
      "2366                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "2367         CudaNdarray_set_dim(rval, 0, slen);\n",
      "2368         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
      "2369         // initialize dimensions > 0 of rval\n",
      "2370         for (int d = 1; d < self->nd; ++d)\n",
      "2371         {\n",
      "2372             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2373             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2374         }\n",
      "2375     }\n",
      "2376     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
      "2377     {\n",
      "2378         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
      "2379         //elements of the tuple can be either integers or slices\n",
      "2380         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
      "2381 \n",
      "2382         if (PyTuple_Size(key) > self->nd)\n",
      "2383         {\n",
      "2384             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
      "2385             return NULL;\n",
      "2386         }\n",
      "2387 \n",
      "2388         //calculate the number of dimensions in the return value\n",
      "2389         int rval_nd = self->nd;\n",
      "2390         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
      "2391         {\n",
      "2392             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
      "2393             //So we use PyArray_IsAnyScalar that should covert everything.\n",
      "2394             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
      "2395         }\n",
      "2396 \n",
      "2397         //allocate our subtensor view\n",
      "2398         py_rval = CudaNdarray_new_nd(rval_nd);\n",
      "2399         rval = (CudaNdarray*) py_rval;\n",
      "2400         if (!rval) return NULL;\n",
      "2401         assert (0 == rval->data_allocated);\n",
      "2402 \n",
      "2403         //initialize the view's data pointer to our own.\n",
      "2404         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "2405         {\n",
      "2406             Py_DECREF(rval);\n",
      "2407             return NULL;\n",
      "2408         }\n",
      "2409 \n",
      "2410         // rval_d will refer to the current dimension in the rval.\n",
      "2411         // It will not be incremented for integer keys, but will be incremented for slice\n",
      "2412         // keys\n",
      "2413         int rval_d = 0;\n",
      "2414 \n",
      "2415         for (int d = 0; d < self->nd; ++d)\n",
      "2416         {\n",
      "2417             // keys can be shorter than self->nd.\n",
      "2418             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
      "2419             if (d >=PyTuple_Size(key))\n",
      "2420             {\n",
      "2421                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2422                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2423                 ++rval_d;\n",
      "2424             }\n",
      "2425             else\n",
      "2426             {\n",
      "2427                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
      "2428 \n",
      "2429                 if (PySlice_Check(key_d))\n",
      "2430                 {\n",
      "2431                     Py_ssize_t start, stop, step, slen;\n",
      "2432                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
      "2433                     {\n",
      "2434                         Py_DECREF(rval);\n",
      "2435                         return NULL;\n",
      "2436                     }\n",
      "2437                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2438                     CudaNdarray_set_stride(rval, rval_d,\n",
      "2439                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2440                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
      "2441                     if (0)\n",
      "2442                     {\n",
      "2443                         std::cerr << \"start \" << start << \"\\n\";\n",
      "2444                         std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2445                         std::cerr << \"step \" << step << \"\\n\";\n",
      "2446                         std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2447                     }\n",
      "2448                     ++rval_d;\n",
      "2449                 }\n",
      "2450                 else if ((intobj=PyNumber_Int(key_d)))\n",
      "2451                 {\n",
      "2452                     assert(PyArray_IsAnyScalar(key_d));\n",
      "2453                     int d_idx = PyInt_AsLong(intobj);\n",
      "2454                     Py_DECREF(intobj);\n",
      "2455                     intobj = NULL;\n",
      "2456                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
      "2457 \n",
      "2458                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2459                     {\n",
      "2460                         //normal indexing\n",
      "2461                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2462                     }\n",
      "2463                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2464                     {\n",
      "2465                         //end-based indexing\n",
      "2466                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2467                     }\n",
      "2468                     else\n",
      "2469                     {\n",
      "2470                         PyErr_Format(PyExc_IndexError,\n",
      "2471                                      \"index out of bounds. Asked %d for dimensions %d, but size of %d\",\n",
      "2472                                      d_idx, d, d_dim);\n",
      "2473                         Py_DECREF(rval);\n",
      "2474                         return NULL;\n",
      "2475                     }\n",
      "2476                 }\n",
      "2477                 else\n",
      "2478                 {\n",
      "2479                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
      "2480                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
      "2481                     Py_DECREF(rval);\n",
      "2482                     return NULL;\n",
      "2483                 }\n",
      "2484             }\n",
      "2485         }\n",
      "2486     }\n",
      "2487     if (py_rval)\n",
      "2488     {\n",
      "2489         if (verbose) fprint_CudaNdarray(stderr, self);\n",
      "2490         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
      "2491     }\n",
      "2492     else\n",
      "2493     {\n",
      "2494         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
      "2495         return NULL;\n",
      "2496     }\n",
      "2497     return py_rval;\n",
      "2498 }\n",
      "2499 \n",
      "2500 // Will by called by __setitem__ in Python\n",
      "2501 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
      "2502 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
      "2503 // Can only be assigned from a CudaNdarray on the right side\n",
      "2504 // Or a ndarray\n",
      "2505 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
      "2506 static int\n",
      "2507 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
      "2508 {\n",
      "2509     int verbose = 0;\n",
      "2510     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
      "2511     // We try to copy directly into this CudaNdarray from the ndarray\n",
      "2512     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
      "2513     CudaNdarray* new_value = NULL;\n",
      "2514 \n",
      "2515     if(!rval){\n",
      "2516         // CudaNdarray_Subscript failed and set the error msg.\n",
      "2517         Py_XDECREF(rval);\n",
      "2518         return -1;\n",
      "2519     }\n",
      "2520 \n",
      "2521     if(rval != (CudaNdarray*)o &&\n",
      "2522                 (rval->data_allocated ||\n",
      "2523                  // The new array should have a base\n",
      "2524                  !(((CudaNdarray*)rval)->base) ||\n",
      "2525                  // If the original array has no base, the base of the new\n",
      "2526                  // array should be the original one\n",
      "2527                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
      "2528                  // Else, the two arrays should have the same base\n",
      "2529                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
      "2530     {\n",
      "2531         // This case shouldn't happen, based on what I see in Subscript\n",
      "2532         // but just in case it happens sometime in the future\n",
      "2533 \n",
      "2534         PyErr_Format(PyExc_RuntimeError,\n",
      "2535                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
      "2536                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
      "2537                      \" o.base=%p o=%p\",\n",
      "2538                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
      "2539         Py_DECREF(rval);\n",
      "2540         return -1;\n",
      "2541     }\n",
      "2542 \n",
      "2543     PyObject * intobj = NULL;\n",
      "2544     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
      "2545         if (verbose)\n",
      "2546             fprintf(stderr,\n",
      "2547                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
      "2548                     \" value is a ndarray\\n\");\n",
      "2549         new_value = (CudaNdarray*) CudaNdarray_New();\n",
      "2550         if(!new_value)\n",
      "2551         {\n",
      "2552             return -1;\n",
      "2553         }\n",
      "2554         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
      "2555         {\n",
      "2556             Py_XDECREF(new_value);\n",
      "2557             Py_XDECREF(rval);\n",
      "2558             return -1;\n",
      "2559         }\n",
      "2560         value = (PyObject *) new_value;\n",
      "2561     }\n",
      "2562     else if ((intobj=PyNumber_Int(value)))\n",
      "2563     {\n",
      "2564         if (verbose)\n",
      "2565             fprintf(stderr,\n",
      "2566                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
      "2567         if(! CudaNdarray_is_c_contiguous(rval)){\n",
      "2568             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2569                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
      "2570                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
      "2571             Py_XDECREF(rval);\n",
      "2572             return -1;\n",
      "2573         }\n",
      "2574 \n",
      "2575         long val = PyInt_AsLong(intobj);\n",
      "2576         Py_DECREF(intobj); intobj=NULL;\n",
      "2577         if (val == 0)\n",
      "2578         {\n",
      "2579             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
      "2580                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
      "2581             Py_XDECREF(rval);\n",
      "2582             if (err)\n",
      "2583             {\n",
      "2584                 // Clear the error flag, cudaMemset doesn't do it.\n",
      "2585                 // Currently this returns the same thing as err, but if in future\n",
      "2586                 // it returns something else I still don't see why we should ignore\n",
      "2587                 // it.  All we want to do here is reset the flag.\n",
      "2588                 cudaGetLastError();\n",
      "2589                 PyErr_SetString(PyExc_RuntimeError,\n",
      "2590                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
      "2591                 return -1;\n",
      "2592             }\n",
      "2593             return 0;\n",
      "2594         } else {\n",
      "2595             Py_XDECREF(rval);\n",
      "2596             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2597                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
      "2598                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
      "2599                 return -1;\n",
      "2600         }\n",
      "2601     }\n",
      "2602 \n",
      "2603     PyErr_Clear(); // clear PyNumber_Int error.\n",
      "2604 \n",
      "2605     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
      "2606     {\n",
      "2607         PyErr_SetString(PyExc_TypeError,\n",
      "2608           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
      "2609           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
      "2610         Py_XDECREF(new_value);\n",
      "2611         return -1;\n",
      "2612     }\n",
      "2613 \n",
      "2614     if (verbose)\n",
      "2615         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
      "2616 \n",
      "2617     if (cnda_copy_structure_to_device(rval))\n",
      "2618     {\n",
      "2619         PyErr_SetString(PyExc_RuntimeError,\n",
      "2620                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
      "2621         Py_DECREF(rval);\n",
      "2622         Py_XDECREF(new_value);\n",
      "2623 \n",
      "2624         if (verbose)\n",
      "2625             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2626         return -1;\n",
      "2627     }\n",
      "2628 \n",
      "2629     PyObject *baseSavedForComparison = rval->base;\n",
      "2630 \n",
      "2631     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
      "2632     {\n",
      "2633         Py_DECREF((PyObject*)rval);\n",
      "2634         Py_XDECREF(new_value);\n",
      "2635 \n",
      "2636         if (verbose)\n",
      "2637             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2638         return -1;\n",
      "2639     }\n",
      "2640 \n",
      "2641     assert (rval->base == baseSavedForComparison);\n",
      "2642     assert (rval->dev_structure_fresh);\n",
      "2643 \n",
      "2644     // Clean up locally-created references\n",
      "2645     Py_DECREF(rval);\n",
      "2646     Py_XDECREF(new_value);\n",
      "2647 \n",
      "2648     return 0;\n",
      "2649 }\n",
      "2650 \n",
      "2651 \n",
      "2652 PyMappingMethods CudaNdarrayMappingMethods = {\n",
      "2653     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
      "2654     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
      "2655     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
      "2656 };\n",
      "2657 \n",
      "2658 ////////////////////\n",
      "2659 //\n",
      "2660 ////////////////////\n",
      "2661 \n",
      "2662 static PyObject *\n",
      "2663 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
      "2664 {\n",
      "2665     if (self->nd < 0)\n",
      "2666     {\n",
      "2667         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2668         return NULL;\n",
      "2669     }\n",
      "2670     PyObject * rval = PyTuple_New(self->nd);\n",
      "2671     for (int i = 0; i < self->nd; ++i)\n",
      "2672     {\n",
      "2673         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
      "2674         {\n",
      "2675             Py_XDECREF(rval);\n",
      "2676             return NULL;\n",
      "2677         }\n",
      "2678 \n",
      "2679     }\n",
      "2680     return rval;\n",
      "2681 }\n",
      "2682 \n",
      "2683 static int\n",
      "2684 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2685 {\n",
      "2686     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
      "2687     return -1;\n",
      "2688 }\n",
      "2689 \n",
      "2690 static PyObject *\n",
      "2691 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
      "2692 {\n",
      "2693     if (self->nd < 0)\n",
      "2694     {\n",
      "2695         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2696         return NULL;\n",
      "2697     }\n",
      "2698     PyObject * rval = PyTuple_New(self->nd);\n",
      "2699     for (int i = 0; i < self->nd; ++i)\n",
      "2700     {\n",
      "2701         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
      "2702         {\n",
      "2703             Py_XDECREF(rval);\n",
      "2704             return NULL;\n",
      "2705         }\n",
      "2706 \n",
      "2707     }\n",
      "2708     return rval;\n",
      "2709 }\n",
      "2710 \n",
      "2711 static int\n",
      "2712 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2713 {\n",
      "2714     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
      "2715     if (PyTuple_Check(value)){\n",
      "2716         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2717             PyErr_SetString(PyExc_ValueError,\n",
      "2718                             \"The new strides tuple must have the same length\"\n",
      "2719                             \" as the number of dimensions\");\n",
      "2720             return -1;\n",
      "2721         }\n",
      "2722     }else if (PyList_Check(value)){\n",
      "2723         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2724             PyErr_SetString(PyExc_ValueError,\n",
      "2725                             \"The new strides list must have the same length\"\n",
      "2726                             \" as the number of dimensions\");\n",
      "2727             return -1;\n",
      "2728         }\n",
      "2729     }else{\n",
      "2730         PyErr_SetString(PyExc_ValueError,\n",
      "2731                         \"The new strides need to be encoded in a tuple or list\");\n",
      "2732         return -1;\n",
      "2733     }\n",
      "2734     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
      "2735     if (PyTuple_Check(value)){\n",
      "2736         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2737             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
      "2738             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2739         }\n",
      "2740     }else if (PyList_Check(value)){\n",
      "2741         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2742             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
      "2743             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2744         }\n",
      "2745     }\n",
      "2746     /*\n",
      "2747     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
      "2748     // to do it.\n",
      "2749     npy_intp dims[PyTuple_Size(value)];\n",
      "2750     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2751         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "2752     }\n",
      "2753     if (!PyArray_CheckStrides(4,\n",
      "2754                               CudaNdarray_NDIM(self),\n",
      "2755                               0, 0,\n",
      "2756                               dims,\n",
      "2757                               newstrides_bytes)){\n",
      "2758         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
      "2759         return -1;\n",
      "2760         }\n",
      "2761     */\n",
      "2762     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2763         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "2764     }\n",
      "2765     return 0;\n",
      "2766 }\n",
      "2767 \n",
      "2768 static PyObject *\n",
      "2769 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
      "2770 {\n",
      "2771     float * p =  CudaNdarray_DEV_DATA(self);\n",
      "2772     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
      "2773     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));\n",
      "2774 }\n",
      "2775 \n",
      "2776 static int\n",
      "2777 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2778 {\n",
      "2779     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);\n",
      "2780     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
      "2781     if (PyErr_Occurred())\n",
      "2782     {\n",
      "2783         return -1;\n",
      "2784     }\n",
      "2785     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
      "2786 }\n",
      "2787 \n",
      "2788 static PyObject *\n",
      "2789 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
      "2790 {\n",
      "2791     return PyString_FromString(\"float32\");\n",
      "2792 }\n",
      "2793 \n",
      "2794 static PyObject *\n",
      "2795 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
      "2796 {\n",
      "2797     return PyInt_FromLong(self->nd);\n",
      "2798 }\n",
      "2799 \n",
      "2800 static PyObject *\n",
      "2801 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
      "2802 {\n",
      "2803     PyObject * base = self->base;\n",
      "2804     if (!base)\n",
      "2805     {\n",
      "2806         // We cannot return a NULL pointer, use None instead\n",
      "2807         base = Py_None;\n",
      "2808     }\n",
      "2809     Py_INCREF(base);\n",
      "2810     return base;\n",
      "2811 }\n",
      "2812 \n",
      "2813 void put_in_dict(PyObject * dict, const char * key, int val)\n",
      "2814 {\n",
      "2815   PyObject * k = PyString_FromString(key);\n",
      "2816   PyObject * v = PyInt_FromLong(val);\n",
      "2817   PyDict_SetItem(dict, k, v);\n",
      "2818   Py_DECREF(k);\n",
      "2819   Py_DECREF(v);\n",
      "2820 }\n",
      "2821 \n",
      "2822 PyObject *\n",
      "2823 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
      "2824 {\n",
      "2825   int dev_id = -1;\n",
      "2826   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
      "2827     return NULL;\n",
      "2828   cudaDeviceProp deviceProp;\n",
      "2829   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
      "2830 \n",
      "2831   PyObject * dict = PyDict_New();\n",
      "2832   PyObject * str= PyString_FromString(\"name\");\n",
      "2833   PyObject * i = PyString_FromString(deviceProp.name);\n",
      "2834   PyDict_SetItem(dict, str, i);\n",
      "2835   Py_DECREF(str);\n",
      "2836   Py_DECREF(i);\n",
      "2837 \n",
      "2838   put_in_dict(dict, \"major\", deviceProp.major);\n",
      "2839   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
      "2840 #if CUDART_VERSION >= 2020\n",
      "2841   int driverVersion = 0, runtimeVersion = 0;\n",
      "2842   cudaDriverGetVersion(&driverVersion);\n",
      "2843   cudaRuntimeGetVersion(&runtimeVersion);\n",
      "2844   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
      "2845   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
      "2846 #endif\n",
      "2847 #if CUDART_VERSION >= 2000\n",
      "2848 \n",
      "2849   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
      "2850   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
      "2851   int sm_cores = -1;\n",
      "2852   if(deviceProp.major==1)\n",
      "2853     sm_cores = 32;\n",
      "2854   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
      "2855     sm_cores = 32;\n",
      "2856   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
      "2857     sm_cores = 48;\n",
      "2858   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
      "2859 #endif\n",
      "2860   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
      "2861   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
      "2862   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
      "2863   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
      "2864   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
      "2865   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
      "2866   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
      "2867   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
      "2868   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
      "2869   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
      "2870   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
      "2871   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
      "2872   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
      "2873   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
      "2874 #if CUDART_VERSION >= 2000\n",
      "2875   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
      "2876 #endif\n",
      "2877 #if CUDART_VERSION >= 2020\n",
      "2878   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
      "2879   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
      "2880   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
      "2881   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
      "2882   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
      "2883 #endif\n",
      "2884 #if CUDART_VERSION >= 3000\n",
      "2885   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
      "2886 #endif\n",
      "2887 #if CUDART_VERSION >= 3010\n",
      "2888   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
      "2889 #endif\n",
      "2890 #if CUDART_VERSION >= 3020\n",
      "2891   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
      "2892 #endif\n",
      "2893 \n",
      "2894   return dict;\n",
      "2895 }\n",
      "2896 \n",
      "2897 /*\n",
      "2898  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
      "2899  */\n",
      "2900 PyObject *\n",
      "2901 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
      "2902 {\n",
      "2903     size_t free = 0, total = 0;\n",
      "2904     if(g_gpu_context_active == 0){\n",
      "2905         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
      "2906         return NULL;\n",
      "2907     }\n",
      "2908 \n",
      "2909     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "2910     if (err != cudaSuccess){\n",
      "2911         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "2912         // Currently this returns the same thing as err, but if in future\n",
      "2913         // it returns something else I still don't see why we should ignore\n",
      "2914         // it.  All we want to do here is reset the flag.\n",
      "2915         cudaGetLastError();\n",
      "2916         PyErr_Format(PyExc_RuntimeError,\n",
      "2917                      \"Error while getting memory info about the gpu: %s\",\n",
      "2918                      cudaGetErrorString(err));\n",
      "2919         return NULL;\n",
      "2920     }\n",
      "2921     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));\n",
      "2922 }\n",
      "2923 \n",
      "2924 /*\n",
      "2925  * Synchronize with all the gpu device stream.\n",
      "2926  */\n",
      "2927 PyObject *\n",
      "2928 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
      "2929 {\n",
      "2930     CNDA_BEGIN_ALLOW_THREADS\n",
      "2931     cudaThreadSynchronize();\n",
      "2932     CNDA_END_ALLOW_THREADS\n",
      "2933     Py_INCREF(Py_None);\n",
      "2934     return Py_None;\n",
      "2935 }\n",
      "2936 \n",
      "2937 /*\n",
      "2938  * Exist and return true if we link with cublas v2.\n",
      "2939  */\n",
      "2940 PyObject *\n",
      "2941 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
      "2942 {\n",
      "2943     Py_INCREF(Py_True);\n",
      "2944     return Py_True;\n",
      "2945 }\n",
      "2946 \n",
      "2947 PyObject *\n",
      "2948 CudaNdarray_select_a_gpu(PyObject* _unused, PyObject* dummy)\n",
      "2949 {\n",
      "2950     void * rval = NULL;\n",
      "2951 \n",
      "2952     cudaError_t err = cudaMalloc(&rval, 4);\n",
      "2953     if (cudaSuccess != err){\n",
      "2954         printf(\"ERR!\\\\n\");\n",
      "2955             PyErr_Format(PyExc_RuntimeError,\n",
      "2956                          \"Not able to do basic stuff on the GPU (alloc of 4 bytes) (%s).\",\n",
      "2957                          cudaGetErrorString(err));\n",
      "2958             return NULL;\n",
      "2959     }\n",
      "2960     err = cudaFree(rval);\n",
      "2961     if (cudaSuccess != err){\n",
      "2962         printf(\"ERR!\\\\n\");\n",
      "2963             PyErr_Format(PyExc_RuntimeError,\n",
      "2964                          \"Not able to do basic stuff on the GPU (cudaFree failed) (%s).\",\n",
      "2965                          cudaGetErrorString(err));\n",
      "2966             return NULL;\n",
      "2967     }\n",
      "2968 \n",
      "2969     Py_INCREF(Py_None);\n",
      "2970     return Py_None;\n",
      "2971 }\n",
      "2972 \n",
      "2973 #if COMPUTE_GPU_MEM_USED\n",
      "2974 /*\n",
      "2975  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
      "2976  */\n",
      "2977 PyObject *\n",
      "2978 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
      "2979 {\n",
      "2980     PyObject* a = PyLong_FromLong(_allocated_size);\n",
      "2981     PyObject* b = PyLong_FromLong(_max_allocated_size);\n",
      "2982 \n",
      "2983     PyObject* tuple = PyTuple_New(2);\n",
      "2984     PyTuple_SetItem(tuple, 0, a);\n",
      "2985     PyTuple_SetItem(tuple, 1, b);\n",
      "2986     return tuple;\n",
      "2987 }\n",
      "2988 #endif\n",
      "2989 \n",
      "2990 static PyGetSetDef CudaNdarray_getset[] = {\n",
      "2991     {\"shape\",\n",
      "2992         (getter)CudaNdarray_get_shape,\n",
      "2993         (setter)CudaNdarray_set_shape,\n",
      "2994         \"shape of this ndarray (tuple)\",\n",
      "2995         NULL},\n",
      "2996     {\"_strides\",\n",
      "2997         (getter)CudaNdarray_get_strides,\n",
      "2998         (setter)CudaNdarray_set_strides,\n",
      "2999         \"data pointer strides (in elements)\",\n",
      "3000         NULL},\n",
      "3001     {\"strides\",\n",
      "3002         (getter)CudaNdarray_get_strides,\n",
      "3003         (setter)CudaNdarray_set_strides,\n",
      "3004         \"data pointer strides (in elements)\",\n",
      "3005         NULL},\n",
      "3006     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
      "3007     {\"gpudata\",\n",
      "3008         (getter)CudaNdarray_get_dev_data,\n",
      "3009         NULL,\n",
      "3010         \"device data pointer\",\n",
      "3011         NULL},\n",
      "3012     {\"_dev_data\",\n",
      "3013         (getter)CudaNdarray_get_dev_data,\n",
      "3014         (setter)CudaNdarray_set_dev_data,\n",
      "3015         \"device data pointer\",\n",
      "3016         NULL},\n",
      "3017     {\"dtype\",\n",
      "3018         (getter)CudaNdarray_get_dtype,\n",
      "3019         NULL,\n",
      "3020         \"The dtype of the element. Now always float32\",\n",
      "3021         NULL},\n",
      "3022     {\"size\",\n",
      "3023         (getter)CudaNdarray_SIZE_Object,\n",
      "3024         NULL,\n",
      "3025         \"The number of elements in this object.\",\n",
      "3026         NULL},\n",
      "3027     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
      "3028     {\"mem_size\",\n",
      "3029         (getter)CudaNdarray_SIZE_Object,\n",
      "3030         NULL,\n",
      "3031         \"The number of elements in this object.\",\n",
      "3032         NULL},\n",
      "3033     {\"ndim\",\n",
      "3034         (getter)CudaNdarray_get_ndim,\n",
      "3035         NULL,\n",
      "3036         \"The number of dimensions in this object.\",\n",
      "3037         NULL},\n",
      "3038     {\"base\",\n",
      "3039         (getter)CudaNdarray_get_base,\n",
      "3040         NULL,\n",
      "3041         \"If this ndarray is a view, base is the original ndarray.\",\n",
      "3042         NULL},\n",
      "3043 \n",
      "3044     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3045 };\n",
      "3046 \n",
      "3047 static PyTypeObject CudaNdarrayType =\n",
      "3048 {\n",
      "3049 #if PY_MAJOR_VERSION >= 3\n",
      "3050     PyVarObject_HEAD_INIT(NULL, 0)\n",
      "3051 #else\n",
      "3052     PyObject_HEAD_INIT(NULL)\n",
      "3053     0,                         /*ob_size*/\n",
      "3054 #endif\n",
      "3055     \"CudaNdarray\",             /*tp_name*/\n",
      "3056     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
      "3057     0,                         /*tp_itemsize*/\n",
      "3058     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
      "3059     0,                         /*tp_print*/\n",
      "3060     0,                         /*tp_getattr*/\n",
      "3061     0,                         /*tp_setattr*/\n",
      "3062     0,                         /*tp_compare*/\n",
      "3063     0,                         /*tp_repr*/\n",
      "3064     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
      "3065     0,                         /*tp_as_sequence*/\n",
      "3066     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
      "3067     0,                         /*tp_hash */\n",
      "3068     0,                         /*tp_call*/\n",
      "3069     0,                         /*tp_str*/\n",
      "3070     0,                         /*tp_getattro*/\n",
      "3071     0,                         /*tp_setattro*/\n",
      "3072     0,                         /*tp_as_buffer*/\n",
      "3073 #if PY_MAJOR_VERSION >= 3\n",
      "3074     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
      "3075     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
      "3076 #else\n",
      "3077     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
      "3078 #endif\n",
      "3079     \"CudaNdarray objects\",     /* tp_doc */\n",
      "3080     0,                         /* tp_traverse */\n",
      "3081     0,                         /* tp_clear */\n",
      "3082     0,                         /* tp_richcompare */\n",
      "3083     0,                         /* tp_weaklistoffset */\n",
      "3084     0,                         /* tp_iter */\n",
      "3085     0,                         /* tp_iternext */\n",
      "3086     CudaNdarray_methods,       /* tp_methods */\n",
      "3087     CudaNdarray_members,       /* tp_members */\n",
      "3088     CudaNdarray_getset,        /* tp_getset */\n",
      "3089     0,                         /* tp_base */\n",
      "3090     0,                         /* tp_dict */\n",
      "3091     0,                         /* tp_descr_get */\n",
      "3092     0,                         /* tp_descr_set */\n",
      "3093     0,                         /* tp_dictoffset */\n",
      "3094     (initproc)CudaNdarray_init,/* tp_init */\n",
      "3095     0,                         /* tp_alloc */\n",
      "3096     CudaNdarray_new,           /* tp_new */\n",
      "3097 };\n",
      "3098 \n",
      "3099 static __global__ void get_gpu_ptr_size(int* dst)\n",
      "3100 {\n",
      "3101     dst[0] = sizeof(float*);\n",
      "3102     dst[1] = sizeof(int);\n",
      "3103 }\n",
      "3104 \n",
      "3105 PyObject *\n",
      "3106 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
      "3107 {\n",
      "3108     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
      "3109     if(gpu_data == NULL){\n",
      "3110         return NULL;\n",
      "3111     }\n",
      "3112     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
      "3113 \n",
      "3114     cudaError_t cudaErr = cudaGetLastError();\n",
      "3115     if (cudaSuccess != cudaErr){\n",
      "3116 \n",
      "3117         device_free(gpu_data);\n",
      "3118         return PyErr_Format(PyExc_RuntimeError,\n",
      "3119                             \"CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)\",\n",
      "3120                             cudaGetErrorString(cudaErr));\n",
      "3121     }\n",
      "3122 \n",
      "3123     // Transfer the result to cpu\n",
      "3124     int gpu_sizes[] = {-1,-1};\n",
      "3125     cublasStatus_t err;\n",
      "3126     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
      "3127     device_free(gpu_data);\n",
      "3128 \n",
      "3129     if (CUBLAS_STATUS_SUCCESS != err){\n",
      "3130         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
      "3131         return NULL;\n",
      "3132     }\n",
      "3133     return Py_BuildValue(\"iiii\", (int) gpu_sizes[0], (int)sizeof(float*),\n",
      "3134                          (int)sizeof(int), (int) gpu_sizes[1]);\n",
      "3135 }\n",
      "3136 \n",
      "3137 static int cublas_init();\n",
      "3138 static void cublas_shutdown();\n",
      "3139 // Initialize the gpu.\n",
      "3140 // Takes two optional parameters, the device number and if we should use cnmem.\n",
      "3141 // If the device number is provided, it sets that device to be the active device.\n",
      "3142 // If not provided (usually just to test whether the gpu is available at all),\n",
      "3143 // it does not set an active device.\n",
      "3144 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
      "3145 // cnmem is threaded like a bool. If converted to 0, don't use cnmem. Otherwise, use it.\n",
      "3146 PyObject *\n",
      "3147 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
      "3148 {\n",
      "3149     int card_nb = 0;\n",
      "3150     int card_number_provided = 1;\n",
      "3151     float cnmem = 0; // Theano flag lib.cnmem\n",
      "3152     // if we're given something wildly invalid, this will throw a TypeError\n",
      "3153     if(!PyArg_ParseTuple(args, \"|if\", &card_nb, &cnmem))\n",
      "3154         return NULL;\n",
      "3155     if(cnmem)\n",
      "3156         g_use_cnmem = true;\n",
      "3157 \n",
      "3158     if(PyTuple_Size(args) == 0) {\n",
      "3159         card_number_provided = 0;\n",
      "3160         card_nb = 0;\n",
      "3161     }\n",
      "3162 \n",
      "3163     int deviceCount;\n",
      "3164     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
      "3165     if(cudaSuccess != err) {\n",
      "3166         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3167                             \"Unable to get the number of gpus available: %s\",\n",
      "3168                             cudaGetErrorString(cudaGetLastError()));\n",
      "3169     }\n",
      "3170 \n",
      "3171     // as soon as the first successful call to a cuda* function is made, a\n",
      "3172     // gpu context has been created\n",
      "3173     g_gpu_context_active = 1;\n",
      "3174 \n",
      "3175     if(deviceCount <= 0) {\n",
      "3176         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3177                             \"Can't use the GPU, no devices support CUDA\");\n",
      "3178     }\n",
      "3179     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
      "3180         return PyErr_Format(PyExc_ValueError,\n",
      "3181                             \"Bad device number %d. Only %d devices available.\",\n",
      "3182                             card_nb,\n",
      "3183                             deviceCount);\n",
      "3184     }\n",
      "3185 \n",
      "3186     cudaDeviceProp deviceProp;\n",
      "3187     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
      "3188     if(cudaSuccess != err) {\n",
      "3189         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3190                             \"Unable to get properties of gpu %i: %s\",\n",
      "3191                             card_nb,\n",
      "3192                             cudaGetErrorString(cudaGetLastError()));\n",
      "3193     }\n",
      "3194 \n",
      "3195     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
      "3196         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3197                             \"There is no device that supports CUDA\");\n",
      "3198     }\n",
      "3199 \n",
      "3200     if(card_number_provided) {\n",
      "3201         err = cudaSetDevice(card_nb);\n",
      "3202         if(cudaSuccess != err) {\n",
      "3203             return PyErr_Format(PyExc_EnvironmentError,\n",
      "3204                                 \"Unable to set device %i: %s\",\n",
      "3205                                 card_nb,\n",
      "3206                                 cudaGetErrorString(cudaGetLastError()));\n",
      "3207         }\n",
      "3208         if (cublas_init() == -1)\n",
      "3209             return NULL;\n",
      "3210     }\n",
      "3211     if(card_number_provided && g_use_cnmem) {\n",
      "3212         size_t mem = 0;\n",
      "3213         if (cnmem > 1)\n",
      "3214             mem = cnmem * 1024 * 1024;\n",
      "3215         else{\n",
      "3216             // Clip to 98.5% to let memory for the driver.\n",
      "3217             if (cnmem > .985){\n",
      "3218                 cnmem = .985;\n",
      "3219             }\n",
      "3220             size_t free = 0, total = 0;\n",
      "3221             cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "3222             if (err != cudaSuccess){\n",
      "3223                 // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "3224                 // Currently this returns the same thing as err, but if in future\n",
      "3225                 // it returns something else I still don't see why we should ignore\n",
      "3226                 // it.  All we want to do here is reset the flag.\n",
      "3227                 cudaGetLastError();\n",
      "3228                 PyErr_Format(PyExc_RuntimeError,\n",
      "3229                              \"Error while getting memory info about the gpu: %s\",\n",
      "3230                              cudaGetErrorString(err));\n",
      "3231                 return NULL;\n",
      "3232             }\n",
      "3233             mem = total * cnmem;\n",
      "3234         }\n",
      "3235         if(initCnmem(card_number_provided, card_nb, mem) == -1){\n",
      "3236             return NULL;\n",
      "3237         }\n",
      "3238     }\n",
      "3239 \n",
      "3240     Py_INCREF(Py_None);\n",
      "3241     return Py_None;\n",
      "3242 }\n",
      "3243 \n",
      "3244 PyObject *\n",
      "3245 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
      "3246     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3247     // really necessary.\n",
      "3248     int currentDevice;\n",
      "3249     cudaGetDevice(&currentDevice);\n",
      "3250     return PyInt_FromLong(currentDevice);\n",
      "3251 }\n",
      "3252 \n",
      "3253 PyObject *\n",
      "3254 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
      "3255     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3256     // really necessary.\n",
      "3257     int currentDevice;\n",
      "3258     cudaGetDevice(&currentDevice);\n",
      "3259 \n",
      "3260     cudaDeviceProp deviceProp;\n",
      "3261     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
      "3262     return PyString_FromString(deviceProp.name);\n",
      "3263 }\n",
      "3264 \n",
      "3265 PyObject *\n",
      "3266 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
      "3267     // Don't handle errors here\n",
      "3268     cublas_shutdown();\n",
      "3269     g_gpu_context_active = 0; // context has now been closed down\n",
      "3270     if(g_use_cnmem) {\n",
      "3271         cnmemStatus_t status = cnmemFinalize();\n",
      "3272         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "3273             fprintf(stderr, \"CudaNdarray_gpu_shutdown: cnmemFinalize failed! Reason=%s\\n\",\n",
      "3274                     cnmemGetErrorString(status));\n",
      "3275             if(status == CNMEM_STATUS_CUDA_ERROR) {\n",
      "3276                 fprintf(stderr, \"  Cuda-Reason=%s\\n\",\n",
      "3277                         cudaGetErrorString(cudaGetLastError()));\n",
      "3278             }\n",
      "3279         }\n",
      "3280     }\n",
      "3281     cudaThreadExit();\n",
      "3282 \n",
      "3283     Py_INCREF(Py_None);\n",
      "3284     return Py_None;\n",
      "3285 }\n",
      "3286 \n",
      "3287 /*\n",
      "3288  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
      "3289  */\n",
      "3290 PyObject *\n",
      "3291 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
      "3292 {\n",
      "3293     int verbose = 0;\n",
      "3294     PyObject *gpu_ptr = NULL;\n",
      "3295     PyObject *shapes = NULL;\n",
      "3296     PyObject *strides = NULL;\n",
      "3297     PyObject *base = NULL;\n",
      "3298     PyObject *rval = NULL;\n",
      "3299 \n",
      "3300     //args should consist of 3 python objects\n",
      "3301     //The first is the gpu ptr\n",
      "3302     //The second if the shape\n",
      "3303     //The third if the strides\n",
      "3304     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
      "3305         return NULL;\n",
      "3306 \n",
      "3307     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
      "3308     if (!PyLong_Check(gpu_ptr))\n",
      "3309     {\n",
      "3310         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
      "3311         return NULL;\n",
      "3312     }\n",
      "3313 \n",
      "3314     Py_ssize_t nd =  PyObject_Length(shapes);\n",
      "3315     if (nd < 0)\n",
      "3316     {\n",
      "3317         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
      "3318         return NULL;\n",
      "3319     }\n",
      "3320     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
      "3321     if (nd_stride < 0)\n",
      "3322     {\n",
      "3323         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
      "3324         return NULL;\n",
      "3325     }\n",
      "3326 \n",
      "3327     if (nd != nd_stride)\n",
      "3328     {\n",
      "3329         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
      "3330         return NULL;\n",
      "3331     }\n",
      "3332 \n",
      "3333     rval = CudaNdarray_New();\n",
      "3334 \n",
      "3335     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
      "3336     {\n",
      "3337         //CudaNdarray_set_nd set the error msg\n",
      "3338         return NULL;\n",
      "3339     }\n",
      "3340     // set gpu pointeur\n",
      "3341     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
      "3342     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
      "3343     {\n",
      "3344         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
      "3345         return NULL;\n",
      "3346 \n",
      "3347     }\n",
      "3348 \n",
      "3349     // Set dims and strides\n",
      "3350     for (int i = nd-1; i >= 0; --i)\n",
      "3351     {\n",
      "3352         PyObject * idx = PyLong_FromLong(i);\n",
      "3353         if (idx == NULL)\n",
      "3354         {\n",
      "3355             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
      "3356             return NULL;\n",
      "3357         }\n",
      "3358         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
      "3359         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
      "3360         if (!PyInt_Check(dim_))\n",
      "3361         {\n",
      "3362             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
      "3363             return NULL;\n",
      "3364         }\n",
      "3365         if (!PyInt_Check(strd_))\n",
      "3366         {\n",
      "3367             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
      "3368             return NULL;\n",
      "3369         }\n",
      "3370         int dim = PyInt_AsLong(dim_);\n",
      "3371         int strd = PyInt_AsLong(strd_);\n",
      "3372         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
      "3373         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
      "3374         Py_DECREF(idx);\n",
      "3375         Py_DECREF(dim_);\n",
      "3376         Py_DECREF(strd_);\n",
      "3377     }\n",
      "3378     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
      "3379     return rval;\n",
      "3380 }\n",
      "3381 \n",
      "3382 PyObject *\n",
      "3383 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
      "3384 {\n",
      "3385     PyObject *l=NULL;\n",
      "3386     PyObject *r=NULL;\n",
      "3387     PyObject * rval = NULL;\n",
      "3388 \n",
      "3389     //args should consist of two python objects (\"OO\")\n",
      "3390     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
      "3391         return NULL;\n",
      "3392 \n",
      "3393     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
      "3394     {\n",
      "3395         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3396         goto CudaNdarray_dot_fail;\n",
      "3397     }\n",
      "3398     if (((CudaNdarray*)l)->nd != 2)\n",
      "3399     {\n",
      "3400         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3401         goto CudaNdarray_dot_fail;\n",
      "3402     }\n",
      "3403     if (((CudaNdarray*)r)->nd != 2)\n",
      "3404     {\n",
      "3405         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3406         goto CudaNdarray_dot_fail;\n",
      "3407     }\n",
      "3408     rval = CudaNdarray_New();\n",
      "3409     if (!rval)\n",
      "3410     {\n",
      "3411         goto CudaNdarray_dot_fail;\n",
      "3412     }\n",
      "3413     int dims[2];\n",
      "3414     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
      "3415     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
      "3416     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
      "3417     {\n",
      "3418         goto CudaNdarray_dot_fail;\n",
      "3419     }\n",
      "3420     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
      "3421     {\n",
      "3422         goto CudaNdarray_dot_fail;\n",
      "3423     }\n",
      "3424 \n",
      "3425     return rval;\n",
      "3426 \n",
      "3427     CudaNdarray_dot_fail:\n",
      "3428     Py_XDECREF(rval);\n",
      "3429     return NULL;\n",
      "3430 }\n",
      "3431 \n",
      "3432 static PyObject *\n",
      "3433 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
      "3434 {\n",
      "3435     /*\n",
      "3436      * TODO: DOC what this function should do in the various cases of\n",
      "3437      * What is 'strict' supposed to mean in the context of this function?\n",
      "3438      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
      "3439      *\n",
      "3440      */\n",
      "3441     PyObject *py_data=NULL;\n",
      "3442     PyArrayObject * data = NULL;\n",
      "3443     int strict = 0;\n",
      "3444     PyObject * broadcastable=NULL;\n",
      "3445     PyObject * storage=NULL;\n",
      "3446     CudaNdarray * rval=NULL;\n",
      "3447 \n",
      "3448     //Python object references which are provided to the caller are borrowed references\n",
      "3449     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
      "3450 \n",
      "3451     if (!PyTuple_Check(broadcastable)){\n",
      "3452         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
      "3453         return NULL;\n",
      "3454     }\n",
      "3455     Py_INCREF(py_data);\n",
      "3456     Py_INCREF(broadcastable);\n",
      "3457 \n",
      "3458     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
      "3459 \n",
      "3460     if (strict || CudaNdarray_Check(py_data))\n",
      "3461     {\n",
      "3462         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
      "3463         if (!CudaNdarray_Check(py_data))\n",
      "3464         {\n",
      "3465             Py_DECREF(py_data);\n",
      "3466             Py_DECREF(broadcastable);\n",
      "3467             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
      "3468             return NULL;\n",
      "3469         }\n",
      "3470         if (cnda->nd != PyTuple_Size(broadcastable))\n",
      "3471         {\n",
      "3472             Py_DECREF(py_data);\n",
      "3473             Py_DECREF(broadcastable);\n",
      "3474             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
      "3475             return NULL;\n",
      "3476         }\n",
      "3477         for (int i = 0; i < cnda->nd; ++i)\n",
      "3478         {\n",
      "3479             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3480             {\n",
      "3481                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
      "3482                 Py_DECREF(py_data);\n",
      "3483                 Py_DECREF(broadcastable);\n",
      "3484                 return NULL;\n",
      "3485             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
      "3486                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
      "3487                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
      "3488                 Py_DECREF(py_data);\n",
      "3489                 Py_DECREF(broadcastable);\n",
      "3490                 return NULL;\n",
      "3491             }\n",
      "3492         }\n",
      "3493         Py_DECREF(broadcastable);\n",
      "3494         return py_data;\n",
      "3495     }\n",
      "3496     else\n",
      "3497     {\n",
      "3498         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
      "3499         if (!data)\n",
      "3500         {\n",
      "3501             //err message already defined\n",
      "3502             Py_DECREF(py_data);\n",
      "3503             Py_DECREF(broadcastable);\n",
      "3504             return NULL;\n",
      "3505         }\n",
      "3506         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
      "3507         {\n",
      "3508             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3509             {\n",
      "3510                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
      "3511                 Py_DECREF(data);\n",
      "3512                 Py_DECREF(py_data);\n",
      "3513                 Py_DECREF(broadcastable);\n",
      "3514                 return NULL;\n",
      "3515             }\n",
      "3516         }\n",
      "3517         if (storage && CudaNdarray_Check(storage))\n",
      "3518         {\n",
      "3519             rval = (CudaNdarray*) storage;\n",
      "3520             Py_INCREF(rval);\n",
      "3521         }\n",
      "3522         else\n",
      "3523         {\n",
      "3524             rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3525         }\n",
      "3526         if (rval)\n",
      "3527         {\n",
      "3528             if (CudaNdarray_CopyFromArray(rval, data))\n",
      "3529             {\n",
      "3530                 Py_DECREF(rval);\n",
      "3531                 rval = NULL;\n",
      "3532             }\n",
      "3533         }\n",
      "3534         Py_DECREF(data);\n",
      "3535         Py_DECREF(py_data);\n",
      "3536         Py_DECREF(broadcastable);\n",
      "3537         return (PyObject*)rval;\n",
      "3538     }\n",
      "3539 }\n",
      "3540 \n",
      "3541 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
      "3542 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
      "3543 static PyMethodDef module_methods[] = {\n",
      "3544     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
      "3545     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
      "3546     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
      "3547     {\"select_a_gpu\", CudaNdarray_select_a_gpu, METH_NOARGS, \"Call this method if you want to select a GPU before gpu_init call and let the driver choose the GPU.\"},\n",
      "3548     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
      "3549     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
      "3550     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
      "3551     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
      "3552     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
      "3553 #if COMPUTE_GPU_MEM_USED\n",
      "3554     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
      "3555 #endif\n",
      "3556     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
      "3557     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
      "3558     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
      "3559     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
      "3560     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
      "3561     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
      "3562      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
      "3563     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3564 };\n",
      "3565 \n",
      "3566 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
      "3567 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
      "3568 \n",
      "3569 #if PY_MAJOR_VERSION == 3\n",
      "3570 static struct PyModuleDef cuda_ndarray_moduledef =\n",
      "3571 {\n",
      "3572     PyModuleDef_HEAD_INIT,\n",
      "3573     CNDA_MOD_NAME,\n",
      "3574     CNDA_DOCSTRING,\n",
      "3575     -1,     /* size of per-interpreter state of the module,\n",
      "3576                or -1 if the module keeps state in global variables. */\n",
      "3577     module_methods\n",
      "3578 };\n",
      "3579 \n",
      "3580 PyMODINIT_FUNC\n",
      "3581 PyInit_cuda_ndarray(void)\n",
      "3582 #else\n",
      "3583 PyMODINIT_FUNC\n",
      "3584 initcuda_ndarray(void)\n",
      "3585 #endif\n",
      "3586 {\n",
      "3587     import_array();\n",
      "3588 \n",
      "3589     PyObject* m;\n",
      "3590 \n",
      "3591     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
      "3592 #if PY_MAJOR_VERSION == 3\n",
      "3593         return NULL;\n",
      "3594 #else\n",
      "3595         return;\n",
      "3596 #endif\n",
      "3597     }\n",
      "3598 \n",
      "3599 #if PY_MAJOR_VERSION == 3\n",
      "3600     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
      "3601 #else\n",
      "3602     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
      "3603 #endif\n",
      "3604 \n",
      "3605     if (m == NULL) {\n",
      "3606 #if PY_MAJOR_VERSION == 3\n",
      "3607         return NULL;\n",
      "3608 #else\n",
      "3609         return;\n",
      "3610 #endif\n",
      "3611     }\n",
      "3612 \n",
      "3613     Py_INCREF(&CudaNdarrayType);\n",
      "3614     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
      "3615 #if COMPUTE_GPU_MEM_USED\n",
      "3616     for(int i=0;i<TABLE_SIZE;i++){\n",
      "3617         _alloc_size_table[i].ptr=NULL;\n",
      "3618         _alloc_size_table[i].size=0;\n",
      "3619     }\n",
      "3620 #endif\n",
      "3621     //    cublasInit();\n",
      "3622     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
      "3623     //{\n",
      "3624         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
      "3625     //}\n",
      "3626     if (0) //TODO: is this necessary?\n",
      "3627     {\n",
      "3628         int deviceId = 0; // TODO: what number goes here?\n",
      "3629         cudaSetDevice(deviceId);\n",
      "3630         cudaError_t err = cudaGetLastError();\n",
      "3631         if( cudaSuccess != err)\n",
      "3632         {\n",
      "3633             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
      "3634         }\n",
      "3635     }\n",
      "3636 \n",
      "3637 #if PY_MAJOR_VERSION == 3\n",
      "3638     return m;\n",
      "3639 #endif\n",
      "3640 }\n",
      "3641 \n",
      "3642 \n",
      "3643 //////////////////////////////////////\n",
      "3644 //\n",
      "3645 // C API FOR CudaNdarray\n",
      "3646 //\n",
      "3647 //////////////////////////////////////\n",
      "3648 \n",
      "3649 int\n",
      "3650 CudaNdarray_Check(const PyObject * ob)\n",
      "3651 {\n",
      "3652     //TODO: doesn't work with inheritance\n",
      "3653     return CudaNdarray_CheckExact(ob);\n",
      "3654 }\n",
      "3655 int\n",
      "3656 CudaNdarray_CheckExact(const PyObject * ob)\n",
      "3657 {\n",
      "3658     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
      "3659 }\n",
      "3660 \n",
      "3661 PyObject *\n",
      "3662 CudaNdarray_New(int nd)\n",
      "3663 {\n",
      "3664     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
      "3665     if (self == NULL)\n",
      "3666     {\n",
      "3667         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
      "3668         return NULL;\n",
      "3669     }\n",
      "3670     CudaNdarray_null_init(self);\n",
      "3671 \n",
      "3672     if (nd == 0)\n",
      "3673     {\n",
      "3674         self->nd = 0;\n",
      "3675     }\n",
      "3676     else if (nd > 0)\n",
      "3677     {\n",
      "3678         if (CudaNdarray_set_nd(self, nd))\n",
      "3679         {\n",
      "3680             Py_DECREF(self);\n",
      "3681             return NULL;\n",
      "3682         }\n",
      "3683     }\n",
      "3684     ++_outstanding_mallocs[1];\n",
      "3685     return (PyObject *)self;\n",
      "3686 }\n",
      "3687 \n",
      "3688 \n",
      "3689 \n",
      "3690 //////////////////////////////\n",
      "3691 //\n",
      "3692 // Published helper functions\n",
      "3693 //\n",
      "3694 //////////////////////////////\n",
      "3695 \n",
      "3696 static int\n",
      "3697 cublas_init()\n",
      "3698 {\n",
      "3699     cublasStatus_t err;\n",
      "3700     err = cublasCreate(&handle);\n",
      "3701     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3702     {\n",
      "3703         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
      "3704             PyErr_SetString(PyExc_RuntimeError,\n",
      "3705                             \"cublasCreate() returned this error \"\n",
      "3706                             \"'the CUDA Runtime initialization failed'\");\n",
      "3707         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
      "3708             PyErr_SetString(PyExc_RuntimeError,\n",
      "3709                             \"cublasCreate() returned this error \"\n",
      "3710                             \"'the resources could not be allocated'\");\n",
      "3711         else\n",
      "3712             PyErr_SetString(PyExc_RuntimeError,\n",
      "3713                             \"unknow error during returned by cublasCreate()\");\n",
      "3714         return -1;\n",
      "3715     }\n",
      "3716     // Set the default stream as the one to execute on (default)\n",
      "3717     cublasSetStream(handle, NULL);\n",
      "3718     // Pointer to scalars are on the host (also default)\n",
      "3719     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
      "3720 #if CUDA_VERSION >= 5000\n",
      "3721     // atomics can be used in kernels to speed up operations (not default)\n",
      "3722     // This may lead to a slight variance from run to run in some operations\n",
      "3723     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
      "3724 #endif\n",
      "3725     return 0;\n",
      "3726 }\n",
      "3727 \n",
      "3728 static void\n",
      "3729 cublas_shutdown()\n",
      "3730 {\n",
      "3731     if (handle != NULL)\n",
      "3732         cublasDestroy(handle);\n",
      "3733     // No point in handling any errors here\n",
      "3734     handle = NULL;\n",
      "3735 }\n",
      "3736 \n",
      "3737 int\n",
      "3738 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
      "3739 {\n",
      "3740     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
      "3741                                            PyArray_DIMS(obj));\n",
      "3742     if (err) {\n",
      "3743         return err;\n",
      "3744     }\n",
      "3745 \n",
      "3746     int typenum = PyArray_TYPE(obj);\n",
      "3747     if (typenum != REAL_TYPENUM)\n",
      "3748     {\n",
      "3749         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
      "3750         return -1;\n",
      "3751     }\n",
      "3752     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
      "3753     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
      "3754         (PyObject*)obj, typenum, self->nd, self->nd);\n",
      "3755     if (!py_src) {\n",
      "3756         return -1;\n",
      "3757     }\n",
      "3758     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
      "3759     void *py_src_data = PyArray_DATA(py_src);\n",
      "3760     cudaError_t cerr;\n",
      "3761     CNDA_BEGIN_ALLOW_THREADS;\n",
      "3762     cerr = cudaMemcpy(self->devdata, py_src_data,\n",
      "3763                       py_src_size * sizeof(real),\n",
      "3764                       cudaMemcpyHostToDevice);\n",
      "3765     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "3766     CNDA_END_ALLOW_THREADS;\n",
      "3767     if (cudaSuccess != cerr)\n",
      "3768     {\n",
      "3769         PyErr_Format(PyExc_RuntimeError,\n",
      "3770                      \"Cuda error '%s' while copying %lli data element\"\n",
      "3771                      \" to device memory\",\n",
      "3772                      cudaGetErrorString(cerr),\n",
      "3773                      (long long)py_src_size);\n",
      "3774         Py_DECREF(py_src);\n",
      "3775         return -1;\n",
      "3776     }\n",
      "3777     Py_DECREF(py_src);\n",
      "3778     return 0;\n",
      "3779 }\n",
      "3780 \n",
      "3781 PyObject *\n",
      "3782 CudaNdarray_new_nd(int nd)\n",
      "3783 {\n",
      "3784     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3785     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
      "3786     {\n",
      "3787         Py_XDECREF(rval);\n",
      "3788         rval = NULL;\n",
      "3789     }\n",
      "3790     return (PyObject *) rval;\n",
      "3791 }\n",
      "3792 \n",
      "3793 \n",
      "3794 /**\n",
      "3795  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
      "3796  */\n",
      "3797 \n",
      "3798 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
      "3799 {\n",
      "3800     if (self->data_allocated)\n",
      "3801     {\n",
      "3802         assert(self->devdata);\n",
      "3803         if (device_free(self->devdata))\n",
      "3804         {\n",
      "3805             self->devdata = NULL;\n",
      "3806             self->data_allocated = 0;\n",
      "3807             return -1;\n",
      "3808         }\n",
      "3809     }\n",
      "3810     // Get the original base object (base.base.base...)\n",
      "3811     PyObject * orig_base = base;\n",
      "3812     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
      "3813     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
      "3814     {\n",
      "3815         // base_base is itself a view\n",
      "3816         orig_base = ((CudaNdarray*) orig_base)->base;\n",
      "3817     }\n",
      "3818     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
      "3819     if (self->base != orig_base)\n",
      "3820     {\n",
      "3821         Py_XDECREF(self->base);\n",
      "3822         self->base = orig_base;\n",
      "3823         Py_XINCREF(self->base);\n",
      "3824     }\n",
      "3825     self->data_allocated = 0;\n",
      "3826     self->devdata = data;\n",
      "3827     return 0;\n",
      "3828 }\n",
      "3829 \n",
      "3830 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
      "3831 {\n",
      "3832     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
      "3833     {\n",
      "3834         y[i*sy] = x[i*sx];\n",
      "3835     }\n",
      "3836 }\n",
      "3837 \n",
      "3838 // N1 through N4 are the size of y\n",
      "3839 static __global__ void k_copy_4d(const int N1,\n",
      "3840         const int N2, const int N3, const int N4,\n",
      "3841         const float * x, const int sx1, const int sx2, const int sx3,\n",
      "3842         const int sx4,  float * y, const int sy1, const int sy2,\n",
      "3843         const int sy3, const int sy4)\n",
      "3844 {\n",
      "3845     // These must be made int instead of unsigned int due to a bug in nvcc\n",
      "3846     int bx = blockIdx.x;\n",
      "3847     int by = blockIdx.y;\n",
      "3848 \n",
      "3849     for (int i = bx; i < N1; i += gridDim.x)\n",
      "3850     {\n",
      "3851         for (int j = by; j < N2; j += gridDim.y)\n",
      "3852         {\n",
      "3853             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
      "3854             {\n",
      "3855                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
      "3856                 {\n",
      "3857                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
      "3858                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
      "3859                 }\n",
      "3860             }\n",
      "3861         }\n",
      "3862     }\n",
      "3863 }\n",
      "3864 \n",
      "3865 //copy from other into self\n",
      "3866 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
      "3867                                     const CudaNdarray * other,\n",
      "3868                                     bool unbroadcast)\n",
      "3869 {\n",
      "3870     int verbose = 0;\n",
      "3871     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
      "3872 \n",
      "3873     //standard elemwise size checks\n",
      "3874     if (self->nd == -1)\n",
      "3875     {\n",
      "3876         PyErr_SetString(PyExc_TypeError,\n",
      "3877                         \"can't copy into un-initialized CudaNdarray\");\n",
      "3878         return -1;\n",
      "3879     }\n",
      "3880     CudaNdarray * new_other = NULL;\n",
      "3881 \n",
      "3882     if (self->nd < other->nd)\n",
      "3883     {\n",
      "3884         PyErr_Format(PyExc_NotImplementedError,\n",
      "3885             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
      "3886             \"destination needs to be >= the number of dimensions of the \"\n",
      "3887             \"source. Got %d and %d.\", self->nd, other->nd);\n",
      "3888         return -1;\n",
      "3889     }\n",
      "3890     else if (self->nd != other->nd)\n",
      "3891     {\n",
      "3892         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
      "3893         int added_dims = self->nd - other->nd;\n",
      "3894         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
      "3895         for(int i = 0; i < added_dims; i++)\n",
      "3896             pattern[i] = -1;\n",
      "3897         for(int i = 0; i < other->nd; i++)\n",
      "3898             pattern[i + added_dims] = i;\n",
      "3899         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
      "3900         other = new_other;\n",
      "3901     }\n",
      "3902     assert(self->nd == other->nd);\n",
      "3903     //standard elemwise dim checks (also compute total size)\n",
      "3904     unsigned int size = 1;\n",
      "3905     unsigned int size_source = 1;\n",
      "3906     for (int i = 0; i< self->nd; ++i)\n",
      "3907     {\n",
      "3908         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "3909             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
      "3910         {\n",
      "3911           PyErr_Format(PyExc_ValueError,\n",
      "3912                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
      "3913                        \" need same dimensions for dim %d,\"\n",
      "3914                        \" destination=%d, source=%d\",\n",
      "3915                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
      "3916                        CudaNdarray_HOST_DIMS(other)[i]);\n",
      "3917           Py_XDECREF(new_other);\n",
      "3918           return -1;\n",
      "3919         }\n",
      "3920         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "3921         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
      "3922     }\n",
      "3923     if (0 == size)\n",
      "3924     {\n",
      "3925         Py_XDECREF(new_other);\n",
      "3926         return 0; //nothing to copy, we're done.\n",
      "3927     }\n",
      "3928     if (CudaNdarray_is_c_contiguous(self) &&\n",
      "3929         CudaNdarray_is_c_contiguous(other) &&\n",
      "3930         size == size_source)\n",
      "3931     {\n",
      "3932         if (verbose)\n",
      "3933             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
      "3934 \n",
      "3935         cublasStatus_t err;\n",
      "3936         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
      "3937                           CudaNdarray_DEV_DATA(self), 1);\n",
      "3938         CNDA_THREAD_SYNC;\n",
      "3939         Py_XDECREF(new_other);\n",
      "3940         if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3941         {\n",
      "3942             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
      "3943             return -1;\n",
      "3944         }\n",
      "3945         return 0;\n",
      "3946     }\n",
      "3947     //TODO: rewrite these copy operations to be more efficient\n",
      "3948     //      See, for example the transpose example in the cuda_sdk.\n",
      "3949     switch (self->nd)\n",
      "3950     {\n",
      "3951         case 0: // scalar\n",
      "3952             {\n",
      "3953                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
      "3954                 assert(0);\n",
      "3955             }; break;\n",
      "3956         case 1: // vector\n",
      "3957             {\n",
      "3958                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
      "3959                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
      "3960                 unsigned int n_blocks = std::min(size,\n",
      "3961                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3962                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
      "3963                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3964                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
      "3965                                             CudaNdarray_DEV_DATA(other),\n",
      "3966                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "3967                                             CudaNdarray_DEV_DATA(self),\n",
      "3968                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "3969                 CNDA_THREAD_SYNC;\n",
      "3970                 cudaError_t err = cudaGetLastError();\n",
      "3971                 if( cudaSuccess != err)\n",
      "3972                 {\n",
      "3973                     PyErr_Format(PyExc_RuntimeError,\n",
      "3974                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "3975                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
      "3976                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
      "3977                     Py_XDECREF(new_other);\n",
      "3978                     return -1;\n",
      "3979                 }\n",
      "3980             }; break;\n",
      "3981         case 4: // 4-tensor\n",
      "3982             {\n",
      "3983                 if (verbose)\n",
      "3984                 {\n",
      "3985                     if (0 != fprint_CudaNdarray(stderr, other))\n",
      "3986                     {\n",
      "3987                         Py_XDECREF(new_other);\n",
      "3988                         return -1;\n",
      "3989                     }\n",
      "3990                 }\n",
      "3991 \n",
      "3992                 // The blocks implement the looping over the first two axes so\n",
      "3993                 // this needs to be (N1, N2)\n",
      "3994                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
      "3995                                         NUM_VECTOR_OP_BLOCKS),\n",
      "3996                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
      "3997                                         NUM_VECTOR_OP_BLOCKS));\n",
      "3998                 // For the threads, just make as many as possible\n",
      "3999                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
      "4000                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
      "4001                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
      "4002                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "4003 \n",
      "4004                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
      "4005                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
      "4006 \n",
      "4007                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
      "4008                                             // size of y\n",
      "4009                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
      "4010                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
      "4011                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
      "4012                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
      "4013                                             CudaNdarray_DEV_DATA(other), // x\n",
      "4014                                             // x strides\n",
      "4015                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "4016                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
      "4017                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
      "4018                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
      "4019                                             CudaNdarray_DEV_DATA(self), // y\n",
      "4020                                             // y strides\n",
      "4021                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
      "4022                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "4023                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "4024                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
      "4025                                             );\n",
      "4026                 CNDA_THREAD_SYNC;\n",
      "4027                 cudaError_t err = cudaGetLastError();\n",
      "4028                 if( cudaSuccess != err)\n",
      "4029                 {\n",
      "4030                     PyErr_Format(PyExc_RuntimeError,\n",
      "4031                                  \"Cuda error: %s: %s.\",\n",
      "4032                                  \"k_copy_4d\",\n",
      "4033                                  cudaGetErrorString(err));\n",
      "4034                     Py_XDECREF(new_other);\n",
      "4035                     return -1;\n",
      "4036                 }\n",
      "4037             }; break;\n",
      "4038         default:\n",
      "4039             {\n",
      "4040                 assert (cudaSuccess == cudaGetLastError());\n",
      "4041                 if (verbose)\n",
      "4042                     fprintf(stderr,\n",
      "4043                             \"Copying with default version unbroadcast=%d\\n\",\n",
      "4044                             unbroadcast);\n",
      "4045                 // call worker routine\n",
      "4046                 unsigned int threads_per_block = std::min(size,\n",
      "4047                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4048                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n",
      "4049                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "4050                 const CudaNdarray * cuda_dims = other;\n",
      "4051                 if(unbroadcast)\n",
      "4052                     cuda_dims = self;\n",
      "4053                 //copy from other into self\n",
      "4054                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
      "4055                         size,\n",
      "4056                         (unsigned int)other->nd,\n",
      "4057                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
      "4058                         (const float*)CudaNdarray_DEV_DATA(other),\n",
      "4059                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
      "4060                         CudaNdarray_DEV_DATA(self),\n",
      "4061                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
      "4062                 CNDA_THREAD_SYNC;\n",
      "4063                 cudaError_t err = cudaGetLastError();\n",
      "4064                 if(verbose>1)\n",
      "4065                     fprintf(stderr,\n",
      "4066                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
      "4067                             \" n_threads_per_block=%i)\\n\",\n",
      "4068                             n_blocks, threads_per_block);\n",
      "4069                 if( cudaSuccess != err)\n",
      "4070                 {\n",
      "4071                     //fprint_CudaNdarray(stderr, self);\n",
      "4072                     //fprint_CudaNdarray(stderr, other);\n",
      "4073                     PyErr_Format(PyExc_RuntimeError,\n",
      "4074                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4075                                  \" n_threads_per_block=%i)\\n\",\n",
      "4076                                  \"k_elemwise_unary_rowmajor_copy\",\n",
      "4077                                  cudaGetErrorString(err), n_blocks,\n",
      "4078                                  threads_per_block);\n",
      "4079                     Py_XDECREF(new_other);\n",
      "4080                     return -1;\n",
      "4081                 }\n",
      "4082             }\n",
      "4083     };\n",
      "4084     Py_XDECREF(new_other);\n",
      "4085     return 0;\n",
      "4086 }\n",
      "4087 \n",
      "4088 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4089 {\n",
      "4090     if (A->nd != 2)\n",
      "4091     {\n",
      "4092         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
      "4093         return -1;\n",
      "4094     }\n",
      "4095     if (B->nd != 2)\n",
      "4096     {\n",
      "4097         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
      "4098         return -1;\n",
      "4099     }\n",
      "4100     if (C->nd != 2)\n",
      "4101     {\n",
      "4102         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
      "4103         return -1;\n",
      "4104     }\n",
      "4105 \n",
      "4106     // We must allow dimensions to be zeros.\n",
      "4107     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4108             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
      "4109             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
      "4110     {\n",
      "4111         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
      "4112                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4113                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4114                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4115                 CudaNdarray_HOST_DIMS(B)[1],\n",
      "4116                 CudaNdarray_HOST_DIMS(C)[0],\n",
      "4117                 CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4118         return -1;\n",
      "4119     }\n",
      "4120 \n",
      "4121     // If matrix A or B has non-unit size and non-unit stride in both\n",
      "4122     // dimensions, we can make a copy.\n",
      "4123     CudaNdarray * A_new = NULL;\n",
      "4124     CudaNdarray * B_new = NULL;\n",
      "4125     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4126          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4127          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4128          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4129         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4130         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4131     {\n",
      "4132         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4133         if (!A_new)\n",
      "4134             return -1;\n",
      "4135         A = A_new;\n",
      "4136     }\n",
      "4137 \n",
      "4138     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
      "4139          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
      "4140          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
      "4141          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
      "4142         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4143         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
      "4144     {\n",
      "4145         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4146         if (!B_new)\n",
      "4147         {\n",
      "4148             // If A_new is NULL, meaning A was not copied nothing happens\n",
      "4149             Py_XDECREF(A_new);\n",
      "4150             return -1;\n",
      "4151         }\n",
      "4152         B = B_new;\n",
      "4153     }\n",
      "4154 \n",
      "4155     // If matrix C has non-unit size and non-unit stride in both\n",
      "4156     // dimensions, or negative strides, we can't operate. We cannot copy\n",
      "4157     // C either, because the calling code will expect the result to be\n",
      "4158     // in the original C container.\n",
      "4159     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
      "4160          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
      "4161          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
      "4162          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
      "4163         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
      "4164         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
      "4165     {\n",
      "4166         PyErr_Format(PyExc_AssertionError,\n",
      "4167                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
      "4168                      CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4169                      CudaNdarray_HOST_STRIDES(C)[1],\n",
      "4170                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4171                      CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4172         Py_XDECREF(A_new);\n",
      "4173         Py_XDECREF(B_new);\n",
      "4174         return -1;\n",
      "4175     }\n",
      "4176 \n",
      "4177     // the unit integer is divided logically into three fields of 4 bits\n",
      "4178     // the lowermost 4 bits encode the stride pattern of the output\n",
      "4179     // the next higher 4 bits encode the B variable (or y)\n",
      "4180     // the next higher 4 bits encode the C variable (or x)\n",
      "4181     //\n",
      "4182     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
      "4183     //                                                 1 for unit stride from row to row (Col major)\n",
      "4184 \n",
      "4185     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
      "4186     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
      "4187     // If a dimension is 0, its stride will not be used either, so we can\n",
      "4188     // consider it a 'unit' stride too.\n",
      "4189     int unit = 0;\n",
      "4190     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
      "4191         unit |= (0x0 << 8);\n",
      "4192     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
      "4193         unit |= (0x1 << 8);\n",
      "4194     } else {\n",
      "4195         unit |= (0x2 << 8);\n",
      "4196     }\n",
      "4197     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
      "4198         unit |= (0x0 << 4);\n",
      "4199     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
      "4200         unit |= (0x1 << 4);\n",
      "4201     } else {\n",
      "4202         unit |= (0x2 << 4);\n",
      "4203     }\n",
      "4204     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
      "4205         unit |= (0x0 << 0);\n",
      "4206     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
      "4207         unit |= (0x1 << 0);\n",
      "4208     } else {\n",
      "4209         unit |= (0x2 << 0);\n",
      "4210     }\n",
      "4211 \n",
      "4212     /* create appropriate strides for malformed matrices that are row or column\n",
      "4213      * vectors\n",
      "4214      */\n",
      "4215     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4216     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4217     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
      "4218     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
      "4219     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
      "4220     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
      "4221 \n",
      "4222     float* a = CudaNdarray_DEV_DATA(A);\n",
      "4223     float* b = CudaNdarray_DEV_DATA(B);\n",
      "4224     float* c = CudaNdarray_DEV_DATA(C);\n",
      "4225     cublasOperation_t N = CUBLAS_OP_N;\n",
      "4226     cublasOperation_t T = CUBLAS_OP_T;\n",
      "4227     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
      "4228     // There should be no negative stride at that point\n",
      "4229 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
      "4230     if (sx == 0){sx = 1;}\\\n",
      "4231     if (sy == 0){sy = 1;}\\\n",
      "4232     if (sz == 0){sz = 1;}\\\n",
      "4233     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
      "4234         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
      "4235     } else { \\\n",
      "4236         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
      "4237         Py_XDECREF(A_new);\\\n",
      "4238         Py_XDECREF(B_new);\\\n",
      "4239         return -1; \\\n",
      "4240     }\n",
      "4241 \n",
      "4242     cublasStatus_t err;\n",
      "4243     switch(unit)\n",
      "4244     {\n",
      "4245         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
      "4246         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
      "4247         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
      "4248         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
      "4249         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
      "4250         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
      "4251         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
      "4252         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
      "4253         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
      "4254                  return -1;\n",
      "4255     };\n",
      "4256     CNDA_THREAD_SYNC;\n",
      "4257     Py_XDECREF(A_new);\n",
      "4258     Py_XDECREF(B_new);\n",
      "4259 \n",
      "4260     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4261     {\n",
      "4262         PyErr_Format(PyExc_RuntimeError,\n",
      "4263                      \"cublasSgemm failed (%i) %s\\n\"\n",
      "4264                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
      "4265                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
      "4266                      err,  cublasGetErrorString(err),\n",
      "4267                      unit, N,\n",
      "4268                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4269                      CudaNdarray_HOST_DIMS(C)[1],\n",
      "4270                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4271                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
      "4272 \n",
      "4273         return -1;\n",
      "4274     }\n",
      "4275     return 0;\n",
      "4276 }\n",
      "4277 \n",
      "4278 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4279 {\n",
      "4280     /**\n",
      "4281     * C <- alpha A B + beta C\n",
      "4282     *    A : matrix\n",
      "4283     *    B, C: vector\n",
      "4284     *    alpha, beta: scalars\n",
      "4285     */\n",
      "4286     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
      "4287     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4288     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4289 \n",
      "4290     // We must allow dimensions to be zeros.\n",
      "4291     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4292             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
      "4293     {\n",
      "4294         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
      "4295                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4296                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4297                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4298                 CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4299         return -1;\n",
      "4300     }\n",
      "4301 \n",
      "4302     // If matrix A has non-unit size and non-unit stride in both\n",
      "4303     // dimensions, or negative strides, we cannot operate, but we can\n",
      "4304     // make a copy.\n",
      "4305     CudaNdarray * A_new = NULL;\n",
      "4306     CudaNdarray * B_new = NULL;\n",
      "4307     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4308          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4309          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4310          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4311         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4312         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4313     {\n",
      "4314         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4315         if (!A_new)\n",
      "4316             return -1;\n",
      "4317         A = A_new;\n",
      "4318     }\n",
      "4319 \n",
      "4320     // If vector B as a negative stride, we also have to make a copy.\n",
      "4321     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4322     {\n",
      "4323         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4324         if (!B_new)\n",
      "4325         {\n",
      "4326             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
      "4327             // do anything\n",
      "4328             Py_XDECREF(A_new);\n",
      "4329             return -1;\n",
      "4330         }\n",
      "4331         B = B_new;\n",
      "4332     }\n",
      "4333 \n",
      "4334     // cudablas does not handle negative strides as expected\n",
      "4335     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4336         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4337     {\n",
      "4338         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
      "4339                 CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4340                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
      "4341         Py_XDECREF(A_new);\n",
      "4342         Py_XDECREF(B_new);\n",
      "4343         return -1;\n",
      "4344     }\n",
      "4345 \n",
      "4346     /* create appropriate strides for malformed matrices that are row or column\n",
      "4347      * vectors\n",
      "4348      */\n",
      "4349     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4350     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4351     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
      "4352     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
      "4353 \n",
      "4354     if (sa_0 == 0)\n",
      "4355         sa_0 = 1;\n",
      "4356     if (sa_1 == 0)\n",
      "4357         sa_1 = 1;\n",
      "4358 \n",
      "4359     // This is important because we can end up not calling Sgemv at all\n",
      "4360     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4361     if (CudaNdarray_SIZE(C)) {\n",
      "4362         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4363             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4364                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4365         {\n",
      "4366             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
      "4367                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4368                     &alpha,\n",
      "4369                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4370                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4371                     &beta,\n",
      "4372                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4373         }\n",
      "4374         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4375                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4376                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4377         {\n",
      "4378             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
      "4379                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
      "4380                     &alpha,\n",
      "4381                     CudaNdarray_DEV_DATA(A), sa_0,\n",
      "4382                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4383                     &beta,\n",
      "4384                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4385         }\n",
      "4386         else\n",
      "4387         {\n",
      "4388             PyErr_Format(PyExc_AssertionError,\n",
      "4389                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
      "4390                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
      "4391                          CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4392                          CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4393                          CudaNdarray_HOST_STRIDES(B)[0],\n",
      "4394                          CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4395                          CudaNdarray_HOST_DIMS(A)[0],\n",
      "4396                          CudaNdarray_HOST_DIMS(A)[1],\n",
      "4397                          CudaNdarray_HOST_DIMS(B)[0],\n",
      "4398                          CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4399             Py_XDECREF(A_new);\n",
      "4400             Py_XDECREF(B_new);\n",
      "4401             return -1;\n",
      "4402         }\n",
      "4403     }\n",
      "4404 \n",
      "4405     CNDA_THREAD_SYNC;\n",
      "4406     Py_XDECREF(A_new);\n",
      "4407     Py_XDECREF(B_new);\n",
      "4408 \n",
      "4409     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4410     {\n",
      "4411         PyErr_Format(PyExc_RuntimeError,\n",
      "4412                      \"cublasSgemv failed (%i)\",\n",
      "4413                      err);\n",
      "4414         return -1;\n",
      "4415     }\n",
      "4416     return 0;\n",
      "4417 }\n",
      "4418 \n",
      "4419 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
      "4420     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
      "4421     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
      "4422     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
      "4423 \n",
      "4424     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
      "4425         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
      "4426         PyErr_Format(PyExc_ValueError,\n",
      "4427                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
      "4428                      CudaNdarray_HOST_DIMS(x)[0],\n",
      "4429                      CudaNdarray_HOST_DIMS(y)[0],\n",
      "4430                      CudaNdarray_HOST_DIMS(A)[0],\n",
      "4431                      CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4432         return -1;\n",
      "4433     }\n",
      "4434 \n",
      "4435     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4436     CudaNdarray * x_new = NULL;\n",
      "4437     if(x_strides == 0){\n",
      "4438         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
      "4439             PyErr_Format(PyExc_RuntimeError,\n",
      "4440                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
      "4441                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4442                          \" that has more than 1 element!\");\n",
      "4443             return -1;\n",
      "4444         }\n",
      "4445         x_strides = 1;\n",
      "4446     } else if(x_strides < 0){\n",
      "4447         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
      "4448         x = x_new;\n",
      "4449         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4450     }\n",
      "4451 \n",
      "4452     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4453     CudaNdarray * y_new = NULL;\n",
      "4454     if(y_strides == 0){\n",
      "4455         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
      "4456             PyErr_Format(PyExc_RuntimeError,\n",
      "4457                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
      "4458                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4459                          \" that has more than 1 elements!\");\n",
      "4460             Py_XDECREF(x_new);\n",
      "4461             return -1;\n",
      "4462         }\n",
      "4463         y_strides = 1;\n",
      "4464     } else if(y_strides < 0){\n",
      "4465         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
      "4466         y = y_new;\n",
      "4467         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4468     }\n",
      "4469 \n",
      "4470     // Create appropriate strides if A is a row or column vector\n",
      "4471     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
      "4472                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4473     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
      "4474                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4475 \n",
      "4476     // This is important because we can end up not calling Sger at all\n",
      "4477     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4478     if(CudaNdarray_SIZE(A)){\n",
      "4479         // If A is in col-major\n",
      "4480         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4481             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4482                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4483         {\n",
      "4484             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
      "4485                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4486                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4487                        CudaNdarray_DEV_DATA(A), sa_1);\n",
      "4488         }\n",
      "4489         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
      "4490         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4491                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4492                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4493         {\n",
      "4494             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
      "4495                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4496                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4497                        CudaNdarray_DEV_DATA(A), sa_0);\n",
      "4498         }\n",
      "4499         // A has to be either c- or f-contiguous, with no negative strides\n",
      "4500         else\n",
      "4501         {\n",
      "4502             PyErr_SetString(PyExc_NotImplementedError,\n",
      "4503                             \"non-contiguous A, or negative strides, in sger\");\n",
      "4504             Py_XDECREF(x_new);\n",
      "4505             Py_XDECREF(y_new);\n",
      "4506             return -1;\n",
      "4507         }\n",
      "4508     }\n",
      "4509     CNDA_THREAD_SYNC;\n",
      "4510     Py_XDECREF(x_new);\n",
      "4511     Py_XDECREF(y_new);\n",
      "4512 \n",
      "4513     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4514     {\n",
      "4515         PyErr_Format(PyExc_RuntimeError,\n",
      "4516                      \"cublasSger failed (%i)\",\n",
      "4517                      err);\n",
      "4518         return -1;\n",
      "4519     }\n",
      "4520 \n",
      "4521     return 0;\n",
      "4522 }\n",
      "4523 \n",
      "4524 /**\n",
      "4525  *\n",
      "4526  * Precondition:\n",
      "4527  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
      "4528  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
      "4529  *\n",
      "4530  *  TODO: templatize this function to support other reductions.\n",
      "4531  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
      "4532  */\n",
      "4533 \n",
      "4534 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
      "4535         const unsigned int nd,\n",
      "4536         const int * dims_a,\n",
      "4537         const int * log2_dims_a,\n",
      "4538         const int * a_str,\n",
      "4539         const float * a_data,\n",
      "4540         const int * z_str,\n",
      "4541         float * z_data)\n",
      "4542 {\n",
      "4543     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "4544     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "4545 \n",
      "4546     //structure data contains the strides and dimensions of both a and z\n",
      "4547     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
      "4548     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
      "4549     // a_str[0], ... a_str[nd-1],\n",
      "4550     // z_str[0], ... z_str[nd-1]\n",
      "4551     extern __shared__ int structure_data[];\n",
      "4552     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
      "4553     {\n",
      "4554         structure_data[i+0*nd] = dims_a[i];\n",
      "4555         structure_data[i+1*nd] = log2_dims_a[i];\n",
      "4556         structure_data[i+2*nd] = a_str[i];\n",
      "4557         structure_data[i+3*nd] = z_str[i];\n",
      "4558     }\n",
      "4559     dims_a = structure_data;\n",
      "4560     log2_dims_a = structure_data + nd;\n",
      "4561     a_str = structure_data + 2*nd;\n",
      "4562     z_str = structure_data + 3*nd;\n",
      "4563 \n",
      "4564     __syncthreads(); //wait for all the shared structure to be loaded\n",
      "4565 \n",
      "4566     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
      "4567     {\n",
      "4568         unsigned int ii = i;\n",
      "4569         const float * a_data_i = a_data;\n",
      "4570         float * z_data_i = z_data;\n",
      "4571         unsigned int n_reduce_elements = 1;\n",
      "4572         unsigned int n_reduce_dims = 0;\n",
      "4573         unsigned int reduce_dim0 = nd-1;\n",
      "4574 \n",
      "4575 \n",
      "4576         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
      "4577         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
      "4578         for (unsigned int d = 0; d < nd; ++d)\n",
      "4579         {\n",
      "4580             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
      "4581             {\n",
      "4582                 n_reduce_elements *= dims_a[d];\n",
      "4583                 n_reduce_dims += 1;\n",
      "4584                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
      "4585             }\n",
      "4586             else //'d' is not a dimension that we are reducing over\n",
      "4587             {\n",
      "4588                 unsigned int pos_d;\n",
      "4589                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
      "4590                 {\n",
      "4591                     // this branch is not preferred,\n",
      "4592                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4593                     pos_d = (ii % dims_a[d]);\n",
      "4594                     ii = (ii / dims_a[d]);\n",
      "4595                 }\n",
      "4596                 else\n",
      "4597                 {\n",
      "4598                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
      "4599                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
      "4600                 }\n",
      "4601                 a_data_i += pos_d * a_str[d];\n",
      "4602                 z_data_i += pos_d * z_str[d];\n",
      "4603             }\n",
      "4604         }\n",
      "4605         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
      "4606         // do a similar loop\n",
      "4607 \n",
      "4608         float sum = 0.0f;\n",
      "4609         switch(n_reduce_dims)\n",
      "4610         {\n",
      "4611             case 0:\n",
      "4612                 {\n",
      "4613                     sum = a_data_i[0];\n",
      "4614                 }\n",
      "4615                 break;\n",
      "4616             case 1:\n",
      "4617                 {\n",
      "4618                     const int stride = a_str[reduce_dim0];\n",
      "4619                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
      "4620                     while (a_data_i != a_data_i_max)\n",
      "4621                     {\n",
      "4622                         sum += a_data_i[0];\n",
      "4623                         a_data_i += stride;\n",
      "4624                     }\n",
      "4625                 }\n",
      "4626                 break;\n",
      "4627             case 2:\n",
      "4628                 {\n",
      "4629                     int rd = reduce_dim0+1;\n",
      "4630                     for (; rd < nd; ++rd)\n",
      "4631                     {\n",
      "4632                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
      "4633                             break;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4634                     }\n",
      "4635                     const int stride0 = a_str[reduce_dim0];\n",
      "4636                     const int stride1 = a_str[rd];\n",
      "4637                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
      "4638                     {\n",
      "4639                         const float * a_data_ri = a_data_i + ii * stride1;\n",
      "4640                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
      "4641                         while (a_data_ri != a_data_ri_max)\n",
      "4642                         {\n",
      "4643                             sum += a_data_ri[0];\n",
      "4644                             a_data_ri += stride0;\n",
      "4645                         }\n",
      "4646                     }\n",
      "4647                 };\n",
      "4648                 break;\n",
      "4649             default:\n",
      "4650                 {\n",
      "4651                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
      "4652                     {\n",
      "4653                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
      "4654                         unsigned int reduce_ii = reduce_i;\n",
      "4655                         const float * a_data_ri = a_data_i;\n",
      "4656 \n",
      "4657                         //This loop finds the element in the a slice to add.\n",
      "4658                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
      "4659                         {\n",
      "4660                             unsigned int pos_d;\n",
      "4661                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
      "4662                             {\n",
      "4663                                 if (log2_dims_a[rd]==-1)\n",
      "4664                                 {\n",
      "4665                                     // this branch is not preferred,\n",
      "4666                                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4667                                     pos_d = (reduce_ii % dims_a[rd]);\n",
      "4668                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
      "4669                                 }\n",
      "4670                                 else\n",
      "4671                                 {\n",
      "4672                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
      "4673                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
      "4674                                 }\n",
      "4675                                 a_data_ri += pos_d * a_str[rd];\n",
      "4676                             }\n",
      "4677                         }\n",
      "4678                         sum += a_data_ri[0];\n",
      "4679                     }\n",
      "4680                 }\n",
      "4681         }\n",
      "4682         z_data_i[0] = sum;\n",
      "4683     }\n",
      "4684 }\n",
      "4685 \n",
      "4686 static __global__ void kernel_reduce_sum_1011(\n",
      "4687         const unsigned int d0,\n",
      "4688         const unsigned int d1,\n",
      "4689         const unsigned int d2,\n",
      "4690         const unsigned int d3,\n",
      "4691         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
      "4692         float * Z, const int sZ0)\n",
      "4693 {\n",
      "4694     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
      "4695     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
      "4696     extern __shared__ float buf[];\n",
      "4697     float mysum = 0.0f;\n",
      "4698 \n",
      "4699     if (warpSize != 32)\n",
      "4700     {\n",
      "4701         return;  //TODO: set error code\n",
      "4702     }\n",
      "4703 \n",
      "4704     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
      "4705     {\n",
      "4706         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
      "4707         mysum += Ai;\n",
      "4708     }\n",
      "4709     buf[threadNum] = mysum;\n",
      "4710     __syncthreads();\n",
      "4711 \n",
      "4712     // rest of function is handled by one warp\n",
      "4713     if (threadNum < warpSize)\n",
      "4714     {\n",
      "4715         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
      "4716         {\n",
      "4717             mysum += buf[i];\n",
      "4718         }\n",
      "4719         buf[threadNum] = mysum;\n",
      "4720         if (threadNum < 16)\n",
      "4721         {\n",
      "4722             //reduce so that threadNum 0 has the sum of everything\n",
      "4723             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
      "4724             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
      "4725             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
      "4726             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
      "4727             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
      "4728             if (threadNum == 0)\n",
      "4729             {\n",
      "4730                 Z[blockIdx.x*sZ0] = buf[0];\n",
      "4731             }\n",
      "4732         }\n",
      "4733     }\n",
      "4734 }\n",
      "4735 /**\n",
      "4736  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
      "4737  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
      "4738  */\n",
      "4739 int\n",
      "4740 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
      "4741 {\n",
      "4742     int verbose = 0;\n",
      "4743     //check input rank\n",
      "4744     if (self->nd != A->nd)\n",
      "4745     {\n",
      "4746         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
      "4747         return -1;\n",
      "4748     }\n",
      "4749     for (int i = 0; i < self->nd; ++i)\n",
      "4750     {\n",
      "4751         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
      "4752         {\n",
      "4753             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
      "4754                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
      "4755             return -1;\n",
      "4756         }\n",
      "4757     }\n",
      "4758 \n",
      "4759     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
      "4760     if (verbose)\n",
      "4761     {\n",
      "4762         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
      "4763         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
      "4764         fprint_CudaNdarray(stderr, A);\n",
      "4765         fprint_CudaNdarray(stderr, self);\n",
      "4766     }\n",
      "4767     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
      "4768             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
      "4769             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
      "4770             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
      "4771        )\n",
      "4772     {\n",
      "4773         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
      "4774         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4775         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
      "4776         n_threads.z -= 1;\n",
      "4777         if (n_threads.z > 64) n_threads.z = 64;\n",
      "4778         if (n_threads.z)\n",
      "4779         {\n",
      "4780             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
      "4781             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
      "4782             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
      "4783                     CudaNdarray_HOST_DIMS(A)[0],\n",
      "4784                     CudaNdarray_HOST_DIMS(A)[1],\n",
      "4785                     CudaNdarray_HOST_DIMS(A)[2],\n",
      "4786                     CudaNdarray_HOST_DIMS(A)[3],\n",
      "4787                     CudaNdarray_DEV_DATA(A),\n",
      "4788                     CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4789                     CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4790                     CudaNdarray_HOST_STRIDES(A)[2],\n",
      "4791                     CudaNdarray_HOST_STRIDES(A)[3],\n",
      "4792                     CudaNdarray_DEV_DATA(self),\n",
      "4793                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
      "4794             CNDA_THREAD_SYNC;\n",
      "4795             if (cudaSuccess == cudaGetLastError()) return 0;\n",
      "4796             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
      "4797         }\n",
      "4798     }\n",
      "4799 \n",
      "4800     int n_threads_per_block = std::min(n_summations,\n",
      "4801             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4802     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
      "4803             NUM_VECTOR_OP_BLOCKS);\n",
      "4804     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
      "4805 \n",
      "4806     if (verbose)\n",
      "4807     {\n",
      "4808         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
      "4809     }\n",
      "4810     assert (self->nd > 0);\n",
      "4811     assert (self->nd == A->nd);\n",
      "4812     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
      "4813             n_summations,\n",
      "4814             self->nd,\n",
      "4815             CudaNdarray_DEV_DIMS(A),\n",
      "4816             CudaNdarray_DEV_LOG2DIMS(A),\n",
      "4817             CudaNdarray_DEV_STRIDES(A),\n",
      "4818             CudaNdarray_DEV_DATA(A),\n",
      "4819             CudaNdarray_DEV_STRIDES(self),\n",
      "4820             CudaNdarray_DEV_DATA(self));\n",
      "4821     CNDA_THREAD_SYNC;\n",
      "4822     cudaError_t err = cudaGetLastError();\n",
      "4823     if (cudaSuccess != err)\n",
      "4824     {\n",
      "4825         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
      "4826         return -1;\n",
      "4827     }\n",
      "4828     return 0;\n",
      "4829 }\n",
      "4830 int\n",
      "4831 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
      "4832 {\n",
      "4833     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4834     return -1;\n",
      "4835 }\n",
      "4836 int\n",
      "4837 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
      "4838 {\n",
      "4839     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4840     return -1;\n",
      "4841 }\n",
      "4842 int\n",
      "4843 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
      "4844 {\n",
      "4845     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4846     return -1;\n",
      "4847 }\n",
      "4848 \n",
      "4849 \n",
      "4850 /**\n",
      "4851  *\n",
      "4852  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
      "4853  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
      "4854  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
      "4855  *\n",
      "4856  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
      "4857  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
      "4858  */\n",
      "4859 int\n",
      "4860 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
      "4861 {\n",
      "4862     //TODO: pass a workspace pointer to avoid the internal malloc\n",
      "4863     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
      "4864     int * newstrides = newdims + len;\n",
      "4865     int * dims_taken = newstrides + len;\n",
      "4866     if (!newdims)\n",
      "4867     {\n",
      "4868         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
      "4869         return -1;\n",
      "4870     }\n",
      "4871     for (int i = 0; i < self->nd; ++i)\n",
      "4872     {\n",
      "4873         dims_taken[i] = 0;\n",
      "4874     }\n",
      "4875     for (int i = 0; i < len; ++i)\n",
      "4876     {\n",
      "4877         if (pattern[i] < 0)\n",
      "4878         {\n",
      "4879             newdims[i] = 1;\n",
      "4880             newstrides[i] = 0;\n",
      "4881         }\n",
      "4882         else if(dims_taken[pattern[i]])\n",
      "4883         {\n",
      "4884             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
      "4885                          pattern[i]);\n",
      "4886             free(newdims);\n",
      "4887             return -1;\n",
      "4888         }\n",
      "4889         else if (pattern[i]>= self->nd)\n",
      "4890         {\n",
      "4891             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
      "4892                          pattern[i], self->nd);\n",
      "4893             free(newdims);\n",
      "4894             return -1;\n",
      "4895         }\n",
      "4896         else\n",
      "4897         {\n",
      "4898             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
      "4899             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
      "4900             dims_taken[pattern[i]] = 1;\n",
      "4901         }\n",
      "4902     }\n",
      "4903     //Check if we dropped not broadcastable dims\n",
      "4904     for (int i = 0; i < self->nd; ++i)\n",
      "4905     {\n",
      "4906         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
      "4907         {\n",
      "4908             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
      "4909             free(newdims);\n",
      "4910             return -1;\n",
      "4911         }\n",
      "4912     }\n",
      "4913     //swap this structure in for the one in self, and sync to the card\n",
      "4914     if (CudaNdarray_set_nd(self, len))\n",
      "4915     {\n",
      "4916         free(newdims);\n",
      "4917         return -1;\n",
      "4918     }\n",
      "4919     for (int i = 0; i < len; ++i)\n",
      "4920     {\n",
      "4921         CudaNdarray_set_dim(self, i, newdims[i]);\n",
      "4922         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "4923     }\n",
      "4924     if (cnda_copy_structure_to_device(self))\n",
      "4925     {\n",
      "4926         free(newdims);\n",
      "4927         return -1;\n",
      "4928     }\n",
      "4929     free(newdims);\n",
      "4930     return 0;\n",
      "4931 }\n",
      "4932 \n",
      "4933 \n",
      "4934 \n",
      "4935 /**\n",
      "4936  *\n",
      "4937  *  This is the function that bind to python.\n",
      "4938  *  See CudaNdarray_dimshuffle to call from C.\n",
      "4939  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
      "4940  */\n",
      "4941 PyObject *\n",
      "4942 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
      "4943 {\n",
      "4944     PyObject * self = NULL;\n",
      "4945     PyObject * pattern_object = NULL;\n",
      "4946     int * pattern = NULL;\n",
      "4947     PyObject * rval = NULL;\n",
      "4948     int success = -1;\n",
      "4949     //const int * dims = NULL;\n",
      "4950 \n",
      "4951     //args should consist of two python objects (\"OO\")\n",
      "4952     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
      "4953         return NULL;\n",
      "4954 \n",
      "4955     if (!CudaNdarray_Check(self) )\n",
      "4956     {\n",
      "4957         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
      "4958         return NULL;\n",
      "4959     }\n",
      "4960 \n",
      "4961     //parse pattern_object into int * pattern\n",
      "4962 \n",
      "4963     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
      "4964 \n",
      "4965     if (pattern_dim < 0)\n",
      "4966     {\n",
      "4967         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
      "4968         return NULL;\n",
      "4969     }\n",
      "4970 \n",
      "4971     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
      "4972 \n",
      "4973     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
      "4974     {\n",
      "4975         PyObject * idx = PyLong_FromLong(i);\n",
      "4976 \n",
      "4977         if (idx == NULL)\n",
      "4978         {\n",
      "4979             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
      "4980             goto CudaNdarray_dimshuffle_fail;\n",
      "4981         }\n",
      "4982 \n",
      "4983         long elem_value = 0;\n",
      "4984 \n",
      "4985         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
      "4986 \n",
      "4987         if (elem == NULL)\n",
      "4988         {\n",
      "4989             Py_XDECREF( elem);\n",
      "4990             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "4991             goto CudaNdarray_dimshuffle_fail;\n",
      "4992         }\n",
      "4993 \n",
      "4994         elem_value = PyInt_AsLong(elem);\n",
      "4995 \n",
      "4996         if (elem_value == -1 && PyErr_Occurred() )\n",
      "4997         {\n",
      "4998             Py_XDECREF(elem);\n",
      "4999             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5000             goto CudaNdarray_dimshuffle_fail;\n",
      "5001         }\n",
      "5002 \n",
      "5003         pattern[i] = elem_value;\n",
      "5004 \n",
      "5005         Py_XDECREF( elem );\n",
      "5006         Py_XDECREF( idx );\n",
      "5007     }\n",
      "5008 \n",
      "5009     //allocate rval\n",
      "5010     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
      "5011 \n",
      "5012     if (rval == NULL)\n",
      "5013     {\n",
      "5014         //CudaNdarray_New should have set the exception string\n",
      "5015         goto CudaNdarray_dimshuffle_fail;\n",
      "5016     }\n",
      "5017 \n",
      "5018 \n",
      "5019     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
      "5020     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
      "5021     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
      "5022     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
      "5023 \n",
      "5024     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
      "5025 \n",
      "5026     if (success != 0)\n",
      "5027     {\n",
      "5028         //Exception string should already be set by CudaNdarray_dimshuffle\n",
      "5029         goto CudaNdarray_dimshuffle_fail;\n",
      "5030     }\n",
      "5031 \n",
      "5032     free(pattern);\n",
      "5033 \n",
      "5034     return rval;\n",
      "5035 \n",
      "5036     CudaNdarray_dimshuffle_fail:\n",
      "5037 \n",
      "5038     if (pattern != NULL)\n",
      "5039         free(pattern);\n",
      "5040 \n",
      "5041     Py_XDECREF(rval);\n",
      "5042     return NULL;\n",
      "5043 }\n",
      "5044 \n",
      "5045 \n",
      "5046 int\n",
      "5047 cnda_structure_size(int nd)\n",
      "5048 {\n",
      "5049     // dim0, dim1, ...\n",
      "5050     // str0, str1, ...\n",
      "5051     // log2(dim0), log2(dim1), ...\n",
      "5052     return nd + nd + nd;\n",
      "5053 }\n",
      "5054 \n",
      "5055 const int *\n",
      "5056 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
      "5057 {\n",
      "5058     return self->host_structure;\n",
      "5059 }\n",
      "5060 \n",
      "5061 const int *\n",
      "5062 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
      "5063 {\n",
      "5064     return self->host_structure + self->nd;\n",
      "5065 }\n",
      "5066 const int *\n",
      "5067 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
      "5068 {\n",
      "5069     return self->host_structure + 2*self->nd;\n",
      "5070 }\n",
      "5071 \n",
      "5072 int\n",
      "5073 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
      "5074 {\n",
      "5075     int verbose = 0;\n",
      "5076 \n",
      "5077     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
      "5078     {\n",
      "5079         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
      "5080         return 0;\n",
      "5081     }\n",
      "5082 \n",
      "5083     if (cnda1->nd != cnda2->nd)\n",
      "5084     {\n",
      "5085         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
      "5086         return 0;\n",
      "5087     }\n",
      "5088 \n",
      "5089     for (int i=0; i < 2*cnda1->nd; i++)\n",
      "5090     {\n",
      "5091         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
      "5092         {\n",
      "5093             if(verbose)\n",
      "5094                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
      "5095             return 0;\n",
      "5096         }\n",
      "5097     }\n",
      "5098 \n",
      "5099     if (!ignoreBase && cnda1->base != cnda2->base)\n",
      "5100     {\n",
      "5101         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
      "5102         return 0;\n",
      "5103     }\n",
      "5104     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
      "5105     {\n",
      "5106         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
      "5107         return 0;\n",
      "5108     }\n",
      "5109     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
      "5110     {\n",
      "5111         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
      "5112         // no need to check devdata if data is not allocated\n",
      "5113         return 0;\n",
      "5114     }\n",
      "5115 \n",
      "5116     return 1;\n",
      "5117 }\n",
      "5118 \n",
      "5119 \n",
      "5120 int\n",
      "5121 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
      "5122 {\n",
      "5123     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
      "5124 }\n",
      "5125 \n",
      "5126 int\n",
      "5127 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
      "5128 {\n",
      "5129     //If the device structure do not exists, create it.\n",
      "5130     //We allocate it here as we do not need it often.\n",
      "5131     //In fact, we need it so infrequently that we expect\n",
      "5132     //that most object won't need it. Not allocating it\n",
      "5133     //save a significant when creating object.\n",
      "5134     //This speed up a benchmark by 8% with the gc.\n",
      "5135     if (!self->dev_structure)\n",
      "5136     {\n",
      "5137         int struct_size = cnda_structure_size(self->nd);\n",
      "5138         if (struct_size)\n",
      "5139         {\n",
      "5140             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
      "5141             if (NULL == self->dev_structure)\n",
      "5142             {\n",
      "5143                 return -1;\n",
      "5144             }\n",
      "5145         }\n",
      "5146     }\n",
      "5147     if (cublasSetVector(cnda_structure_size(self->nd),\n",
      "5148                         sizeof(int),\n",
      "5149                         self->host_structure,\n",
      "5150                         1,\n",
      "5151                         self->dev_structure,\n",
      "5152                         1) != CUBLAS_STATUS_SUCCESS)\n",
      "5153     {\n",
      "5154         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
      "5155         return -1;\n",
      "5156     }\n",
      "5157     self->dev_structure_fresh = 1;\n",
      "5158     return 0;\n",
      "5159 }\n",
      "5160 \n",
      "5161 const int *\n",
      "5162 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
      "5163 {\n",
      "5164     if (!self->dev_structure_fresh)\n",
      "5165     {\n",
      "5166         if (cnda_copy_structure_to_device(self))\n",
      "5167             return NULL;\n",
      "5168     }\n",
      "5169     return self->dev_structure;\n",
      "5170 }\n",
      "5171 const int *\n",
      "5172 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
      "5173 {\n",
      "5174     if (!self->dev_structure_fresh)\n",
      "5175     {\n",
      "5176         if (cnda_copy_structure_to_device(self))\n",
      "5177             return NULL;\n",
      "5178     }\n",
      "5179     return self->dev_structure + self->nd;\n",
      "5180 }\n",
      "5181 const int *\n",
      "5182 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
      "5183 {\n",
      "5184     if (!self->dev_structure_fresh)\n",
      "5185     {\n",
      "5186         if (cnda_copy_structure_to_device(self))\n",
      "5187             return NULL;\n",
      "5188     }\n",
      "5189     return self->dev_structure + 2*self->nd;\n",
      "5190 }\n",
      "5191 float *\n",
      "5192 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
      "5193 {\n",
      "5194     return self->devdata;\n",
      "5195 }\n",
      "5196 \n",
      "5197 /**\n",
      "5198  * Return the number of elements in the ndarray (product of the dimensions)\n",
      "5199  */\n",
      "5200 size_t\n",
      "5201 CudaNdarray_SIZE(const CudaNdarray *self)\n",
      "5202 {\n",
      "5203     if (self->nd == -1) return 0;\n",
      "5204     size_t size = 1;\n",
      "5205     for (int i = 0; i < self->nd; ++i)\n",
      "5206     {\n",
      "5207         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5208     }\n",
      "5209     return size;\n",
      "5210 }\n",
      "5211 \n",
      "5212 PyObject *\n",
      "5213 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
      "5214 {\n",
      "5215     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
      "5216 }\n",
      "5217 \n",
      "5218 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
      "5219 {\n",
      "5220     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
      "5221 }\n",
      "5222 \n",
      "5223 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
      "5224 {\n",
      "5225     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
      "5226 }\n",
      "5227 \n",
      "5228 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
      "5229 {\n",
      "5230     cudaError_t err = cudaGetLastError();\n",
      "5231     if( cudaSuccess != err)\n",
      "5232     {\n",
      "5233         PyErr_Format(PyExc_RuntimeError,\n",
      "5234                      \"Cuda error: %s: %s.\",\n",
      "5235                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
      "5236                      cudaGetErrorString(err));\n",
      "5237         return -1;\n",
      "5238     }\n",
      "5239     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
      "5240             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
      "5241     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
      "5242     for (int i = 0; i < self->nd; ++i)\n",
      "5243     {\n",
      "5244         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
      "5245     }\n",
      "5246     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
      "5247     for (int i = 0; i < self->nd; ++i)\n",
      "5248     {\n",
      "5249         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "5250     }\n",
      "5251 \n",
      "5252     if (self->dev_structure)\n",
      "5253     {\n",
      "5254         int data=0;\n",
      "5255         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
      "5256         for (int i = 0; i < self->nd; ++i)\n",
      "5257         {\n",
      "5258             cublasGetVector(1, sizeof(int),\n",
      "5259                             self->dev_structure+i, 1,\n",
      "5260                             &data, 1);\n",
      "5261             fprintf(fd, \"%i\\t\", data);\n",
      "5262         }\n",
      "5263         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
      "5264         for (int i = 0; i < self->nd; ++i)\n",
      "5265         {\n",
      "5266             cublasGetVector(1, sizeof(int),\n",
      "5267                             self->dev_structure + self->nd+i, 1,\n",
      "5268                             &data, 1);\n",
      "5269             fprintf(fd, \"%i \\t\", data);\n",
      "5270         }\n",
      "5271         fprintf(fd, \"\\n\");\n",
      "5272     }\n",
      "5273     else\n",
      "5274     {\n",
      "5275         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
      "5276     }\n",
      "5277 \n",
      "5278     err = cudaGetLastError();\n",
      "5279     if( cudaSuccess != err)\n",
      "5280     {\n",
      "5281         PyErr_Format(PyExc_RuntimeError,\n",
      "5282                      \"Cuda error: %s: %s.\",\n",
      "5283                      \"fprint_CudaNdarray\",\n",
      "5284                      cudaGetErrorString(err));\n",
      "5285         return -1;\n",
      "5286     }\n",
      "5287     return 0;\n",
      "5288 }\n",
      "5289 \n",
      "5290 \n",
      "5291 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
      "5292                             const int * dims, int fortran)\n",
      "5293 {\n",
      "5294     bool allocated = false;\n",
      "5295     if (*arr == NULL)\n",
      "5296     {\n",
      "5297         // This allocates the metadata but not the data\n",
      "5298         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
      "5299         if (*arr == NULL)\n",
      "5300             return -1;\n",
      "5301         allocated = true;\n",
      "5302     }\n",
      "5303 \n",
      "5304     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
      "5305     {\n",
      "5306         if (allocated)\n",
      "5307         {\n",
      "5308             Py_DECREF(*arr);\n",
      "5309             *arr = NULL;\n",
      "5310         }\n",
      "5311         return -1;\n",
      "5312     }\n",
      "5313     return 0;\n",
      "5314 }\n",
      "5315 \n",
      "5316 \n",
      "5317 /*\n",
      "5318   Local Variables:\n",
      "5319   mode:c++\n",
      "5320   c-basic-offset:4\n",
      "5321   c-file-style:\"stroustrup\"\n",
      "5322   indent-tabs-mode:nil\n",
      "5323   fill-column:79\n",
      "5324   End:\n",
      "5325 */\n",
      "5326 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
      "5327 \n",
      "===============================\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -Xlinker /DEBUG -D HAVE_ROUND -m64 -Xcompiler -DCUDA_NDARRAY_CUH=m18715462c72ed6afcd7ca5d52813ce90,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD -IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\theano\\\\sandbox\\\\cuda -IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include -IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\include -IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\theano\\\\gof -o C:\\\\Users\\\\Deep-Learning-PC\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-10-10.0.16299-SP0-Intel64_Family_6_Model_79_Stepping_1_GenuineIntel-3.5.3-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd mod.cu -LC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\libs -LC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow -lpython35 -lcublas -lcudart')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\r\n",
      "nvcc fatal   : Cannot find compiler 'cl.exe' in PATH\r\n",
      "\n",
      "['nvcc', '-shared', '-O3', '-Xlinker', '/DEBUG', '-D HAVE_ROUND', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=m18715462c72ed6afcd7ca5d52813ce90,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD', '-IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\theano\\\\sandbox\\\\cuda', '-IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include', '-IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\include', '-IC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\theano\\\\gof', '-o', 'C:\\\\Users\\\\Deep-Learning-PC\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-10-10.0.16299-SP0-Intel64_Family_6_Model_79_Stepping_1_GenuineIntel-3.5.3-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd', 'mod.cu', '-LC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow\\\\libs', '-LC:\\\\Users\\\\Deep-Learning-PC\\\\Anaconda3\\\\envs\\\\tensorflow', '-lpython35', '-lcublas', '-lcudart']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from main import siamese_word_model_neg_sampling_distillation_loss\n",
    "from main import ppdb_utils\n",
    "from main import utils\n",
    "from main import params\n",
    "from main.tree import addOOVwords\n",
    "from main import evaluate\n",
    "from main import marriage\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load the Configuration Parameters:\n",
    "\n",
    "For the Out-of-Vocabulary (OOV) words, we initialize them with values sampled from a normal distribution ($X \\sim \\mathcal{N}(\\mu=0,\\,\\sigma^{2}=0.01)\\,$).\n",
    "\n",
    "Please set the input of the load_from_yaml method to:\n",
    "1. 'data/ma-nci/ma-nci_params.yaml' for performing ontology matching between the **Adult Mouse Anatomical Dictionary** and the **Foundation Model of Anatomy** ontology.\n",
    "2. 'data/fma-nci/fma-nci_params.yaml' for performing ontology matching between the **Foundation Model of Anatomy** ontology and the **NCI Thesaurus**.\n",
    "3. 'data/fma-snomed/fma-snomed_params.yaml' for performing ontology matching between the **Foundation Model of Anatomy** ontology and **SNOMED CT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that existed in the pre-trained words vectors trained on PubMed and PMC is: 3418\n"
     ]
    }
   ],
   "source": [
    "params = params.params()\n",
    "params.load_from_yaml('data/fma-snomed/fma-snomed_params.yaml')\n",
    "(words, We) = utils.getWordmap(params.wordfile)\n",
    "print('The number of words that existed in the pre-trained words vectors trained on PubMed and PMC is: %d' % (len(words)))\n",
    "# Read the training data and add the OOV words in the dictionary.\n",
    "examples = utils.getDataset(params.train, words)\n",
    "We = addOOVwords(examples, words, We, mean=0, sigma=0.01)\n",
    "\n",
    "params.batchsize = len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Print the resulted Configuration Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialize the Phrase Retrofitting Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = siamese_word_model_neg_sampling_distillation_loss.ppdb_word_model(We, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extract possible examples of 'descriptive associated' terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms_of_ontology_1 = params.terms_of_ontology_1\n",
    "terms_of_ontology_2 = params.terms_of_ontology_2\n",
    "ontology_1_ants = utils.getAntonyms(terms_of_ontology_1, words)\n",
    "ontology_2_ants = utils.getAntonyms(terms_of_ontology_2, words)\n",
    "ants = ontology_1_ants + ontology_2_ants\n",
    "utils.getAntRepresentations(model, ants)\n",
    "syns = utils.createSet(examples, words)\n",
    "utils.getAntRepresentations(model, syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train the Phrase Retroffiting Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppdb_utils.train(model, examples, words, params, synonyms=syns, antonyms=ants, start=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run (1) the <cite>[McVitie et al.][1]</cite> algorithm for solving the Stable Marriage Assignment problem and (2) display the resulted performance:\n",
    "\n",
    "\n",
    "[1]:https://link.springer.com/article/10.1007/BF01934199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "start_time = time()\n",
    "alignments = marriage.ontology_alignment(model, terms_of_ontology_1, terms_of_ontology_2, words, ceil=0.2)\n",
    "end_time = time()\n",
    "print(\"Total matching time:\", (end_time - start_time))\n",
    "results = marriage.alignment_evaluation(model, words, alignments, params.ground_truth_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess the synonymy data so as to feed them to the <cite>[Denoising Autoencoder][1]</cite>\n",
    "\n",
    "Note: We preprocess the data as described in <cite>[LeCun et al.][2]</cite>\n",
    "\n",
    "\n",
    "[1]:https://dl.acm.org/citation.cfm?id=1390294\n",
    "[2]:http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from main.utils import prepare_data\n",
    "\n",
    "left = []\n",
    "for example in examples:\n",
    "    example[0].populate_embeddings(words)\n",
    "    left.append(example[0].embeddings)\n",
    "right = []\n",
    "for example in examples:\n",
    "    example[1].populate_embeddings(words)\n",
    "    right.append(example[1].embeddings)\n",
    "    \n",
    "X1, M1 = prepare_data(left)\n",
    "X2, M2 = prepare_data(right)\n",
    "# Data preprocessing\n",
    "embg1 = model.feedforward_function(X1, M1)\n",
    "embg2 = model.feedforward_function(X2, M2)\n",
    "embg1 = embg1 - embg1.mean(axis=1, keepdims=True)\n",
    "embg2 = embg2 - embg2.mean(axis=1, keepdims=True)\n",
    "\n",
    "embg1 = embg1/ np.linalg.norm(embg1)\n",
    "embg2 = embg1/ np.linalg.norm(embg2)\n",
    "# End of Data preprocessing\n",
    "\n",
    "# As the number of training data is not huge, we extract only the 1% of them for checking the validation loss.\n",
    "train_len = int(len(embg1)*0.99)\n",
    "train_origins = embg1[:train_len]\n",
    "train_targets = embg2[:train_len]\n",
    "test_origins = embg1[train_len:]\n",
    "test_targets = embg2[train_len:]\n",
    "\n",
    "train_all1 = np.concatenate((train_origins, train_origins, train_targets, train_targets), axis=0)\n",
    "train_all2 = np.concatenate((train_origins, train_targets, train_origins, train_targets), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialize the Denoising Autoencoder (DAE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 6.25, assuming the input is 200 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(200,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "v = 0.4\n",
    "dropped_input = Dropout(v)(input_img)\n",
    "\n",
    "encoded = Dense(encoding_dim, \n",
    "                activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-05)\n",
    "               )(dropped_input)\n",
    "decoded = Dense(200, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define the DAE's loss and the optimization method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train the DAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_callback = evaluate.TimeHistory()\n",
    "autoencoder.fit(train_all1, train_all2,\n",
    "                epochs=15,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_origins, test_targets),\n",
    "                callbacks=[time_callback])\n",
    "print(\"DAE's total training time:\", (time_callback.overall_training_time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Define some helpful functions so as to get the output of the DAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2em(model,sent,words):\n",
    "    d = evaluate.sen2Embgs(model,sent,words)\n",
    "    emb = np.zeros(200)\n",
    "    \n",
    "    for key, value in d.items():\n",
    "        emb += value\n",
    "    return emb/len(d)\n",
    "\n",
    "def ae_sim(model, string, words):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sentence_1, sentence_2 = string.split(', ')\n",
    "    x1 = sen2em(model,sentence_1,words)\n",
    "    x2 = sen2em(model,sentence_2,words)\n",
    "    encoded_imgs_1 = encoder.predict(x1.reshape(1,200))\n",
    "    encoded_imgs_2 = encoder.predict(x2.reshape(1,200))\n",
    "    return np.squeeze(cosine_similarity(encoded_imgs_1,encoded_imgs_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the Outlier Detection Component based on the DAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "detected_outliers=[]\n",
    "outlier_threshold = 0.2\n",
    "for x in results[0]:\n",
    "    string = x.split(') -> ')[0][1:]\n",
    "    ae_value = 1-ae_sim(model, string, words)\n",
    "    if ae_value >= outlier_threshold:\n",
    "        detected_outliers.append(x)\n",
    "        cnt+=1\n",
    "    elif ae_value < outlier_threshold:\n",
    "        print(x + ' ||| ' +str(ae_value))\n",
    "print('The number of detected outliers is: %d' % (cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute the number of correctly detected misalignments and the number of correct alignments that were marked wrongly as misalignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong, correct = 0, 0\n",
    "for x in detected_outliers:\n",
    "    if '\\x1b[0m' in x:\n",
    "        wrong+=1\n",
    "    else:\n",
    "        correct+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Display the resulted performance after the application of the DAE based Outlier Detection Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The DAE outlier detector discovered correctly %d misalignments' % (wrong))\n",
    "print('However, the DAE outlier detector also confused %d true alignments as misalignments' % (correct))\n",
    "\n",
    "precision = (1.0*(results[3]-correct))/((results[3]-correct)+(results[4]-wrong))\n",
    "recall = (1.0*(results[3]-correct))/results[2]\n",
    "\n",
    "print('The new precision is: %f' % (precision))\n",
    "print('The new recall    is: %f' % (recall))\n",
    "print('The new F1-score  is  %f' % ((2.0*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
