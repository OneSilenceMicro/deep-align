{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd() + '\\\\main\\\\'\n",
    "sys.path.append(cwd)\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from main import siamese_word_model_neg_sampling_distillation_loss\n",
    "from main import ppdb_utils\n",
    "from main import utils\n",
    "from main import params\n",
    "from main.tree import addOOVwords\n",
    "from main import evaluate\n",
    "from main import marriage\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load the Configuration Parameters:\n",
    "\n",
    "For the Out-of-Vocabulary (OOV) words, we initialize them with values sampled from a normal distribution ($X \\sim \\mathcal{N}(\\mu=0,\\,\\sigma^{2}=0.01)\\,$).\n",
    "\n",
    "Please set the input of the load_from_yaml method to:\n",
    "1. 'data/ma-nci/ma-nci_params.yaml' for performing ontology matching between the **Adult Mouse Anatomical Dictionary** and the **Foundation Model of Anatomy** ontology.\n",
    "2. 'data/fma-nci/fma-nci_params.yaml' for performing ontology matching between the **Foundation Model of Anatomy** ontology and the **NCI Thesaurus**.\n",
    "3. 'data/fma-snomed/fma-snomed_params.yaml' for performing ontology matching between the **Foundation Model of Anatomy** ontology and **SNOMED CT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params.params()\n",
    "params.load_from_yaml('data/fma-snomed/fma-snomed_params.yaml')\n",
    "(words, We) = utils.getWordmap(params.wordfile)\n",
    "print('The number of words that existed in the pre-trained words vectors trained on PubMed and PMC is: %d' % (len(words)))\n",
    "# Read the training data and add the OOV words in the dictionary.\n",
    "examples = utils.getDataset(params.train, words)\n",
    "We = addOOVwords(examples, words, We, mean=0, sigma=0.01)\n",
    "\n",
    "params.batchsize = len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Print the resulted Configuration Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialize the Phrase Retrofitting Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = siamese_word_model_neg_sampling_distillation_loss.ppdb_word_model(We, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extract possible examples of 'descriptive associated' terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms_of_ontology_1 = params.terms_of_ontology_1\n",
    "terms_of_ontology_2 = params.terms_of_ontology_2\n",
    "ontology_1_ants = utils.getAntonyms(terms_of_ontology_1, words)\n",
    "ontology_2_ants = utils.getAntonyms(terms_of_ontology_2, words)\n",
    "ants = ontology_1_ants + ontology_2_ants\n",
    "utils.getAntRepresentations(model, ants)\n",
    "syns = utils.createSet(examples, words)\n",
    "utils.getAntRepresentations(model, syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train the Phrase Retroffiting Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppdb_utils.train(model, examples, words, params, synonyms=syns, antonyms=ants, start=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run (1) the <cite>[McVitie et al.][1]</cite> algorithm for solving the Stable Marriage Assignment problem and (2) display the resulted performance:\n",
    "\n",
    "\n",
    "[1]:https://link.springer.com/article/10.1007/BF01934199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "start_time = time()\n",
    "alignments = marriage.ontology_alignment(model, terms_of_ontology_1, terms_of_ontology_2, words, ceil=0.2)\n",
    "end_time = time()\n",
    "print(\"Total matching time:\", (end_time - start_time))\n",
    "results = marriage.alignment_evaluation(model, words, alignments, params.ground_truth_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess the synonymy data so as to feed them to the <cite>[Denoising Autoencoder][1]</cite>\n",
    "\n",
    "Note: We preprocess the data as described in <cite>[LeCun et al.][2]</cite>\n",
    "\n",
    "\n",
    "[1]:https://dl.acm.org/citation.cfm?id=1390294\n",
    "[2]:http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from main.utils import prepare_data\n",
    "\n",
    "left = []\n",
    "for example in examples:\n",
    "    example[0].populate_embeddings(words)\n",
    "    left.append(example[0].embeddings)\n",
    "right = []\n",
    "for example in examples:\n",
    "    example[1].populate_embeddings(words)\n",
    "    right.append(example[1].embeddings)\n",
    "    \n",
    "X1, M1 = prepare_data(left)\n",
    "X2, M2 = prepare_data(right)\n",
    "# Data preprocessing\n",
    "embg1 = model.feedforward_function(X1, M1)\n",
    "embg2 = model.feedforward_function(X2, M2)\n",
    "embg1 = embg1 - embg1.mean(axis=1, keepdims=True)\n",
    "embg2 = embg2 - embg2.mean(axis=1, keepdims=True)\n",
    "\n",
    "embg1 = embg1/ np.linalg.norm(embg1)\n",
    "embg2 = embg1/ np.linalg.norm(embg2)\n",
    "# End of Data preprocessing\n",
    "\n",
    "# As the number of training data is not huge, we extract only the 1% of them for checking the validation loss.\n",
    "train_len = int(len(embg1)*0.99)\n",
    "train_origins = embg1[:train_len]\n",
    "train_targets = embg2[:train_len]\n",
    "test_origins = embg1[train_len:]\n",
    "test_targets = embg2[train_len:]\n",
    "\n",
    "train_all1 = np.concatenate((train_origins, train_origins, train_targets, train_targets), axis=0)\n",
    "train_all2 = np.concatenate((train_origins, train_targets, train_origins, train_targets), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialize the Denoising Autoencoder (DAE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 6.25, assuming the input is 200 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(200,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "v = 0.4\n",
    "dropped_input = Dropout(v)(input_img)\n",
    "\n",
    "encoded = Dense(encoding_dim, \n",
    "                activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-05)\n",
    "               )(dropped_input)\n",
    "decoded = Dense(200, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define the DAE's loss and the optimization method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train the DAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_callback = evaluate.TimeHistory()\n",
    "autoencoder.fit(train_all1, train_all2,\n",
    "                epochs=15,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_origins, test_targets),\n",
    "                callbacks=[time_callback])\n",
    "print(\"DAE's total training time:\", (time_callback.overall_training_time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Define some helpful functions so as to get the output of the DAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2em(model,sent,words):\n",
    "    d = evaluate.sen2Embgs(model,sent,words)\n",
    "    emb = np.zeros(200)\n",
    "    \n",
    "    for key, value in d.items():\n",
    "        emb += value\n",
    "    return emb/len(d)\n",
    "\n",
    "def ae_sim(model, string, words):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sentence_1, sentence_2 = string.split(', ')\n",
    "    x1 = sen2em(model,sentence_1,words)\n",
    "    x2 = sen2em(model,sentence_2,words)\n",
    "    encoded_imgs_1 = encoder.predict(x1.reshape(1,200))\n",
    "    encoded_imgs_2 = encoder.predict(x2.reshape(1,200))\n",
    "    return np.squeeze(cosine_similarity(encoded_imgs_1,encoded_imgs_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the Outlier Detection Component based on the DAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "detected_outliers=[]\n",
    "outlier_threshold = 0.2\n",
    "for x in results[0]:\n",
    "    string = x.split(') -> ')[0][1:]\n",
    "    ae_value = 1-ae_sim(model, string, words)\n",
    "    if ae_value >= outlier_threshold:\n",
    "        detected_outliers.append(x)\n",
    "        cnt+=1\n",
    "    elif ae_value < outlier_threshold:\n",
    "        print(x + ' ||| ' +str(ae_value))\n",
    "print('The number of detected outliers is: %d' % (cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute the number of correctly detected misalignments and the number of correct alignments that were marked wrongly as misalignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong, correct = 0, 0\n",
    "for x in detected_outliers:\n",
    "    if '\\x1b[0m' in x:\n",
    "        wrong+=1\n",
    "    else:\n",
    "        correct+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Display the resulted performance after the application of the DAE based Outlier Detection Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The DAE outlier detector discovered correctly %d misalignments' % (wrong))\n",
    "print('However, the DAE outlier detector also confused %d true alignments as misalignments' % (correct))\n",
    "\n",
    "precision = (1.0*(results[3]-correct))/((results[3]-correct)+(results[4]-wrong))\n",
    "recall = (1.0*(results[3]-correct))/results[2]\n",
    "\n",
    "print('The new precision is: %f' % (precision))\n",
    "print('The new recall    is: %f' % (recall))\n",
    "print('The new F1-score  is  %f' % ((2.0*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
